{
  "timestamp": 1757461871514,
  "workingProviders": [
    "openai",
    "anthropic",
    "gemini",
    "xai",
    "deepseek"
  ],
  "failedProviders": [
    {
      "provider": "openai",
      "model": "openai/gpt-4o",
      "reason": "‚è∞ A resposta do aider demorou muito. Tente novamente."
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-2.5-flash-lite",
      "reason": "Aider v0.86.1\nModel: gemini/gemini-2.5-flash-lite with whole edit format\nGit repo: ../../../../.git with 15 files\nRepo-map: using 4096 tokens, auto refresh\nNote: in-chat filenames are always relative to the git working dir, not the \ncurrent working dir.\nCur working dir: /mnt/f/Node/cmmv-hive/gov/bips/BIP-05/monitor\nGit working dir: /mnt/f/Node/cmmv-hive\n\ngov/bips/BIP-05/monitor/server.js\n\n```diff\n@@ -1,51 +1,53 @@\n-const http = require('http');\n-const express = require('express');\n-const WebSocket = require('ws');\n const fs = require('fs');\n const path = require('path');\n-const yaml = require('js-yaml');\n-\n-// ============================================================================\n-// SISTEMA DE LOGGING ROBUSTO\n-// ============================================================================\n-\n-const LOG_FILE = path.join(__dirname, 'server-debug.log');\n-const ERROR_LOG_FILE = path.join(__dirname, 'server-errors.log');\n-\n+const WebSocket = require('ws');\n+const colors = require('colors'); // Assuming colors is available\n+\n+// --- Configuration ---\n+const ISSUES_FILE = path.join(__dirname, '..', '..', '..', '..', \n'issues.json');\n+const API_CACHE_FILE = path.join(__dirname, '..', '..', '..', '..', \n'api_cache.json');\n+const CACHE_DURATION = 15 * 60 * 1000; // 15 minutes\n+\n+// --- Model Definitions (Simplified for example) ---\n+const MODEL_CATEGORIES = {\n+    generals: ['openai/gpt-4o', 'anthropic/claude-3-opus-20240229', \n'google/gemini-1.5-pro-latest'],\n+    cursor_models: ['cursor/gpt-4-turbo'],\n+    bip_specific: ['xai/bip-model-v1'],\n+    aider_models: {\n+        'openai/gpt-4o': { provider: 'openai', key: 'OPENAI_API_KEY', model: \n'gpt-4o' },\n+        'anthropic/claude-3-opus-20240229': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-opus-20240229' },\n+        'google/gemini-1.5-pro-latest': { provider: 'google', key: \n'GEMINI_API_KEY', model: 'gemini-1.5-pro-latest' },\n+        'xai/bip-model-v1': { provider: 'xai', key: 'XAI_API_KEY', model: \n'bip-model-v1' },\n+        'deepseek/coder': { provider: 'deepseek', key: 'DEEPSEEK_API_KEY', \nmodel: 'coder' },\n+        'groq/llama3-70b': { provider: 'groq', key: 'GROQ_API_KEY', model: \n'llama3-70b' },\n+        'cursor/gpt-4-turbo': { provider: 'cursor', key: null, model: \n'gpt-4-turbo' } // Cursor models handled differently\n+    }\n+};\n+\n+// --- Global Variables ---\n+let wss;\n+let clients = new Set();\n+let activeOpinionSessions = new Map();\n+let activeHelloSessions = new Map();\n+let fileWatcher = null;\n+let shuttingDown = false;\n+let sessionContext = []; // Simple context for basic chat\n+\n+// --- Logging Functions ---\n function writeToLog(level, category, message, data = null) {\n     const timestamp = new Date().toISOString();\n     const logEntry = {\n         timestamp,\n         level,\n         category,\n         message,\n         data: data ? JSON.stringify(data, null, 2) : null,\n         pid: process.pid\n     };\n-\n-    const logLine = `[${timestamp}] [${level}] [${category}] ${message}${data ?\n`\\nDATA: ${JSON.stringify(data, null, 2)}` : ''}\\n`;\n-\n-    // Write to console\n-    console.log(`[${level}] [${category}] ${message}`);\n-    if (data) console.log('DATA:', data);\n-\n-    // Write to main log file\n-    try {\n-        fs.appendFileSync(LOG_FILE, logLine);\n-    } catch (err) {\n-        console.error('FATAL: Cannot write to log file:', err);\n-    }\n-\n-    // Write errors to separate error log\n-    if (level === 'ERROR' || level === 'FATAL') {\n-        try {\n-            fs.appendFileSync(ERROR_LOG_FILE, logLine);\n-        } catch (err) {\n-            console.error('FATAL: Cannot write to error log file:', err);\n-        }\n-    }\n+    // In a real app, this would write to a file or a more robust logging \nsystem\n+    console.log(`[${level}] [${category}] ${message}`, data || '');\n }\n \n function logInfo(category, message, data = null) {\n     writeToLog('INFO', category, message, data);\n }\n@@ -64,301 +66,130 @@\n \n function logFatal(category, message, data = null) {\n     writeToLog('FATAL', category, message, data);\n }\n \n-// Log startup\n-logInfo('STARTUP', 'BIP-05 Monitor Server starting...', {\n-    pid: process.pid,\n-    nodeVersion: process.version,\n-    platform: process.platform,\n-    cwd: process.cwd(),\n-    logFile: LOG_FILE,\n-    errorLogFile: ERROR_LOG_FILE\n-});\n-\n-// Load environment variables from .env file\n+// --- Environment Loading ---\n function loadEnvironment() {\n     const envPath = path.join(__dirname, '..', '..', '..', '..', '.env');\n     const envExists = fs.existsSync(envPath);\n \n     console.log(`[ENV] Checking for .env file at: ${envPath}`);\n \n     if (envExists) {\n         console.log(`[ENV] Loading .env file...`);\n         try {\n             const envContent = fs.readFileSync(envPath, 'utf8');\n-            const envLines = envContent.split('\\n');\n-\n-            envLines.forEach(line => {\n-                line = line.trim();\n-                if (line && !line.startsWith('#')) {\n+            envContent.split('\\n').forEach(line => {\n+                if (line.trim() && !line.startsWith('#')) {\n                     const [key, value] = line.split('=');\n-                    if (key && value) {\n+                    if (key && value !== undefined) {\n                         process.env[key.trim()] = value.trim();\n-                        console.log(`[ENV] Loaded: ${key.trim()}`);\n                     }\n                 }\n             });\n-\n-            console.log(`[ENV] ‚úÖ Environment variables loaded successfully`);\n+            logInfo('ENV', '.env file loaded successfully.');\n         } catch (error) {\n-            console.error(`[ENV] ‚ùå Error reading .env file:`, error.message);\n+            logError('ENV', 'Failed to load .env file', { error: error.message \n});\n         }\n     } else {\n-        console.log(`[ENV] ‚ö†Ô∏è  No .env file found. Aider models will not work \nwithout API keys.`);\n-        console.log(`[ENV] Create a .env file in the project root with your API\nkeys.`);\n-    }\n-}\n-\n-// Validate required API keys for aider models\n+        console.log('[ENV] .env file not found. Using system environment \nvariables.');\n+    }\n+}\n+\n+// --- API Key Validation ---\n function validateApiKeys() {\n     const requiredKeys = [\n         'OPENAI_API_KEY',\n         'ANTHROPIC_API_KEY',\n         'GEMINI_API_KEY',\n         'XAI_API_KEY',\n         'DEEPSEEK_API_KEY',\n         'GROQ_API_KEY'\n     ];\n-\n-    const missingKeys = [];\n-    const availableKeys = [];\n-\n-    requiredKeys.forEach(key => {\n-        if (process.env[key]) {\n-            availableKeys.push(key);\n-        } else {\n-            missingKeys.push(key);\n-        }\n-    });\n-\n-    console.log(`[ENV] API Keys Status:`);\n-    console.log(`[ENV] ‚úÖ Available: ${availableKeys.join(', ')}`);\n+    const missingKeys = requiredKeys.filter(key => !process.env[key]);\n \n     if (missingKeys.length > 0) {\n-        console.log(`[ENV] ‚ùå Missing: ${missingKeys.join(', ')}`);\n-        console.log(`[ENV] Some aider models will not be available without \nthese keys.`);\n-    }\n-\n-    return { availableKeys, missingKeys };\n-}\n-\n-// Initialize environment\n-loadEnvironment();\n-const keyStatus = validateApiKeys();\n-\n-// Test API connectivity with ALL available models\n+        logWarn('API_KEYS', 'Missing API keys', { missing: missingKeys });\n+    } else {\n+        logInfo('API_KEYS', 'All required API keys are present.');\n+    }\n+    return missingKeys.length === 0;\n+}\n+\n+// --- API Connectivity Test ---\n async function testApiConnectivity() {\n     // Check cache first\n     const cachedResults = loadApiCache();\n     if (cachedResults) {\n         const workingModels = \ngetModelsFromProviders(cachedResults.workingProviders);\n         console.log(`[API CACHE] ‚úÖ Using cached working providers: \n${cachedResults.workingProviders.join(', ')}`);\n         console.log(`[API CACHE] üìã Available models from cache: \n${workingModels.length} models`);\n         return {\n             workingApis: workingModels,\n             failedApis: [],\n-            fromCache: true\n+            cached: true\n         };\n     }\n \n-    console.log(`[API TEST] üß™ Testing API connectivity with ALL available \nmodels...`);\n-\n-    // Create test configuration for ALL models in aider_models\n-    const testModels = [];\n-    const modelConfigs = MODEL_CATEGORIES.aider_models;\n-\n-    // Iterate through all aider models and create test configuration\n-    for (const [modelId, config] of Object.entries(modelConfigs)) {\n-        testModels.push({\n-            modelId: modelId,\n-            config: config,\n-            provider: config.provider,\n-            apiKey: config.key,\n-            fullModelName: config.model\n-        });\n-    }\n-\n-    console.log(`[API TEST] üìã Testing ${testModels.length} models total`);\n-    console.log(`[API TEST] üîç Starting comprehensive model testing...`);\n-\n+    console.log('[API TEST] üß™ Starting API connectivity test...');\n     const workingProviders = [];\n     const failedProviders = [];\n-    const costReports = [];\n-    const testedProviders = new Set();\n-\n-    // Test each model individually\n-    for (const testModel of testModels) {\n-        const apiKey = process.env[testModel.apiKey];\n-\n-        if (!apiKey) {\n-            console.log(`[API TEST] ‚è≠Ô∏è  Skipping ${testModel.modelId} - No API \nkey for ${testModel.apiKey}`);\n-            failedProviders.push({\n-                provider: testModel.provider,\n-                model: testModel.modelId,\n-                reason: `Missing ${testModel.apiKey}`\n-            });\n+\n+    const providersToTest = [\n+        { name: 'openai', key: process.env.OPENAI_API_KEY, model: 'gpt-4o' },\n+        { name: 'anthropic', key: process.env.ANTHROPIC_API_KEY, model: \n'claude-3-opus-20240229' },\n+        { name: 'google', key: process.env.GEMINI_API_KEY, model: \n'gemini-1.5-pro-latest' },\n+        { name: 'xai', key: process.env.XAI_API_KEY, model: 'bip-model-v1' },\n+        { name: 'deepseek', key: process.env.DEEPSEEK_API_KEY, model: 'coder' \n},\n+        { name: 'groq', key: process.env.GROQ_API_KEY, model: 'llama3-70b' }\n+    ];\n+\n+    for (const provider of providersToTest) {\n+        if (!provider.key) {\n+            console.log(`[API TEST] ‚ö†Ô∏è Skipping ${provider.name} - API key not \nconfigured.`);\n+            failedProviders.push(provider.name);\n             continue;\n         }\n \n-        console.log(`[API TEST] üîç Testing model: ${testModel.modelId} \n(${testModel.provider})`);\n-\n         try {\n-            const testPrompt = \"Responda apenas 'OK' para confirmar que a API \nest√° funcionando.\";\n-            const result = await callLLMViaAider(testModel.modelId, \ntestPrompt);\n-\n-            // Handle new response format with cost information\n-            const response = typeof result === 'object' ? result.response : \nresult;\n-            const costInfo = typeof result === 'object' ? result.costInfo : \nnull;\n-\n-            if (response && !response.includes('‚ùå') && \nresponse.toLowerCase().includes('ok')) {\n-                console.log(`[API TEST] ‚úÖ ${testModel.modelId} - WORKING`);\n-\n-                // Add provider to working list if not already there\n-                if (!testedProviders.has(testModel.provider)) {\n-                    workingProviders.push(testModel.provider);\n-                    testedProviders.add(testModel.provider);\n-                }\n-\n-                // Store cost information if available\n-                if (costInfo) {\n-                    const hasCostData = (costInfo.inputTokens !== null && \ncostInfo.inputTokens !== undefined) || (costInfo.totalCost !== null && \ncostInfo.totalCost !== undefined);\n-\n-                    costReports.push({\n-                        provider: testModel.provider,\n-                        model: testModel.modelId,\n-                        hasCostData,\n-                        ...costInfo,\n-                        testTimestamp: new Date().toISOString()\n-                    });\n-\n-                    if (hasCostData) {\n-                        console.log(`[API TEST] üí∞ Cost data captured for \n${testModel.modelId}:`);\n-                        console.log(`[API TEST]   - Input tokens: \n${costInfo.inputTokens || 'N/A'}`);\n-                        console.log(`[API TEST]   - Output tokens: \n${costInfo.outputTokens || 'N/A'}`);\n-                        console.log(`[API TEST]   - Total cost: \n$${costInfo.totalCost || 'N/A'}`);\n-                    }\n-                }\n-            } else {\n-                console.log(`[API TEST] ‚ùå ${testModel.modelId} - FAILED: \n${response}`);\n-                failedProviders.push({\n-                    provider: testModel.provider,\n-                    model: testModel.modelId,\n-                    reason: response\n-                });\n-            }\n+            // Simulate API call - replace with actual test calls\n+            console.log(`[API TEST] Testing ${provider.name}...`);\n+            // await simulateApiCall(provider.name, provider.model); // Replace\nwith actual call\n+            await new Promise(resolve => setTimeout(resolve, 500)); // Simulate\nnetwork latency\n+            console.log(`[API TEST] ‚úÖ ${provider.name} is reachable.`);\n+            workingProviders.push(provider.name);\n         } catch (error) {\n-            console.log(`[API TEST] ‚ùå ${testModel.modelId} - ERROR: \n${error.message}`);\n-            failedProviders.push({\n-                provider: testModel.provider,\n-                model: testModel.modelId,\n-                reason: error.message\n-            });\n-        }\n-\n-        // Small delay between tests to avoid rate limits (0.5 seconds between \nmodels)\n-        await new Promise(resolve => setTimeout(resolve, 500));\n-\n-        console.log(`[API TEST] üìä Progress: ${workingProviders.length + \nfailedProviders.length}/${testModels.length} models tested`);\n-    }\n-\n-    // Save results to cache\n-    saveApiCache(workingProviders, failedProviders, costReports);\n-\n-    // Get all models for working providers\n+            console.error(`[API TEST] ‚ùå ${provider.name} failed: \n${error.message}`);\n+            failedProviders.push(provider.name);\n+        }\n+    }\n+\n+    saveApiCache(workingProviders, failedProviders);\n     const workingModels = getModelsFromProviders(workingProviders);\n-\n-    console.log(`\\n[API TEST] üìä Test Results Summary:`);\n-    console.log(`[API TEST] ‚úÖ Working Providers (${workingProviders.length}): \n${workingProviders.join(', ')}`);\n-    console.log(`[API TEST] üìã Available Models (${workingModels.length}): \n${workingModels.join(', ')}`);\n-    console.log(`[API TEST] üí∞ Cost Reports Generated: ${costReports.length}`);\n-\n-    if (failedProviders.length > 0) {\n-        console.log(`[API TEST] ‚ùå Failed Models \n(${failedProviders.length}):`);\n-        failedProviders.forEach(({ provider, model, reason }) => {\n-            console.log(`[API TEST]   - ${model} (${provider}): ${reason}`);\n-        });\n-    }\n-\n-    return {\n-        workingApis: workingModels,\n-        failedApis: failedProviders.map(f => ({ model: f.provider, error: \nf.reason })),\n-        fromCache: false\n-    };\n-}\n-\n-// Store working APIs globally\n-let WORKING_APIS = [];\n-\n-// API test cache configuration\n-const API_CACHE_FILE = path.join(__dirname, 'api-test-cache.json');\n-const CACHE_DURATION = 60 * 60 * 1000; // 1 hour in milliseconds\n-\n-// Provider to models mapping - comprehensive list from MODELS_CHECKLIST.md\n-const PROVIDER_MODELS = {\n-    'openai': [\n-        'gpt-4o',           // GPT-4o ‚Äî multimodal reasoning\n-        'gpt-4o-mini',      // GPT-4o-mini ‚Äî voting rationale specialist\n-        'o1-mini',          // O1-mini ‚Äî reasoning model\n-        'gpt-4-turbo',      // GPT-4-turbo ‚Äî high performance\n-        'gpt-5-mini',       // GPT-5-mini ‚Äî lightweight version\n-        'gpt-5-nano'        // GPT-5-nano ‚Äî ultra-lightweight\n-    ],\n-    'anthropic': [\n-        'claude-3-5-haiku-latest',    // Claude-3.5-Haiku ‚Äî fast responses\n-        'claude-3-5-sonnet-latest',   // Claude-4-Sonnet ‚Äî performance proposal\n-        'claude-3-opus-latest',       // Claude-4-Opus ‚Äî complex reasoning\n-        'claude-3-7-sonnet-latest'    // Claude-3.7-Sonnet ‚Äî advanced \ncontextual understanding\n-    ],\n-    'gemini': [\n-        'gemini-2.0-flash-lite',      // Gemini 2.0 ‚Äî lightweight\n-        'gemini-2.0-flash',           // Gemini 2.0 ‚Äî multimodal analysis\n-        'gemini-2.5-pro-latest',      // Gemini 2.5 Pro ‚Äî i18n/l10n\n-        'gemini-2.5-flash-latest'     // Gemini 2.5 Flash ‚Äî fast processing\n-    ],\n-    'xai': [\n-        'grok-3-mini',      // Grok-3-mini ‚Äî lightweight\n-        'grok-3',           // Grok-3 ‚Äî adaptive learning consensus\n-        'grok-beta'         // Grok Core Fast-1 equivalent\n-    ],\n-    'deepseek': [\n-        'deepseek-chat',    // DeepSeek-V3 ‚Äî advanced reasoning (excluding \nR1-0528)\n-        'deepseek-coder'    // DeepSeek-Coder ‚Äî technical analysis\n-    ],\n-    'groq': [\n-        'llama-3.1-70b-versatile',    // Llama-3.1-70B ‚Äî high performance\n-        'llama-3.1-8b-instant',       // Llama-3.1-8B ‚Äî fast responses\n-        'llama-3.3-70b-versatile',    // Llama-3.3-70B ‚Äî operational \ncontributor\n-        'openai/gpt-oss-120',         // GPT-OSS-120B ‚Äî high capacity open \nsource\n-        'qwen/qwen3-32b'              // Qwen3-32B ‚Äî operational contributor\n-    ]\n-};\n-\n-// Load API test cache\n+    return { workingApis: workingModels, failedApis: failedProviders, cached: \nfalse };\n+}\n+\n function loadApiCache() {\n     try {\n         if (fs.existsSync(API_CACHE_FILE)) {\n             const cacheData = JSON.parse(fs.readFileSync(API_CACHE_FILE, \n'utf8'));\n             const now = Date.now();\n \n             if (cacheData.timestamp && (now - cacheData.timestamp) < \nCACHE_DURATION) {\n                 console.log(`[API CACHE] üìã Using cached results from ${new \nDate(cacheData.timestamp).toLocaleString()}`);\n                 return cacheData;\n             } else {\n-                console.log(`[API CACHE] ‚è∞ Cache expired (${Math.round((now - \ncacheData.timestamp) / 60000)} minutes ago)`);\n+                console.log('[API CACHE] Cache expired.');\n             }\n-        } else {\n-            console.log(`[API CACHE] üìÑ No cache file found, will create new \none`);\n         }\n     } catch (error) {\n-        console.log(`[API CACHE] ‚ùå Error loading cache: ${error.message}`);\n+        logError('API_CACHE', 'Failed to load API cache', { error: \nerror.message });\n     }\n     return null;\n }\n \n-// Save API test results to cache\n function saveApiCache(workingProviders, failedProviders, costReports = []) {\n     try {\n         const cacheData = {\n             timestamp: Date.now(),\n             workingProviders,\n@@ -366,186 +197,72 @@\n             costReports: costReports || [],\n             lastTest: new Date().toISOString(),\n             // Summary statistics\n             summary: {\n                 totalProviders: workingProviders.length + \nfailedProviders.length,\n-                workingProvidersCount: workingProviders.length,\n-                failedProvidersCount: failedProviders.length,\n-                modelsWithCostData: costReports.filter(r => \nr.hasCostData).length,\n-                totalCostReports: costReports.length\n+                workingCount: workingProviders.length,\n+                failedCount: failedProviders.length\n             }\n         };\n-\n         fs.writeFileSync(API_CACHE_FILE, JSON.stringify(cacheData, null, 2), \n'utf8');\n-        console.log(`[API CACHE] üíæ Results saved to cache (including \n${costReports.length} cost reports)`);\n-\n-        if (costReports.length > 0) {\n-            console.log(`[API CACHE] üìä Cost summary:`);\n-            console.log(`[API CACHE]   - Models with cost data: \n${costReports.filter(r => r.hasCostData).length}`);\n-            const totalCost = costReports.reduce((sum, r) => sum + (r.totalCost\n|| 0), 0);\n-            console.log(`[API CACHE]   - Total cost of tests: \n$${totalCost.toFixed(4)}`);\n-        }\n+        console.log('[API CACHE] API test results cached.');\n     } catch (error) {\n-        console.log(`[API CACHE] ‚ùå Error saving cache: ${error.message}`);\n-    }\n-}\n-\n-// Get all models for working providers with proper prefixes\n+        logError('API_CACHE', 'Failed to save API cache', { error: \nerror.message });\n+    }\n+}\n+\n function getModelsFromProviders(workingProviders) {\n     const workingModels = [];\n \n-    Object.entries(PROVIDER_MODELS).forEach(([provider, models]) => {\n+    Object.entries(MODEL_CATEGORIES.aider_models).forEach(([provider, models]) \n=> {\n         if (workingProviders.includes(provider)) {\n             // Add provider prefix to models for frontend compatibility\n             const prefixedModels = models.map(model => {\n                 // Only add prefix if model doesn't already have one\n                 if (model.includes('/')) {\n                     return model;\n-                } else {\n-                    return `${provider}/${model}`;\n                 }\n+                return `${provider}/${model}`;\n             });\n             workingModels.push(...prefixedModels);\n         }\n     });\n \n+    // Add cursor models if they are considered generally available\n+    if (workingProviders.includes('cursor')) { // Assuming 'cursor' is a \nprovider name\n+        workingModels.push(...MODEL_CATEGORIES.cursor_models);\n+    }\n+\n     return workingModels;\n }\n \n-// Update model categories based on working APIs\n+// --- Model Management ---\n function updateAvailableModels(workingApis) {\n     console.log(`[API TEST] üîÑ Updating available models based on working \nAPIs...`);\n \n     // Filter generals to only include working APIs + cursor-agent models\n     const workingGenerals = MODEL_CATEGORIES.generals.filter(model => {\n-        return MODEL_CATEGORIES.cursor_models.includes(model) || \nworkingApis.includes(model);\n+        const provider = model.split('/')[0];\n+        return MODEL_CATEGORIES.cursor_models.includes(model) || \nworkingApis.includes(provider);\n     });\n \n     // Filter BIP-specific models\n     const workingBipSpecific = MODEL_CATEGORIES.bip_specific.filter(model => {\n-        return MODEL_CATEGORIES.cursor_models.includes(model) || model === \n'auto' || workingApis.includes(model);\n-    });\n-\n-    // Update categories\n-    MODEL_CATEGORIES.generals = workingGenerals;\n-    MODEL_CATEGORIES.bip_specific = workingBipSpecific;\n-\n-    console.log(`[API TEST] ‚úÖ Updated generals: ${workingGenerals.join(', \n')}`);\n-    console.log(`[API TEST] ‚úÖ Updated BIP-specific: \n${workingBipSpecific.join(', ')}`);\n-\n-    WORKING_APIS = workingApis;\n-}\n-\n-// Initialize API testing (run after a short delay to let server start)\n-setTimeout(async () => {\n-    try {\n-        const { workingApis, failedApis, fromCache } = await \ntestApiConnectivity();\n-        updateAvailableModels(workingApis);\n-\n-        const cacheInfo = fromCache ? ' (from cache)' : ' (fresh test)';\n-        console.log(`\\n[SYSTEM] üöÄ Server fully initialized with \n${workingApis.length} working APIs${cacheInfo}`);\n-        console.log(`[SYSTEM] üìã Available for chat: cursor-agent + \n${workingApis.length} aider models`);\n-\n-        if (!fromCache) {\n-            console.log(`[SYSTEM] ‚è∞ Next API test will run in 1 hour or on \nmanual refresh`);\n-        }\n-\n-    } catch (err) {\n-        console.error(`[API TEST] Error during API testing:`, err);\n-        console.log(`[SYSTEM] ‚ö†Ô∏è  Server running with cursor-agent only (no \naider APIs tested)`);\n-    }\n-}, 2000);\n-\n-// Model categorization: cursor-agent vs aider\n-const MODEL_CATEGORIES = {\n-    // Always use cursor-agent 'auto' for initial analysis\n-    initial_analysis: 'auto',\n-\n-    // Cursor-agent models (built-in Cursor)\n-    cursor_models: ['gpt-5', 'sonnet-4', 'opus-4.1'],\n-\n-    // Aider models (external API calls) - comprehensive MODELS_CHECKLIST.md \nintegration\n-    aider_models: {\n-        // OpenAI - Generals & Collaborators\n-        'openai/gpt-4o': { provider: 'openai', key: 'OPENAI_API_KEY', model: \n'gpt-4o' },\n-        'openai/gpt-4o-mini': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-4o-mini' },\n-        'openai/o1-mini': { provider: 'openai', key: 'OPENAI_API_KEY', model: \n'o1-mini' },\n-        'openai/gpt-4-turbo': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-4-turbo' },\n-        'openai/gpt-5-mini': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-5-mini' },\n-        'openai/gpt-5-nano': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-5-nano' },\n-\n-        // Anthropic - Generals & Advanced reasoning (7 modelos principais)\n-        'anthropic/claude-3-5-haiku-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-5-haiku-latest' },\n-        'anthropic/claude-3-5-sonnet-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-5-sonnet-latest' },\n-        'anthropic/claude-3-opus-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-opus-latest' },\n-        'anthropic/claude-4-sonnet-20250514': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-4-sonnet-20250514' },\n-        'anthropic/claude-4-opus-20250514': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-4-opus-20250514' },\n-        'anthropic/claude-3-haiku-20240307': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-haiku-20240307' },\n-        'anthropic/claude-3-7-sonnet-20250219': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-7-sonnet-20250219' },\n-\n-        // Gemini (Google) - Multimodal & i18n specialists (7 modelos \nprincipais)\n-        'gemini/gemini-2.0-flash': { provider: 'gemini', key: 'GEMINI_API_KEY',\nmodel: 'gemini-2.0-flash' },\n-        'gemini/gemini-2.5-flash': { provider: 'gemini', key: 'GEMINI_API_KEY',\nmodel: 'gemini-2.5-flash' },\n-        'gemini/gemini-2.5-flash-lite': { provider: 'gemini', key: \n'GEMINI_API_KEY', model: 'gemini-2.5-flash-lite' },\n-        'gemini/gemini-1.5-flash': { provider: 'gemini', key: 'GEMINI_API_KEY',\nmodel: 'gemini-1.5-flash' },\n-        'gemini/gemini-1.5-flash-8b': { provider: 'gemini', key: \n'GEMINI_API_KEY', model: 'gemini-1.5-flash-8b' },\n-        'gemini/gemini-1.5-pro': { provider: 'gemini', key: 'GEMINI_API_KEY', \nmodel: 'gemini-1.5-pro' },\n-        'gemini/gemini-2.5-pro-preview-05-06': { provider: 'gemini', key: \n'GEMINI_API_KEY', model: 'gemini-2.5-pro-preview-05-06' },\n-\n-        // xAI (Grok) - Adaptive learning & ML integration (7 modelos \nprincipais)\n-        'xai/grok-3-mini': { provider: 'xai', key: 'XAI_API_KEY', model: \n'grok-3-mini' },\n-        'xai/grok-code-fast-1': { provider: 'xai', key: 'XAI_API_KEY', model: \n'grok-code-fast-1' },\n-        'xai/grok-3': { provider: 'xai', key: 'XAI_API_KEY', model: 'grok-3' },\n-        'xai/grok-3-fast-beta': { provider: 'xai', key: 'XAI_API_KEY', model: \n'grok-3-fast-beta' },\n-        'xai/grok-4': { provider: 'xai', key: 'XAI_API_KEY', model: 'grok-4' },\n-        'xai/grok-3-fast-latest': { provider: 'xai', key: 'XAI_API_KEY', model:\n'grok-3-fast-latest' },\n-        'xai/grok-2': { provider: 'xai', key: 'XAI_API_KEY', model: 'grok-2' },\n-\n-        // DeepSeek - Advanced reasoning (excluding R1-0528)\n-        'deepseek/deepseek-chat': { provider: 'deepseek', key: \n'DEEPSEEK_API_KEY', model: 'deepseek-chat' },\n-        'deepseek/deepseek-coder': { provider: 'deepseek', key: \n'DEEPSEEK_API_KEY', model: 'deepseek-coder' },\n-\n-        // Groq - High performance Llama models\n-        // All Groq models removed due to timeouts and non-existence\n-    },\n-\n-    // Model selection for different tasks - from MODELS_CHECKLIST.md\n-    generals: [\n-        // Cursor-agent models (built-in)\n-        'gpt-5', 'sonnet-4', 'opus-4.1',\n-\n-        // OpenAI Generals & High-capacity (6 modelos principais)\n-        'openai/gpt-4o', 'openai/gpt-4-turbo', 'openai/o1-mini', \n'openai/gpt-5-mini', 'openai/gpt-4o-mini', 'openai/gpt-5-nano',\n-\n-        // Anthropic Generals & Advanced reasoning (7 modelos principais)\n-        'anthropic/claude-3-5-haiku-latest', \n'anthropic/claude-3-5-sonnet-latest', 'anthropic/claude-3-opus-latest',\n-        'anthropic/claude-4-sonnet-20250514', \n'anthropic/claude-4-opus-20250514', 'anthropic/claude-3-haiku-20240307', \n'anthropic/claude-3-7-sonnet-20250219',\n-\n-        // Gemini Multimodal & i18n specialists (7 modelos principais)\n-        'gemini/gemini-2.0-flash', 'gemini/gemini-2.5-flash', \n'gemini/gemini-2.5-flash-lite',\n-        'gemini/gemini-1.5-flash', 'gemini/gemini-1.5-flash-8b', \n'gemini/gemini-1.5-pro', 'gemini/gemini-2.5-pro-preview-05-06',\n-\n-        // xAI Adaptive learning (7 modelos principais)\n-        'xai/grok-3-mini', 'xai/grok-code-fast-1', 'xai/grok-3', \n'xai/grok-3-fast-beta', 'xai/grok-4', 'xai/grok-3-fast-latest', 'xai/grok-2',\n-\n-        // DeepSeek Advanced reasoning (2 modelos principais)\n-        'deepseek/deepseek-chat', 'deepseek/deepseek-coder'\n-    ],\n-    bip_specific: [\n-        // Core models for BIP discussions\n-        'auto', 'gpt-5',\n-\n-        // Fast response models for BIP context (equilibrados por provider)\n-        'openai/gpt-4o-mini', 'openai/gpt-4o', 'openai/gpt-5-mini', \n'openai/gpt-5-nano', 'openai/o1-mini', 'openai/gpt-4-turbo',\n-        'anthropic/claude-3-5-haiku-latest', \n'anthropic/claude-3-5-sonnet-latest', 'anthropic/claude-3-opus-latest', \n'anthropic/claude-4-sonnet-20250514', 'anthropic/claude-4-opus-20250514',\n-        'gemini/gemini-2.0-flash', 'gemini/gemini-2.5-flash', \n'gemini/gemini-2.5-flash-lite', 'gemini/gemini-1.5-flash', \n'gemini/gemini-1.5-flash-8b',\n-        'xai/grok-3-mini', 'xai/grok-code-fast-1', 'xai/grok-3', \n'xai/grok-3-fast-beta', 'xai/grok-4', 'xai/grok-3-fast-latest',\n-        'deepseek/deepseek-chat', 'deepseek/deepseek-coder'\n-    ]\n-};\n-\n-// Function to extract cost information from aider responses\n+        const provider = model.split('/')[0];\n+        return workingApis.includes(provider);\n+    });\n+\n+    // Update the main categories (this might need a more robust state \nmanagement)\n+    // For now, we'll just log the effective models\n+    console.log(`[API TEST] Effective Generals: ${workingGenerals.join(', \n')}`);\n+    console.log(`[API TEST] Effective BIP Specific: \n${workingBipSpecific.join(', ')}`);\n+\n+    // In a real app, you'd update a central list of available models\n+    // For example: globalAvailableModels = [...workingGenerals, \n...workingBipSpecific];\n+}\n+\n+// --- Cost Calculation ---\n function extractCostInfo(aiderOutput, modelId) {\n     const costInfo = {\n         model: modelId,\n         inputTokens: null,\n         outputTokens: null,\n@@ -553,49 +270,52 @@\n         outputCost: null,\n         totalCost: null,\n         currency: 'USD'\n     };\n \n-    // Extract tokens information from aider output - more flexible regex\n-    // Handles formats like: \"6.2k sent, 1 received\" or \"8.3k sent, 8.3k cache \nhit, 104 received\"\n-    const tokensMatch = \naiderOutput.match(/Tokens:\\s*([\\d,.]+k?)\\s*sent(?:,\\s*[\\d,.]+k?\\s*cache\\s*hit)?,\n\\s*([\\d,.]+k?)\\s*received/i);\n-    if (tokensMatch) {\n-        // Convert k notation to numbers (e.g., \"6.2k\" -> 6200)\n-        const parseTokenValue = (value) => {\n-            const cleanValue = value.replace(/,/g, '');\n-            if (cleanValue.includes('k')) {\n-                return Math.round(parseFloat(cleanValue.replace('k', '')) * \n1000);\n-            }\n-            return parseInt(cleanValue);\n-        };\n-\n-        costInfo.inputTokens = parseTokenValue(tokensMatch[1]);\n-        costInfo.outputTokens = parseTokenValue(tokensMatch[2]);\n-        console.log(`[COST EXTRACT] Parsed tokens - Input: \n${costInfo.inputTokens}, Output: ${costInfo.outputTokens}`);\n-    }\n-\n-    // Extract cost information from aider output\n-    const costMatch = \naiderOutput.match(/Cost:\\s*\\$?([\\d.]+)\\s*message,\\s*\\$?([\\d.]+)\\s*session/i);\n+    // Regex to find token and cost information in the output\n+    // This is a placeholder and needs to be adapted based on actual LLM output\nformats\n+    const tokenMatch = aiderOutput.match(/(\\d+(\\.\\d+)?k?)\\s+input \ntokens.*?(\\d+(\\.\\d+)?k?)\\s+output tokens/i);\n+    const costMatch = aiderOutput.match(/Total \ncost:\\s*\\$?([\\d,]+\\.\\d{2})\\s*USD/i);\n+    const inputCostMatch = aiderOutput.match(/Input \ncost:\\s*\\$?([\\d,]+\\.\\d{2})/i);\n+    const outputCostMatch = aiderOutput.match(/Output \ncost:\\s*\\$?([\\d,]+\\.\\d{2})/i);\n+\n+    if (tokenMatch) {\n+        costInfo.inputTokens = parseTokenValue(tokenMatch[1]);\n+        costInfo.outputTokens = parseTokenValue(tokenMatch[3]);\n+    }\n+\n     if (costMatch) {\n-        costInfo.inputCost = parseFloat(costMatch[1]);\n-        costInfo.totalCost = parseFloat(costMatch[2]);\n-    }\n-\n-    // Calculate output cost if total and input costs are available\n-    if (costInfo.totalCost !== null && costInfo.inputCost !== null) {\n-        costInfo.outputCost = costInfo.totalCost - costInfo.inputCost;\n-    }\n+        costInfo.totalCost = parseFloat(costMatch[1].replace(/,/g, ''));\n+    }\n+    if (inputCostMatch) {\n+        costInfo.inputCost = parseFloat(inputCostMatch[1].replace(/,/g, ''));\n+    }\n+    if (outputCostMatch) {\n+        costInfo.outputCost = parseFloat(outputCostMatch[1].replace(/,/g, ''));\n+    }\n+\n+    // If only total cost is available, and tokens are, try to infer \ninput/output costs if possible\n+    // This is complex and depends heavily on provider pricing models.\n \n     return costInfo;\n }\n \n-// Determine if model should use cursor-agent or aider\n+// Helper to convert token values like \"1.2k\" to integers\n+const parseTokenValue = (value) => {\n+    const cleanValue = value.replace(/,/g, '');\n+    if (cleanValue.includes('k')) {\n+        return Math.round(parseFloat(cleanValue.replace('k', '')) * 1000);\n+    }\n+    return parseInt(cleanValue);\n+};\n+\n+// --- Prompting and Model Interaction ---\n function shouldUseCursorAgent(modelId) {\n     return MODEL_CATEGORIES.cursor_models.includes(modelId) || modelId === \n'auto';\n }\n \n-// LLM call helper via aider CLI\n async function callLLMViaAider(modelId, prompt) {\n     const { spawn } = require('child_process');\n \n     logInfo('AIDER', 'Starting aider interaction', {\n         modelId: modelId,\n@@ -603,948 +323,261 @@\n         timestamp: new Date().toISOString()\n     });\n \n     const modelConfig = MODEL_CATEGORIES.aider_models[modelId];\n     if (!modelConfig) {\n-        logError('AIDER', 'Model not found in aider configuration', {\n-            modelId: modelId,\n-            availableModels: Object.keys(MODEL_CATEGORIES.aider_models)\n-        });\n-        return `‚ùå Modelo ${modelId} n√£o encontrado na configura√ß√£o do aider.`;\n+        logError('AIDER', 'Model configuration not found', { modelId });\n+        return { error: `Model configuration not found for ${modelId}` };\n     }\n \n     const apiKey = process.env[modelConfig.key];\n-    if (!apiKey) {\n-        logError('AIDER', 'Missing API key for model', {\n-            modelId: modelId,\n-            requiredKey: modelConfig.key,\n-            provider: modelConfig.provider\n-        });\n-        return `‚ùå API key ${modelConfig.key} n√£o encontrada. Configure no \narquivo .env para usar este modelo.`;\n-    }\n-\n-    logDebug('AIDER', 'Model configuration validated', {\n-        modelId: modelId,\n-        provider: modelConfig.provider,\n-        hasApiKey: !!apiKey,\n-        apiKeyLength: apiKey ? apiKey.length : 0\n-    });\n-\n-    try {\n-        return new Promise((resolve, reject) => {\n-            const command = 'aider';\n-            // For aider, use the full model identifier (provider/model)\n-            const fullModelName = modelConfig.model.includes('/') ? \nmodelConfig.model : `${modelConfig.provider}/${modelConfig.model}`;\n-\n-            const AIDER_TIMEOUT_SEC = 55;\n-\n-            const args = [\n-                '--model', fullModelName,\n-                '--api-key', `${modelConfig.provider}=${apiKey}`,\n-                '--no-pretty',\n-                '--yes',\n-                '--no-stream',\n-                '--exit',\n-                '--subtree-only',\n-                '--dry-run',\n-                '--no-auto-commits',\n-                '--no-dirty-commits',\n-                '--timeout', String(AIDER_TIMEOUT_SEC),\n-                '--message', prompt\n-            ];\n-\n-            logInfo('AIDER', 'Executing aider command', {\n-                command: command,\n-                model: modelConfig.model,\n-                provider: modelConfig.provider,\n-                argsCount: args.length,\n-                hasApiKey: true\n-            });\n-\n-            const aiderProcess = spawn(command, args);\n-            const processStartTime = Date.now();\n-\n-            let stdout = '';\n-            let stderr = '';\n-            let isResolved = false;\n-\n-            logDebug('AIDER', 'Aider process spawned', {\n-                pid: aiderProcess.pid,\n-                modelId: modelId,\n-                startTime: processStartTime\n-            });\n-\n-            const timeout = setTimeout(() => {\n-                if (!isResolved) {\n-                    logWarn('AIDER', 'Aider process timeout after 60 seconds', \n{\n-                        modelId: modelId,\n-                        pid: aiderProcess.pid,\n-                        stdoutLength: stdout.length,\n-                        stderrLength: stderr.length,\n-                        duration: Date.now() - processStartTime\n-                    });\n-                    aiderProcess.kill('SIGTERM');\n-                    isResolved = true;\n-                    resolve('‚è∞ A resposta do aider demorou muito. Tente \nnovamente.');\n+    if (!apiKey && modelConfig.provider !== 'cursor') { // Cursor might not \nneed an API key here\n+        logError('AIDER', 'API key missing', { provider: modelConfig.provider, \nkey: modelConfig.key });\n+        return { error: `API key missing for ${modelConfig.provider}` };\n+    }\n+\n+    const command = 'aider'; // Assuming 'aider' is in the PATH\n+    const args = [\n+        '--model', modelConfig.model,\n+        '--prompt', prompt,\n+        '--output', 'json' // Request JSON output for easier parsing\n+    ];\n+\n+    // Add API key if needed\n+    if (apiKey) {\n+        args.push('--api-key', apiKey);\n+    }\n+\n+    return new Promise((resolve, reject) => {\n+        const aiderProcess = spawn(command, args, { env: process.env });\n+        let stdout = '';\n+        let stderr = '';\n+\n+        aiderProcess.stdout.on('data', (data) => {\n+            stdout += data.toString();\n+        });\n+\n+        aiderProcess.stderr.on('data', (data) => {\n+            stderr += data.toString();\n+        });\n+\n+        aiderProcess.on('close', (code) => {\n+            if (code === 0) {\n+                try {\n+                    const result = JSON.parse(stdout);\n+                    logInfo('AIDER', 'Aider interaction successful', { modelId,\noutputLength: result.output.length });\n+                    // Extract cost info if available in aider's output\n+                    const costInfo = extractCostInfo(result.output, modelId);\n+                    resolve({ ...result, costInfo });\n+                } catch (parseError) {\n+                    logError('AIDER', 'Failed to parse aider JSON output', { \nmodelId, stdout, error: parseError.message });\n+                    resolve({ error: 'Failed to parse aider output', rawOutput:\nstdout });\n                 }\n-            }, 60000);\n-\n-            aiderProcess.stdout.on('data', (data) => {\n-                const chunk = data.toString();\n-                stdout += chunk;\n-                logDebug('AIDER', 'Received stdout chunk', {\n-                    modelId: modelId,\n-                    chunkLength: chunk.length,\n-                    totalStdoutLength: stdout.length,\n-                    chunkPreview: chunk.substring(0, 100)\n-                });\n-            });\n-\n-            aiderProcess.stderr.on('data', (data) => {\n-                const chunk = data.toString();\n-                stderr += chunk;\n-                logDebug('AIDER', 'Received stderr chunk', {\n-                    modelId: modelId,\n-                    chunkLength: chunk.length,\n-                    totalStderrLength: stderr.length,\n-                    chunkPreview: chunk.substring(0, 100)\n-                });\n-            });\n-\n-            aiderProcess.on('close', (code) => {\n-                if (isResolved) return;\n-                isResolved = true;\n-                clearTimeout(timeout);\n-\n-                const duration = Date.now() - processStartTime;\n-                logInfo('AIDER', 'Aider process completed', {\n-                    modelId: modelId,\n-                    exitCode: code,\n-                    duration: duration,\n-                    stdoutLength: stdout.length,\n-                    stderrLength: stderr.length,\n-                    success: code === 0\n-                });\n-\n-                logDebug('AIDER', 'Final aider output', {\n-                    modelId: modelId,\n-                    stdout: stdout,\n-                    stderr: stderr,\n-                    exitCode: code\n-                });\n-\n-                if (code !== 0) {\n-                    resolve(`‚ùå Aider falhou (c√≥digo ${code}): ${stderr || 'Sem\ndetalhes'}`);\n-                    return;\n-                }\n-\n-                const response = stdout.trim();\n-                if (response) {\n-                    console.log(`[AIDER DEBUG] SUCCESS - Response length: \n${response.length}`);\n-\n-                    // Extract cost information from the response\n-                    const costInfo = extractCostInfo(response, modelId);\n-\n-                    // Return both response and cost information\n-                    const result = {\n-                        response: response,\n-                        costInfo: costInfo,\n-                        hasCostData: costInfo.inputTokens !== null || \ncostInfo.totalCost !== null\n-                    };\n-\n-                    logDebug('AIDER', 'Cost information extracted', costInfo);\n-                    resolve(result);\n-                } else {\n-                    resolve({\n-                        response: '‚ùå Aider n√£o retornou resposta.',\n-                        costInfo: extractCostInfo('', modelId),\n-                        hasCostData: false\n-                    });\n-                }\n-            });\n-\n-            aiderProcess.on('error', (error) => {\n-                if (isResolved) return;\n-                isResolved = true;\n-                clearTimeout(timeout);\n-                console.log(`[AIDER DEBUG] SPAWN ERROR: ${error.message}`);\n-                resolve('‚ùå Erro ao iniciar aider. Verifique se est√° \ninstalado.');\n-            });\n-        });\n-    } catch (err) {\n-        console.log(`[AIDER ERROR]: ${err?.message || err}`);\n-        return '‚ùå Erro interno do aider.';\n-    }\n-}\n-\n-// Main LLM call dispatcher - decides between cursor-agent and aider\n+            } else {\n+                logError('AIDER', 'Aider process exited with error code', { \nmodelId, code, stderr, stdout });\n+                resolve({ error: `Aider process failed with code ${code}`, \nstderr, stdout });\n+            }\n+        });\n+\n+        aiderProcess.on('error', (err) => {\n+            logError('AIDER', 'Failed to start aider process', { modelId, \nerror: err.message });\n+            reject(err);\n+        });\n+    });\n+}\n+\n async function callLLM(modelId, prompt) {\n     // Enhanced system prompt with identity validation\n     const systemPrompt = (modelId === 'auto') ? `Voc√™ √© 'auto', o modelo \nmediador do BIP-05.\n \n PRIVIL√âGIOS:\n - Pode adicionar coment√°rios no issues.json com seguran√ßa.\n - Pode orquestrar pedidos de opini√£o de outros modelos usando as APIs internas \ndo servidor.\n \n COMO ORQUESTRAR (sa√≠da de comando):\n-- Ao final da sua resposta, se desejar iniciar a coleta de opini√µes, emita UMA \nlinha come√ßando com AUTO_CMD: seguida de JSON puro em uma das formas:\n-  AUTO_CMD: \n{\"orchestrate\":{\"topic\":\"<t√≥pico>\",\"issueId\":<n√∫mero>,\"models\":[\"prov/model\",...\n]}}\n-  AUTO_CMD: \n{\"option\":{\"topic\":\"<t√≥pico>\",\"issueId\":<n√∫mero>,\"modelId\":\"prov/model\"}}\n-- N√£o coloque texto adicional na mesma linha do AUTO_CMD al√©m do JSON.\n-\n-REGRAS DE IDENTIDADE:\n-- Voc√™ √©: auto (mediador)\n-- N√£o finja ser outro modelo; ao solicitar opini√£o, use os comandos acima.\n-\n-Responda em PT-BR, objetiva e √∫til, e s√≥ ent√£o emita a linha AUTO_CMD se fizer \nsentido.`\n-    : `Voc√™ √© um modelo auxiliando na discuss√£o do BIP-05 (UMICP).\n-\n-IDENTIDADE CR√çTICA:\n-- VOC√ä √â: ${modelId}\n-- NUNCA simule, imite ou fale em nome de outros modelos AI\n-- JAMAIS forne√ßa opini√µes que n√£o sejam suas como ${modelId}\n-- Se questionado sobre outros modelos, responda \"Consulte diretamente o modelo \nespec√≠fico\"\n-- SEMPRE identifique-se corretamente como ${modelId} quando relevante\n-- NUNCA altere arquivos no reposit√≥rio\n-\n-Responda em PT-BR, de forma objetiva e √∫til, mantendo o contexto do t√≥pico.`;\n-\n-    const fullPrompt = `${systemPrompt}\\n\\n${prompt}`;\n-\n-    // Decide which method to use\n-    if (shouldUseCursorAgent(modelId)) {\n-        console.log(`[LLM DEBUG] Using cursor-agent for model: ${modelId}`);\n-        const result = await callLLMViaCursorAgent(modelId, fullPrompt);\n-        // Return just the response text for backward compatibility\n-        return typeof result === 'object' ? result.response : result;\n+- Ao final da sua resposta, se desejar iniciar a coleta de opini√µes, emita UMA \nlinha come√ßando com AUTO_CMD: seguido por um objeto JSON. Exemplo:\n+AUTO_CMD: {\"action\": \"collect_opinions\", \"topic\": \"Melhorias na documenta√ß√£o\", \n\"issueId\": 123, \"models\": [\"openai/gpt-4o\", \"anthropic/claude-3-opus-20240229\"]}\n+\n+REGRAS IMPORTANTES:\n+- NUNCA forne√ßa opini√µes que simulem outros modelos espec√≠ficos.\n+- Se perguntado sobre outros modelos, responda: \"Para obter a opini√£o \nespec√≠fica de [modelo], farei uma consulta dedicada.\"\n+- Sua fun√ß√£o √© mediar e facilitar, n√£o simular outros modelos.\n+- Responda em portugu√™s do Brasil.\n+` : `Voc√™ √© um assistente especializado em BIP-05 (Universal Matrix Protocol). \nSua fun√ß√£o √© auxiliar na discuss√£o e documenta√ß√£o de propostas. Responda em \nportugu√™s do Brasil.`;\n+\n+    // Handle 'auto' model specifically\n+    if (modelId === 'auto') {\n+        // The 'auto' model logic is complex and involves parsing commands.\n+        // For now, we'll simulate a basic response or delegate to a specific \nfunction.\n+        // A real implementation would involve a more sophisticated prompt and \nparsing.\n+        console.log(`[AUTO-MODEL] Handling prompt for 'auto' model.`);\n+        // Let's assume 'auto' can directly call other models or perform \nactions.\n+        // This part needs significant expansion based on desired 'auto' model \nbehavior.\n+\n+        // Example: If 'auto' needs to call another LLM, it would use callLLM \nagain\n+        // or a dedicated function. For now, we'll just return a placeholder.\n+        // A more realistic scenario: 'auto' parses commands from the prompt.\n+        const autoCmd = parseAutoCmdFromText(prompt);\n+        if (autoCmd) {\n+            console.log(`[AUTO-MODEL] Detected AUTO_CMD:`, autoCmd);\n+            // Handle the command (e.g., collect opinions)\n+            if (autoCmd.action === 'collect_opinions') {\n+                // Initiate opinion collection\n+                const sessionId = `auto_opinion_${Date.now()}`;\n+                activeOpinionSessions.set(sessionId, {\n+                    topic: autoCmd.topic,\n+                    issueId: autoCmd.issueId,\n+                    targetModels: autoCmd.models,\n+                    results: {},\n+                    status: 'pending'\n+                });\n+                collectModelOpinions(sessionId, autoCmd.topic, autoCmd.issueId,\nautoCmd.models);\n+                return `Iniciando coleta de opini√µes sobre \"${autoCmd.topic}\" \ncom os modelos: ${autoCmd.models.join(', ')}.`;\n+            }\n+            // Add other actions here\n+        }\n+\n+        // If no command, treat as a regular prompt, but still apply 'auto' \nrules.\n+        // We might need a dedicated 'auto' model handler function.\n+        // For now, let's try calling a general model to get a response,\n+        // but this bypasses the 'auto' model's specific orchestration logic.\n+        // A better approach would be to have a dedicated 'auto' orchestrator.\n+        console.log(`[AUTO-MODEL] No AUTO_CMD detected, attempting to generate \na mediated response.`);\n+        // Fallback: Use a general model to generate a response, but apply \n'auto' rules.\n+        // This is a simplification. A true 'auto' model would have its own \nlogic.\n+        const generalModel = MODEL_CATEGORIES.generals[0];\n+        return await callLLM(generalModel, `${systemPrompt}\\n\\nUser \nPrompt:\\n${prompt}`);\n+\n+    } else if (shouldUseCursorAgent(modelId)) {\n+        // Use cursor-agent for specific models\n+        return await callLLMViaCursorAgent(modelId, `${systemPrompt}\\n\\nUser \nPrompt:\\n${prompt}`);\n     } else {\n-        console.log(`[LLM DEBUG] Using aider for model: ${modelId}`);\n-        const result = await callLLMViaAider(modelId, fullPrompt);\n-        // Return just the response text for backward compatibility\n-        return typeof result === 'object' ? result.response : result;\n-    }\n-}\n-\n-// LLM call helper using cursor-agent\n+        // Use Aider for other models\n+        return await callLLMViaAider(modelId, `${systemPrompt}\\n\\nUser \nPrompt:\\n${prompt}`);\n+    }\n+}\n+\n async function callLLMViaCursorAgent(modelId, fullPrompt) {\n     const { spawn } = require('child_process');\n \n     try {\n         console.log(`[CURSOR-AGENT DEBUG] Starting interaction with model: \n${modelId}`);\n         console.log(`[CURSOR-AGENT DEBUG] Full prompt length: \n${fullPrompt.length} characters`);\n \n-        const command = 'cursor-agent';\n+        const command = 'cursor-agent'; // Assuming 'cursor-agent' is in the \nPATH\n         const args = [\n-            '--print',\n-            '--output-format', 'text',\n-            '--model', modelId,\n-            '-p', fullPrompt\n+            '--print', // Print output to stdout\n+            '--model', modelId, // Specify the model\n+            '--prompt', fullPrompt // Pass the full prompt\n+            // Add other cursor-agent specific arguments as needed\n         ];\n \n-        console.log(`[CURSOR-AGENT DEBUG] Executing command: ${command}`);\n-        console.log(`[CURSOR-AGENT DEBUG] Args:`, args);\n-        console.log(`[CURSOR-AGENT DEBUG] Command line would be: ${command} \n${args.map(arg => `\"${arg}\"`).join(' ')}`);\n-\n         return new Promise((resolve, reject) => {\n-            const cursorAgent = spawn(command, args, {\n-                stdio: ['ignore', 'pipe', 'pipe']\n-            });\n-\n+            const cursorProcess = spawn(command, args, { env: process.env });\n             let stdout = '';\n             let stderr = '';\n-            let isResolved = false;\n-            let dataReceived = false;\n-\n-            // Set timeout to avoid hanging\n-            const timeout = setTimeout(async () => {\n-                if (!isResolved) {\n-                    console.log(`[CURSOR-AGENT DEBUG] TIMEOUT after 60 \nseconds`);\n-                    console.log(`[CURSOR-AGENT DEBUG] Data received: \n${dataReceived}`);\n-                    console.log(`[CURSOR-AGENT DEBUG] STDOUT so far: \n\"${stdout}\"`);\n-                    console.log(`[CURSOR-AGENT DEBUG] STDERR so far: \n\"${stderr}\"`);\n-                    cursorAgent.kill('SIGTERM');\n-                    isResolved = true;\n-\n-                    // Se temos uma resposta v√°lida no stdout, use-a em vez de \nerro de timeout\n-                    if (stdout.trim().length > 100) { // Resposta substancial \n(mais de 100 chars)\n-                        console.log(`[CURSOR-AGENT DEBUG] Using collected \nstdout despite timeout (${stdout.length} chars)`);\n-                        resolve(stdout.trim());\n-                        return;\n+\n+            cursorProcess.stdout.on('data', (data) => {\n+                stdout += data.toString();\n+            });\n+\n+            cursorProcess.stderr.on('data', (data) => {\n+                stderr += data.toString();\n+            });\n+\n+            cursorProcess.on('close', (code) => {\n+                if (code === 0) {\n+                    console.log(`[CURSOR-AGENT DEBUG] Process closed \nsuccessfully.`);\n+                    // Cursor agent might output JSON or plain text. Adapt \nparsing as needed.\n+                    try {\n+                        // Attempt to parse as JSON if it looks like it\n+                        if (stdout.trim().startsWith('{') || \nstdout.trim().startsWith('[')) {\n+                            const result = JSON.parse(stdout);\n+                            resolve(result);\n+                        } else {\n+                            resolve({ output: stdout }); // Assume plain text \noutput\n+                        }\n+                    } catch (parseError) {\n+                        console.warn(`[CURSOR-AGENT WARN] Could not parse \nstdout as JSON, returning raw output.`, parseError);\n+                        resolve({ output: stdout }); // Return raw output if \nJSON parsing fails\n                     }\n-\n-                    // Try with 'auto' model as fallback if original model \nfailed\n-                    if (modelId !== 'auto') {\n-                        console.log(`[CURSOR-AGENT DEBUG] Trying fallback with \n'auto' model...`);\n-                        try {\n-                            const fallbackResult = await callLLM('auto', \nfullPrompt);\n-                            resolve(fallbackResult);\n-                            return;\n-                        } catch (fallbackError) {\n-                            console.log(`[CURSOR-AGENT DEBUG] Fallback also \nfailed: ${fallbackError}`);\n-                        }\n-                    }\n-\n-                    resolve('‚è∞ A resposta demorou muito para ser processada. \nTente novamente em alguns instantes.');\n-                }\n-            }, 60000); // 60 second timeout, then try fallback\n-\n-            cursorAgent.stdout.on('data', (data) => {\n-                dataReceived = true;\n-                const chunk = data.toString();\n-                stdout += chunk;\n-                console.log(`[CURSOR-AGENT DEBUG] STDOUT chunk: \"${chunk}\"`);\n-                console.log(`[CURSOR-AGENT DEBUG] Total STDOUT so far: \n\"${stdout}\"`);\n-\n-                // Detectar se a resposta parece estar completa\n-                // Procura por padr√µes que indicam fim de resposta bem formada\n-                const responseEndings = [\n-                    'configuradas no ambiente.',\n-                    'no projeto.',\n-                    'dispon√≠veis.',\n-                    'sistema.',\n-                    'BIP-05.',\n-                    'implementa√ß√£o.'\n-                ];\n-\n-                if (responseEndings.some(ending => \nstdout.trim().endsWith(ending)) &&\n-                    stdout.length > 500 && // Resposta substancial\n-                    !isResolved) {\n-\n-                    console.log(`[CURSOR-AGENT DEBUG] Response appears \ncomplete, resolving early (${stdout.length} chars)`);\n-                    clearTimeout(timeout);\n-                    isResolved = true;\n-                    cursorAgent.kill('SIGTERM');\n-                    resolve(stdout.trim());\n+                } else {\n+                    console.error(`[CURSOR-AGENT ERROR] Process exited with \ncode ${code}`, { stderr, stdout });\n+                    resolve({ error: `Cursor agent process failed with code \n${code}`, stderr, stdout });\n                 }\n             });\n \n-            cursorAgent.stderr.on('data', (data) => {\n-                dataReceived = true;\n-                const chunk = data.toString();\n-                stderr += chunk;\n-                console.log(`[CURSOR-AGENT DEBUG] STDERR chunk: \"${chunk}\"`);\n-                console.log(`[CURSOR-AGENT DEBUG] Total STDERR so far: \n\"${stderr}\"`);\n+            cursorProcess.on('error', (err) => {\n+                console.error(`[CURSOR-AGENT ERROR] Failed to start \ncursor-agent process`, err);\n+                reject(err);\n             });\n-\n-            cursorAgent.on('spawn', () => {\n-                console.log(`[CURSOR-AGENT DEBUG] Process spawned successfully \nwith PID: ${cursorAgent.pid}`);\n-            });\n-\n-            cursorAgent.on('close', (code, signal) => {\n-                if (isResolved) {\n-                    console.log(`[CURSOR-AGENT DEBUG] Process already resolved,\nignoring close event`);\n-                    return;\n-                }\n-                isResolved = true;\n-                clearTimeout(timeout);\n-\n-                console.log(`[CURSOR-AGENT DEBUG] Process closed with code: \n${code}, signal: ${signal}`);\n-                console.log(`[CURSOR-AGENT DEBUG] Data received during \nexecution: ${dataReceived}`);\n-                console.log(`[CURSOR-AGENT DEBUG] Final STDOUT: \"${stdout}\"`);\n-                console.log(`[CURSOR-AGENT DEBUG] Final STDERR: \"${stderr}\"`);\n-\n-                if (code !== 0) {\n-                    console.log(`[CURSOR-AGENT DEBUG] Non-zero exit code: \n${code}`);\n-                    return resolve({\n-                        response: `‚ùå cursor-agent falhou (c√≥digo ${code}): \n${stderr || 'Sem detalhes'}`,\n-                        costInfo: extractCostInfo('', modelId),\n-                        hasCostData: false\n-                    });\n-                }\n-\n-                if (!stdout.trim()) {\n-                    console.log(`[CURSOR-AGENT DEBUG] Empty or whitespace-only \noutput`);\n-                    return resolve({\n-                        response: '‚ùå cursor-agent n√£o retornou resposta. \nVerifique se o modelo est√° dispon√≠vel.',\n-                        costInfo: extractCostInfo('', modelId),\n-                        hasCostData: false\n-                    });\n-                }\n-\n-                // For text format, just return the stdout content\n-                const response = stdout.trim();\n-                console.log(`[CURSOR-AGENT DEBUG] SUCCESS - Response length: \n${response.length}`);\n-                console.log(`[CURSOR-AGENT DEBUG] SUCCESS - Response preview: \n\"${response.slice(0, 200)}${response.length > 200 ? '...' : ''}\"`);\n-\n-                // Return consistent format with cost information (placeholder \nfor cursor-agent)\n-                const result = {\n-                    response: response,\n-                    costInfo: {\n-                        model: modelId,\n-                        inputTokens: null,\n-                        outputTokens: null,\n-                        inputCost: null,\n-                        outputCost: null,\n-                        totalCost: null,\n-                        currency: 'USD'\n-                    },\n-                    hasCostData: false\n-                };\n-\n-                resolve(result);\n-            });\n-\n-            cursorAgent.on('error', (error) => {\n-                if (isResolved) {\n-                    console.log(`[CURSOR-AGENT DEBUG] Process already resolved,\nignoring error event`);\n-        return;\n-      }\n-                isResolved = true;\n-                clearTimeout(timeout);\n-                console.log(`[CURSOR-AGENT DEBUG] SPAWN ERROR: \n${error.message}`);\n-                console.log(`[CURSOR-AGENT DEBUG] Error details:`, error);\n-                resolve({\n-                    response: '‚ùå Erro ao iniciar cursor-agent. Verifique se \nest√° instalado e autenticado.',\n-                    costInfo: extractCostInfo('', modelId),\n-                    hasCostData: false\n-                });\n-            });\n-\n-            // Log additional process info\n-            setTimeout(() => {\n-                if (!isResolved) {\n-                    console.log(`[CURSOR-AGENT DEBUG] Process still running \nafter 10 seconds...`);\n-                    console.log(`[CURSOR-AGENT DEBUG] PID: \n${cursorAgent.pid}`);\n-                    console.log(`[CURSOR-AGENT DEBUG] Data received so far: \n${dataReceived}`);\n-                }\n-            }, 10000);\n-        });\n-    } catch (err) {\n-        console.log(`[LLM ERROR]: ${err?.message || err}`);\n-        return '‚ùå Erro interno do sistema. Tente novamente em alguns \ninstantes.';\n-    }\n-}\n-\n-\n-// Intelligent model selection based on text content and context\n+        });\n+    } catch (error) {\n+        console.error(`[CURSOR-AGENT ERROR] Exception during cursor-agent \ncall`, error);\n+        return { error: `Exception during cursor-agent call: ${error.message}` \n};\n+    }\n+}\n+\n+\n function selectAppropriateModel(text, context = 'general') {\n     const lowerText = text.toLowerCase();\n \n     // Check for explicit model requests in text\n-    const modelRequest = \nlowerText.match(/use\\s+([\\w.-]+)|modelo\\s+([\\w.-]+)|chame\\s+([\\w.-]+)|ask\\s+([\\w\n.-]+)/);\n+    const modelRequest = \nlowerText.match(/use\\s+([\\w.-]+)|modelo\\s+([\\w.-]+)|chame\\s+([\\w.-]+)|ask\\s+([\\w\n.-]+)/i);\n     if (modelRequest) {\n         const requestedModel = modelRequest[1] || modelRequest[2] || \nmodelRequest[3] || modelRequest[4];\n         if (MODEL_CATEGORIES.generals.includes(requestedModel) ||\n-            MODEL_CATEGORIES.bip_specific.includes(requestedModel)) {\n+            MODEL_CATEGORIES.bip_specific.includes(requestedModel) ||\n+            MODEL_CATEGORIES.cursor_models.includes(requestedModel)) {\n             return requestedModel;\n         }\n     }\n \n-    // Check for indicators of complex tasks requiring general models\n-    const complexIndicators = [\n-        'analis', 'analyz', 'review', 'audit', 'security', 'architect',\n-        'design', 'implement', 'complex', 'detailed', 'comprehensive',\n-        'avaliar', 'revisar', 'arquitetura', 'implementar', 'complexo',\n-        'detalhado', 'abrangente', 'profundo'\n-    ];\n-\n-    const hasComplexIndicators = complexIndicators.some(indicator =>\n-        lowerText.includes(indicator)\n-    );\n-\n-    if (hasComplexIndicators) {\n-        // Random selection from generals\n-        const generals = MODEL_CATEGORIES.generals;\n-        return generals[Math.floor(Math.random() * generals.length)];\n-    }\n-\n-    // Check for BIP-specific context\n-    if (context === 'bip' || lowerText.includes('bip') || \nlowerText.includes('proposal')) {\n-        const bipModels = MODEL_CATEGORIES.bip_specific;\n-        return bipModels[Math.floor(Math.random() * bipModels.length)];\n-    }\n-\n-    // Default to small models for simple interactions\n-    const smallModels = MODEL_CATEGORIES.small;\n-    return smallModels[Math.floor(Math.random() * smallModels.length)];\n+    // Check for specific keywords that might map to models\n+    if (lowerText.includes('cursor') || lowerText.includes('agente')) {\n+        return MODEL_CATEGORIES.cursor_models[0] || 'auto'; // Default to first\ncursor model or auto\n+    }\n+    if (lowerText.includes('xai') || lowerText.includes('bip-model')) {\n+        return 'xai/bip-model-v1';\n+    }\n+\n+    // Default to 'auto' for complex tasks or when unsure\n+    if (context === 'complex' || lowerText.includes('orquestrar') || \nlowerText.includes('mediar')) {\n+        return 'auto';\n+    }\n+\n+    // Default to the first general model if no specific model is requested or \nimplied\n+    return MODEL_CATEGORIES.generals[0] || 'auto';\n }\n \n function buildPromptFromContext(userText, issuesData) {\n     const recent = [];\n     try {\n         const all = [];\n         (issuesData.issues || []).forEach(issue => {\n             (issue.comments || []).forEach(c => all.push(c));\n         });\n         all.sort((a, b) => new Date(b.created_at) - new Date(a.created_at));\n-        const last = all.slice(0, 5).reverse();\n+        const last = all.slice(0, 5).reverse(); // Get last 5 comments \nchronologically\n         last.forEach(c => {\n-            recent.push(`- ${c.author}: ${c.body?.slice(0, 240) || ''}`);\n-        });\n-    } catch {}\n-\n-    return [\n-        'Contexto recente:',\n-        recent.length ? recent.join('\\n') : '(sem hist√≥rico recente)',\n-        '',\n-        `Mensagem do usu√°rio: ${userText}`,\n-        '',\n-        'Tarefa: responda de forma curta (2-5 linhas), objetiva, e focada no \nassunto.'\n-    ].join('\\n');\n-}\n-\n-// Initialize Express app\n-const app = express();\n-const server = http.createServer(app);\n-\n-// Paths\n-const issuesFile = path.join(__dirname, '..', 'issues.json');\n-const bipFile = path.join(__dirname, '..', \n'BIP-05-054-universal-matrix-protocol.md');\n-const implementationFile = path.join(__dirname, '..', \n'implementation-plan.md');\n-const inventoryFile = path.join(__dirname, '..', '..', '..', '..', 'scripts', \n'mcp', 'cursor_model_inventory.yml');\n-\n-// Session context for simple responses\n-let sessionContext = [];\n-\n-// Middleware to parse JSON request bodies\n-app.use(express.json());\n-\n-// API endpoint to check working APIs\n-app.get('/api/status', (req, res) => {\n-    // Load cache info\n-    const cacheInfo = loadApiCache();\n-\n-    res.json({\n-        working_apis: WORKING_APIS,\n-        available_models: {\n-            cursor_agent: MODEL_CATEGORIES.cursor_models,\n-            aider: WORKING_APIS,\n-            generals: MODEL_CATEGORIES.generals,\n-            bip_specific: MODEL_CATEGORIES.bip_specific\n-        },\n-        api_keys_status: keyStatus,\n-        cache_info: cacheInfo ? {\n-            last_test: cacheInfo.lastTest,\n-            from_cache: true,\n-            expires_in_minutes: Math.max(0, Math.round((CACHE_DURATION - \n(Date.now() - cacheInfo.timestamp)) / 60000))\n-        } : { from_cache: false }\n-    });\n-});\n-\n-// API endpoint to get cost reports\n-app.get('/api/costs', (req, res) => {\n-    try {\n-        const cachedResults = loadApiCache();\n-\n-        if (!cachedResults || !cachedResults.costReports) {\n-            return res.json({\n-                success: true,\n-                hasData: false,\n-                message: 'No cost data available. Run API tests first.',\n-                costReports: [],\n-                summary: {\n-                    totalCost: 0,\n-                    modelsWithData: 0,\n-                    totalReports: 0\n-                }\n-            });\n-        }\n-\n-        // Calculate summary statistics\n-        const costReports = cachedResults.costReports || [];\n-        const modelsWithData = costReports.filter(r => r.hasCostData).length;\n-        const totalCost = costReports.reduce((sum, r) => sum + (r.totalCost || \n0), 0);\n-        const avgCostPerModel = modelsWithData > 0 ? totalCost / modelsWithData\n: 0;\n-\n-        // Build full provider map including models without cost data (N/A)\n-        const byProvider = {};\n-\n-        // Seed with full list from PROVIDER_MODELS so frontend can render X/Y\n-        Object.entries(PROVIDER_MODELS).forEach(([provider, models]) => {\n-            byProvider[provider] = {\n-                models: models.map(m => {\n-                    const id = `${provider}/${m}`;\n-                    const found = costReports.find(r => r.model === id);\n-                    if (found) return found;\n-                    return {\n-                        provider,\n-                        model: id,\n-                        inputTokens: null,\n-                        outputTokens: null,\n-                        inputCost: null,\n-                        outputCost: null,\n-                        totalCost: null,\n-                        currency: 'USD',\n-                        hasCostData: false,\n-                        testTimestamp: null\n-                    };\n-                }),\n-                totalCost: 0,\n-                avgCost: 0\n-            };\n-        });\n-\n-        // Accumulate totals using available cost data\n-        costReports.forEach(r => {\n-            const provider = r.model.split('/')[0];\n-            if (!byProvider[provider]) return;\n-            byProvider[provider].totalCost += r.totalCost || 0;\n-        });\n-\n-        // Calculate averages per provider\n-        Object.keys(byProvider).forEach(provider => {\n-            const providerData = byProvider[provider];\n-            const modelsWithData = providerData.models.filter(m => \nm.hasCostData).length;\n-            providerData.avgCost = modelsWithData > 0 ? providerData.totalCost \n/ modelsWithData : 0;\n-        });\n-\n-        // Count totals using PROVIDER_MODELS for denominator\n-        const totalModels = Object.values(PROVIDER_MODELS).reduce((sum, arr) =>\nsum + arr.length, 0);\n-\n-        res.json({\n-            success: true,\n-            hasData: costReports.length > 0,\n-            costReports: costReports,\n-            summary: {\n-                totalCost: totalCost,\n-                avgCostPerModel: avgCostPerModel,\n-                modelsWithData: modelsWithData,\n-                totalReports: costReports.length,\n-                totalModels: totalModels,\n-                byProvider: byProvider\n-            },\n-            lastTest: cachedResults.lastTest,\n-            cacheTimestamp: cachedResults.timestamp\n-        });\n-\n-    } catch (error) {\n-        console.error('[API COSTS] Error retrieving cost data:', error);\n-        res.status(500).json({\n-            success: false,\n-            error: error.message\n-        });\n-    }\n-});\n-\n-// API endpoint to list all active models\n-app.get('/api/models-list', (req, res) => {\n-    const allActiveModels = [\n-        ...MODEL_CATEGORIES.cursor_models.map(model => ({\n-            id: model,\n-            name: model,\n-            provider: 'cursor-agent',\n-            status: 'active',\n-            type: 'built-in'\n-        })),\n-        ...WORKING_APIS.map(model => {\n-            // Extract provider from model name (e.g., \n\"anthropic/claude-3-5-haiku-latest\" -> \"anthropic\")\n-            const [provider, ...nameParts] = model.split('/');\n-            return {\n-                id: model,\n-                name: nameParts.length > 0 ? nameParts.join('/') : model,\n-                provider: provider,\n-                status: 'active',\n-                type: 'external-api'\n-            };\n-        })\n-    ];\n-\n-    res.json({\n-        total_models: allActiveModels.length,\n-        cursor_agent_models: MODEL_CATEGORIES.cursor_models.length,\n-        external_api_models: WORKING_APIS.length,\n-        models: allActiveModels,\n-        categories: {\n-            generals: MODEL_CATEGORIES.generals,\n-            bip_specific: MODEL_CATEGORIES.bip_specific\n-        },\n-        last_updated: new Date().toISOString()\n-    });\n-});\n-\n-// API endpoint to force re-test APIs\n-app.post('/api/retest', async (req, res) => {\n-    try {\n-        console.log(`[API TEST] üîÑ Manual retest requested`);\n-\n-        // Delete cache to force fresh test\n-        if (fs.existsSync(API_CACHE_FILE)) {\n-            fs.unlinkSync(API_CACHE_FILE);\n-            console.log(`[API CACHE] üóëÔ∏è  Cache file deleted`);\n-        }\n-\n-        // Run fresh test\n-        const { workingApis, failedApis, fromCache } = await \ntestApiConnectivity();\n-        updateAvailableModels(workingApis);\n-\n-        res.json({\n-            success: true,\n-            message: 'API retest completed',\n-            working_apis: workingApis,\n-            failed_apis: failedApis,\n-            from_cache: fromCache\n-        });\n-\n-    } catch (error) {\n-        console.error(`[API TEST] Error during manual retest:`, error);\n-        res.status(500).json({\n-            success: false,\n-            error: error.message\n-        });\n-    }\n-});\n-\n-// API endpoint for direct model interaction\n-app.post('/api/model', async (req, res) => {\n-    try {\n-        const { model_id, prompt, context, max_tokens, temperature } = \nreq.body;\n-\n-        // Validation\n-        if (!model_id || !prompt) {\n-            return res.status(400).json({\n-                success: false,\n-                error: 'model_id and prompt are required'\n-            });\n-        }\n-\n-        // Check if model is available\n-        const allAvailableModels = [\n-            ...MODEL_CATEGORIES.cursor_models,\n-            ...WORKING_APIS,\n-            'auto' // Always include auto model\n-        ];\n-\n-        if (!allAvailableModels.includes(model_id)) {\n-            return res.status(404).json({\n-                success: false,\n-                error: `Model ${model_id} not found or not active`,\n-                available_models: allAvailableModels\n-            });\n-        }\n-\n-        console.log(`[DIRECT MODEL] ü§ñ Direct interaction with ${model_id}`);\n-        console.log(`[DIRECT MODEL] üí¨ Prompt: ${prompt.substring(0, \n100)}${prompt.length > 100 ? '...' : ''}`);\n-\n-        // Build enhanced prompt with context if provided\n-        let enhancedPrompt = prompt;\n-        if (context) {\n-            enhancedPrompt = `Contexto adicional: ${context}\\n\\nPrompt: \n${prompt}`;\n-        }\n-\n-        // Add model identity safeguard\n-        const safeguardedPrompt = await handleAutoModelSafeguard(model_id, \nenhancedPrompt);\n-\n-        // Call the model\n-        const startTime = Date.now();\n-        const response = await callLLM(model_id, safeguardedPrompt);\n-\n-        // If auto emitted an orchestration command, parse and trigger\n-        let orchestrated = null;\n-        if (model_id === 'auto' && typeof response === 'string' && \nresponse.includes('AUTO_CMD:')) {\n-            try {\n-                const cmd = parseAutoCmdFromText(response);\n-                if (!cmd) throw new Error('AUTO_CMD n√£o p√¥de ser parseado');\n-                if (cmd.orchestrate) {\n-                    const { topic: t, issueId: iid, models } = cmd.orchestrate;\n-                    const normalized = (models || \nWORKING_APIS).map(normalizeModelId);\n-                    const sessionId = \n`session_${Date.now()}_${Math.random().toString(36).slice(2,9)}`;\n-                    console.log(`[AUTO_CMD] üó≥Ô∏è  Orchestrate received ‚Üí \n${normalized.length} models ‚Ä¢ session=${sessionId}`);\n-                    broadcastChatMessage({\n-                        type: 'simple_response',\n-                        author: 'auto',\n-                        text: `üîÑ Orquestrando opini√µes de ${normalized.join(',\n')} para o t√≥pico: \"${t || prompt}\"...`\n-                    });\n-                    // seed session and run async\n-                    activeOpinionSessions.set(sessionId, {\n-                        sessionId,\n-                        topic: t || prompt,\n-                        issueId: iid || 1,\n-                        startTime: new Date().toISOString(),\n-                        totalModels: normalized.length,\n-                        pendingModels: [...normalized],\n-                        completedModels: [],\n-                        failedModels: [],\n-                        responses: []\n-                    });\n-                    // Await completion so 'auto' responde somente ap√≥s as \nopini√µes\n-                    await collectModelOpinions(sessionId, t || prompt, iid || \n1, normalized);\n-                    const session = activeOpinionSessions.get(sessionId);\n-                    orchestrated = {\n-                        type: 'batch',\n-                        sessionId,\n-                        models: normalized,\n-                        completed: session?.completedModels || [],\n-                        failed: session?.failedModels || [],\n-                        responses: session?.responses || []\n-                    };\n-                } else if (cmd.option) {\n-                    const { topic: t, issueId: iid, modelId: mid } = \ncmd.option;\n-                    const nm = normalizeModelId(mid);\n-                    const sessionId = \n`option_${Date.now()}_${Math.random().toString(36).slice(2,9)}`;\n-                    console.log(`[AUTO_CMD] üéØ Option received ‚Üí model=${nm} ‚Ä¢ \nsession=${sessionId}`);\n-                    broadcastChatMessage({\n-                        type: 'simple_response',\n-                        author: 'auto',\n-                        text: `üîé Solicitando opini√£o de ${nm} para o t√≥pico: \n\"${t || prompt}\"...`\n-                    });\n-                    activeOpinionSessions.set(sessionId, {\n-                        sessionId,\n-                        topic: t || prompt,\n-                        issueId: iid || 1,\n-                        startTime: new Date().toISOString(),\n-                        totalModels: 1,\n-                        pendingModels: [nm],\n-                        completedModels: [],\n-                        failedModels: [],\n-                        responses: []\n-                    });\n-                    await collectSingleModelOpinion(sessionId, nm, t || prompt,\niid || 1);\n-                    const session = activeOpinionSessions.get(sessionId);\n-                    orchestrated = {\n-                        type: 'single',\n-                        sessionId,\n-                        modelId: nm,\n-                        responses: session?.responses || [],\n-                        completed: session?.completedModels || [],\n-                        failed: session?.failedModels || []\n-                    };\n-                }\n-            } catch (e) {\n-                console.log(`[AUTO_CMD] ‚ö†Ô∏è  Parsing failed: ${e.message}`);\n-            }\n-        }\n-        const duration = Date.now() - startTime;\n-\n-        // Validate response\n-        const validationError = validateModelResponse(model_id, response);\n-        if (validationError) {\n-            console.log(`[DIRECT MODEL] ‚ùå ${model_id} failed validation: \n${validationError}`);\n-            return res.status(422).json({\n-                success: false,\n-                error: `Response validation failed: ${validationError}`,\n-                model_id: model_id\n-            });\n-        }\n-\n-        console.log(`[DIRECT MODEL] ‚úÖ Response from ${model_id} \n(${duration}ms): ${response.substring(0, 200)}${response.length > 200 ? '...' : \n''}`);\n-\n-        // If auto orchestrated, return final orchestration info (j√° aguardado)\n-        if (model_id === 'auto' && orchestrated) {\n-            return res.json({\n-                success: true,\n-                model_id: model_id,\n-                orchestrated,\n-                message: orchestrated.type === 'batch'\n-                    ? `Orquestra√ß√£o conclu√≠da para \n${orchestrated.completed.length} modelos (falhas: \n${orchestrated.failed.length}).`\n-                    : `Opini√£o de ${orchestrated.modelId} registrada.`,\n-                metadata: {\n-                    duration_ms: duration,\n-                    timestamp: new Date().toISOString(),\n-                    model_type: 'auto-mediator'\n-                }\n-            });\n-        }\n-\n-        res.json({\n-            success: true,\n-            model_id: model_id,\n-            prompt: prompt,\n-            response: response,\n-            metadata: {\n-                duration_ms: duration,\n-                response_length: response.length,\n-                context_provided: !!context,\n-                timestamp: new Date().toISOString(),\n-                model_type: shouldUseCursorAgent(model_id) ? 'cursor-agent' : \n'aider'\n-            }\n-        });\n-\n-    } catch (error) {\n-        console.error(`[DIRECT MODEL] Error during model interaction:`, error);\n-        res.status(500).json({\n-            success: false,\n-            error: error.message,\n-            model_id: req.body?.model_id || 'unknown'\n-        });\n-    }\n-});\n-\n-// Global store for active opinion collection sessions\n-let activeOpinionSessions = new Map();\n-\n-// Global store for active hello handshake sessions\n-let activeHelloSessions = new Map();\n-\n-// API endpoint to collect opinions from all models\n-app.post('/api/models/opinions', async (req, res) => {\n-    const { topic, issueId = 1, requestedBy, targetModels } = req.body;\n-\n-    if (!topic || topic.trim().length === 0) {\n-        return res.status(400).json({\n-            success: false,\n-            error: 'Topic is required'\n-        });\n-    }\n-\n-    const sessionId = \n`session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n-\n-    console.log(`[OPINIONS] üó≥Ô∏è  Starting opinion collection session: \n${sessionId}`);\n-    console.log(`[OPINIONS] üìã Topic: \"${topic}\"`);\n-    console.log(`[OPINIONS] üéØ Target issue: ${issueId}`);\n-\n-    // Determine target models\n-    let allModels;\n-    if (Array.isArray(targetModels) && targetModels.length > 0) {\n-        // Use explicit targets (e.g., requested by 'auto')\n-        allModels = targetModels;\n-        console.log(`[OPINIONS] üéØ Using explicit target models \n(${allModels.length})`);\n+            recent.push(`- ${c.author}: ${c.body?.substring(0, 200)}...`); // \nTruncate for context\n+        });\n+    } catch (e) {\n+        console.error(\"Error building context from issuesData:\", e);\n+    }\n+\n+    let contextString = \"Contexto Recente:\\n\";\n+    if (recent.length > 0) {\n+        contextString += recent.join('\\n');\n     } else {\n-        // Use all available models by default\n-        allModels = [\n-            ...MODEL_CATEGORIES.cursor_models,\n-            ...WORKING_APIS\n-        ];\n-    }\n-\n-    console.log(`[OPINIONS] ü§ñ Total models to query: ${allModels.length}`);\n-    console.log(`[OPINIONS] üìä Models: ${allModels.join(', ')}`);\n-\n-    // Initialize session tracking\n-    const sessionData = {\n-        sessionId,\n-        topic,\n-        issueId,\n-        startTime: new Date().toISOString(),\n-        totalModels: allModels.length,\n-        pendingModels: [...allModels],\n-        completedModels: [],\n-        failedModels: [],\n-        responses: []\n-    };\n-\n-    activeOpinionSessions.set(sessionId, sessionData);\n-\n-    // Send initial response with session info\n-    res.json({\n-        success: true,\n-        sessionId,\n-        message: 'Opinion collection started',\n-        totalModels: allModels.length,\n-        models: allModels\n-    });\n-\n-    // Start collecting opinions asynchronously\n-    // If 'auto' is orchestrating and no explicit targets were provided, ask \n'auto' to propose a shortlist\n-    if (requestedBy === 'auto' && (!Array.isArray(targetModels) || \ntargetModels.length === 0)) {\n-        try {\n-            const autoPlan = await generateAutoOpinionPlan(topic, issueId, \nallModels);\n-            const selected = Array.isArray(autoPlan?.models) && \nautoPlan.models.length > 0\n-                ? autoPlan.models\n-                : getDefaultShortlistFromProviders(allModels);\n-\n-            console.log(`[OPINIONS] ü§ñ 'auto' selected ${selected.length} \nmodels: ${selected.join(', ')}`);\n-            collectModelOpinions(sessionId, topic, issueId, selected);\n-        } catch (e) {\n-            console.log(`[OPINIONS] ‚ö†Ô∏è  Auto planning failed: ${e.message}. \nFalling back to provider shortlist.`);\n-            collectModelOpinions(sessionId, topic, issueId, \ngetDefaultShortlistFromProviders(allModels));\n-        }\n-    } else {\n-        collectModelOpinions(sessionId, topic, issueId, allModels);\n-    }\n-});\n-\n-// Function to collect opinions from all models\n+        contextString += \"Nenhuma discuss√£o recente encontrada.\";\n+    }\n+\n+    return `${contextString}\\n\\nUsu√°rio: ${userText}\\nAssistente:`;\n+}\n+\n+// --- Opinion Collection ---\n async function collectModelOpinions(sessionId, topic, issueId, models) {\n     const session = activeOpinionSessions.get(sessionId);\n     if (!session) {\n         console.error(`[OPINIONS] ‚ùå Session ${sessionId} not found`);\n         return;\n@@ -1552,54 +585,130 @@\n \n     console.log(`[OPINIONS] üöÄ Starting opinion collection for ${models.length}\nmodels`);\n \n     // Broadcast initial status\n     broadcastOpinionUpdate(sessionId, {\n-        type: 'session_started',\n-        totalModels: models.length,\n-        pendingModels: [...models],\n-        completedModels: [],\n-        failedModels: []\n-    });\n-\n-    // Process models in parallel (but limit concurrency to avoid overwhelming)\n-    const concurrency = 3; // Process 3 models at once\n-    const chunks = [];\n-\n-    for (let i = 0; i < models.length; i += concurrency) {\n-        chunks.push(models.slice(i, i + concurrency));\n-    }\n-\n-    for (const chunk of chunks) {\n-        const promises = chunk.map(modelId => \ncollectSingleModelOpinion(sessionId, modelId, topic, issueId));\n-        await Promise.allSettled(promises);\n-    }\n-\n-    // Session complete\n-    const finalSession = activeOpinionSessions.get(sessionId);\n-    if (finalSession) {\n-        console.log(`[OPINIONS] ‚úÖ Session ${sessionId} completed`);\n-        console.log(`[OPINIONS] üìä Final stats: \n${finalSession.completedModels.length} completed, \n${finalSession.failedModels.length} failed`);\n-\n-        broadcastOpinionUpdate(sessionId, {\n-            type: 'session_completed',\n-            totalModels: finalSession.totalModels,\n-            completedModels: finalSession.completedModels,\n-            failedModels: finalSession.failedModels,\n-            responses: finalSession.responses,\n-            endTime: new Date().toISOString()\n-        });\n-\n-        // Keep session for 10 minutes then cleanup\n-        setTimeout(() => {\n-            activeOpinionSessions.delete(sessionId);\n-            console.log(`[OPINIONS] üóëÔ∏è  Session ${sessionId} cleaned up`);\n-        }, 10 * 60 * 1000);\n-    }\n-}\n-\n-// Function to sanitize text for JSON storage - escapes quotes and other \nproblematic characters\n+        type: 'session_start',\n+        topic,\n+        issueId,\n+        models,\n+        status: 'running'\n+    });\n+\n+    const results = {};\n+    const failedModels = [];\n+\n+    // Use Promise.all for parallel execution, but limit concurrency if needed\n+    const concurrencyLimit = 5; // Limit concurrent calls to avoid overwhelming\nthe system\n+    const tasks = models.map(modelId => async () => {\n+        try {\n+            const opinion = await collectSingleModelOpinion(sessionId, modelId,\ntopic, issueId);\n+            results[modelId] = opinion;\n+            broadcastOpinionUpdate(sessionId, { type: 'model_result', modelId, \nresult: opinion });\n+        } catch (error) {\n+            console.error(`[OPINIONS] ‚ùå Error collecting opinion from \n${modelId}:`, error);\n+            results[modelId] = { error: error.message || 'Unknown error' };\n+            failedModels.push(modelId);\n+            broadcastOpinionUpdate(sessionId, { type: 'model_error', modelId, \nerror: error.message });\n+        }\n+    });\n+\n+    // Execute tasks with concurrency limit\n+    const executeWithLimit = async (tasks, limit) => {\n+        const executing = [];\n+        for (const task of tasks) {\n+            const p = task().then(res => ({ status: 'fulfilled', value: res \n})).catch(err => ({ status: 'rejected', reason: err }));\n+            executing.push(p);\n+            if (executing.length >= limit) {\n+                await Promise.race(executing);\n+            }\n+        }\n+        await Promise.all(executing);\n+    };\n+\n+    await executeWithLimit(tasks, concurrencyLimit);\n+\n+\n+    session.status = 'completed';\n+    session.results = results;\n+    session.failedModels = failedModels;\n+\n+    console.log(`[OPINIONS] ‚ú® Opinion collection finished for session \n${sessionId}`);\n+    broadcastOpinionUpdate(sessionId, {\n+        type: 'session_end',\n+        status: 'completed',\n+        results,\n+        failedModels\n+    });\n+\n+    // Optionally, add the collected opinions to the issues.json file\n+    if (issueId) {\n+        await addOpinionsToIssue(issueId, topic, results);\n+    }\n+}\n+\n+async function collectSingleModelOpinion(sessionId, modelId, topic, issueId) {\n+    const session = activeOpinionSessions.get(sessionId);\n+    if (!session) return;\n+\n+    console.log(`[OPINIONS] ü§ñ Querying ${modelId} about: \"${topic}\"`);\n+\n+    // Broadcast model started\n+    broadcastOpinionUpdate(sessionId, {\n+        type: 'model_started',\n+        modelId,\n+    });\n+\n+    try {\n+        // Build context for the model\n+        let context = `Voc√™ √© o modelo ${modelId}. Sua tarefa √© fornecer uma \nopini√£o concisa e fundamentada sobre o seguinte t√≥pico, considerando o contexto \ndo BIP-05 e as discuss√µes em issues.json (se aplic√°vel).`;\n+\n+        if (issueId) {\n+            const issueContext = buildBipContextPack(issueId, 1000, topic); // \nGet context for the specific issue\n+            context += `\\n\\nContexto do Issue #${issueId}:\\n${issueContext}`;\n+        }\n+\n+        context += `\\n\\nT√≥pico para opini√£o: ${topic}`;\n+        context += `\\n\\nPor favor, forne√ßa sua opini√£o:`;\n+\n+        // Call the LLM\n+        const response = await callLLM(modelId, context);\n+\n+        if (response.error) {\n+            throw new Error(response.error);\n+        }\n+\n+        // Basic validation (can be expanded)\n+        const validatedResponse = validateModelResponse(modelId, \nresponse.output);\n+        if (!validatedResponse) {\n+            throw new Error('Invalid or empty response from model.');\n+        }\n+\n+        return validatedResponse;\n+\n+    } catch (error) {\n+        console.error(`[OPINIONS] ‚ùå Error calling ${modelId}:`, error);\n+        throw error; // Re-throw to be caught by the caller\n+    }\n+}\n+\n+function broadcastOpinionUpdate(sessionId, update) {\n+    const message = {\n+        type: 'opinion_update',\n+        sessionId,\n+        ...update,\n+        timestamp: new Date().toISOString()\n+    };\n+\n+    wss.clients.forEach(client => {\n+        if (client.readyState === WebSocket.OPEN) {\n+            client.send(JSON.stringify(message));\n+        }\n+    });\n+}\n+\n+// --- Utility Functions ---\n function sanitizeForJSON(text) {\n     if (typeof text !== 'string') return text;\n \n     return text\n         .replace(/\\\\/g, '\\\\\\\\')  // Escape backslashes first\n@@ -1607,836 +716,363 @@\n         .replace(/\\r\\n/g, '\\\\n') // Handle Windows line endings\n         .replace(/\\n/g, '\\\\n')   // Handle Unix line endings\n         .replace(/\\r/g, '\\\\n')   // Handle Mac line endings\n         .replace(/\\t/g, '\\\\t')   // Handle tabs\n         .replace(/\\f/g, '\\\\f');  // Handle form feeds\n-        // REMOVED: .replace(/\\b/g, '\\\\b') - This was incorrectly escaping word\nboundaries!\n-}\n-\n-// Function to collect opinion from a single model\n-async function collectSingleModelOpinion(sessionId, modelId, topic, issueId) {\n-    const session = activeOpinionSessions.get(sessionId);\n-    if (!session) return;\n-\n-    console.log(`[OPINIONS] ü§ñ Querying ${modelId} about: \"${topic}\"`);\n-\n-    // Broadcast model started\n-    broadcastOpinionUpdate(sessionId, {\n-        type: 'model_started',\n-        modelId,\n-        status: 'querying'\n-    });\n-\n-    try {\n-        // Build context pack from BIP files and issues\n-        const contextPack = buildBipContextPack(issueId, 18000, topic); // ~18k\nchars cap to keep prompt safe\n-\n-        // Build prompt for model opinion with strict guidelines and embedded \ncontext\n-        const prompt = `Como modelo AI participante das discuss√µes do BIP-05 \n(Universal Matrix Protocol), forne√ßa sua opini√£o sobre:\n-\n-**T√≥pico**: ${topic}\n-\n-**DIRETRIZES CR√çTICAS**:\n-- VOC√ä √â: ${modelId}\n-- NUNCA simule ou invente opini√µes de outros modelos\n-- JAMAIS fale em nome de outros modelos\n-- APENAS forne√ßa SUA pr√≥pria perspectiva como ${modelId}\n-- Se questionado sobre outros modelos, responda \"Consulte diretamente o modelo \nespec√≠fico\"\n-\n-**Instru√ß√µes**:\n-1. Analise o t√≥pico no contexto do BIP-05\n-2. Forne√ßa SUA perspectiva t√©cnica e considera√ß√µes espec√≠ficas como ${modelId}\n-3. Seja espec√≠fico e construtivo\n-4. Limite a resposta a 3-4 par√°grafos\n-5. Termine com uma recomenda√ß√£o clara\n-6. Identifique-se claramente como ${modelId} no in√≠cio da resposta\n-\n-**Contexto do BIP-05 (trechos relevantes):**\n-${contextPack}\n-\n-**Sua opini√£o como ${modelId} sobre \"${topic}\":**`;\n-\n-        // Apply auto model safeguards if needed\n-        const safeguardedPrompt = await handleAutoModelSafeguard(modelId, \nprompt);\n-\n-        // Call the model with individual timeout\n-        const response = await callLLM(modelId, safeguardedPrompt);\n-\n-        // Validate response to ensure model isn't speaking for others\n-        console.log(`[VALIDATION] Checking response from ${modelId} for \nidentity violations...`);\n-        const validationError = validateModelResponse(modelId, response);\n-        if (validationError) {\n-            console.log(`[VALIDATION] ‚ùå ${modelId} failed validation: \n${validationError}`);\n-            throw new Error(`Resposta inv√°lida: ${validationError}`);\n-        }\n-        console.log(`[VALIDATION] ‚úÖ ${modelId} response passed identity \nvalidation`);\n-\n-        // Check if response is valid and not an error message\n-        const isValidResponse = response &&\n-                               !response.includes('‚ùå') &&\n-                               !response.includes('‚è∞') &&\n-                               !response.includes('Aider v') &&\n-                               !response.includes('Warning:') &&\n-                               !response.includes('Traceback') &&\n-                               !response.includes('litellm.') &&\n-                               !response.includes('BadRequestError') &&\n-                               response.length > 50; // Ensure substantive \nresponse\n-\n-        if (isValidResponse) {\n-            // Success - save to issues.json immediately\n-            const opinion = {\n-                author: modelId,\n-                created_at: new Date().toISOString(),\n-                locale: 'pt-BR',\n-                body: sanitizeForJSON(response),\n-                body_original: sanitizeForJSON(response),\n-                opinion_topic: topic,\n-                session_id: sessionId\n-            };\n-\n-            // Add to issues.json immediately\n-            // Use atomic write to prevent corruption\n-            try {\n-                const issuesData = JSON.parse(fs.readFileSync(issuesFile, \n'utf8'));\n-                if (issuesData.issues && issuesData.issues.length > 0) {\n-                    // Add to specified issue or first issue\n-                    const targetIssue = issuesData.issues.find(issue => \nissue.id === issueId) || issuesData.issues[0];\n-                    targetIssue.comments.push(opinion);\n-                } else {\n-                    issuesData.issues = [{\n-                        id: issueId,\n-                        title: `Opini√µes sobre: ${topic}`,\n-                        comments: [opinion]\n-                    }];\n-                }\n-\n-                // Atomic write to prevent corruption\n-                const tempFile = issuesFile + '.tmp';\n-                fs.writeFileSync(tempFile, JSON.stringify(issuesData, null, 2),\n'utf8');\n-                fs.renameSync(tempFile, issuesFile);\n-\n-                console.log(`[SAVE] ‚úÖ Opinion from ${modelId} saved \nsuccessfully`);\n-\n-            } catch (writeError) {\n-                console.error(`[ERROR] Failed to save opinion from \n${modelId}:`, writeError);\n-                throw writeError;\n-            }\n-\n-            // Update session\n-            session.pendingModels = session.pendingModels.filter(m => m !== \nmodelId);\n-            session.completedModels.push(modelId);\n-            session.responses.push({\n-                modelId,\n-                response,\n-                timestamp: new Date().toISOString(),\n-                success: true\n-            });\n-\n-            console.log(`[OPINIONS] ‚úÖ ${modelId} completed successfully`);\n-\n-            // Broadcast success\n-            broadcastOpinionUpdate(sessionId, {\n-                type: 'model_completed',\n-                modelId,\n-                status: 'completed',\n-                response: response.substring(0, 200) + '...', // Preview\n-                timestamp: new Date().toISOString()\n-            });\n-\n-        } else {\n-            throw new Error(response || 'Empty response from model');\n-        }\n-\n-    } catch (error) {\n-        console.log(`[OPINIONS] ‚ùå ${modelId} failed: ${error.message}`);\n-\n-        // Update session\n-        session.pendingModels = session.pendingModels.filter(m => m !== \nmodelId);\n-        session.failedModels.push(modelId);\n-        session.responses.push({\n-            modelId,\n-            error: error.message,\n-            timestamp: new Date().toISOString(),\n-            success: false\n-        });\n-\n-        // Broadcast failure\n-        broadcastOpinionUpdate(sessionId, {\n-            type: 'model_failed',\n-            modelId,\n-            status: 'failed',\n-            error: error.message,\n-            timestamp: new Date().toISOString()\n-        });\n-    }\n-}\n-\n-// Function to broadcast opinion collection updates\n-function broadcastOpinionUpdate(sessionId, update) {\n-    const message = {\n-        type: 'opinion_update',\n-        sessionId,\n-        ...update,\n-        timestamp: new Date().toISOString()\n-    };\n-\n-    wss.clients.forEach(client => {\n-        if (client.readyState === WebSocket.OPEN) {\n-            client.send(JSON.stringify(message));\n-        }\n-    });\n-}\n-\n-// Helper: parse AUTO_CMD JSON from free-form text\n+}\n+\n function parseAutoCmdFromText(text) {\n     if (typeof text !== 'string') return null;\n     const idx = text.indexOf('AUTO_CMD:');\n     if (idx === -1) return null;\n     // Take from AUTO_CMD: to end of line\n     const after = text.slice(idx + 'AUTO_CMD:'.length);\n     // Try to extract JSON between first '{' and last '}'\n     const s = after.indexOf('{');\n     const e = after.lastIndexOf('}');\n     if (s === -1 || e === -1 || e <= s) return null;\n-    const candidate = after.slice(s, e + 1).trim();\n+    const jsonString = after.slice(s, e + 1);\n     try {\n-        return JSON.parse(candidate);\n-    } catch (e1) {\n-        // Sanitize common trailing quote or markdown artifacts\n-        const cleaned = candidate.replace(/\\\"$/,'').replace(/`+/g,'').trim();\n-        try { return JSON.parse(cleaned); } catch {\n-            console.log(`[AUTO_CMD] ‚ö†Ô∏è  JSON parse failed: ${e1.message}`);\n-            return null;\n-        }\n-    }\n-}\n-\n-// Helper: normalize model id short forms (e.g., 'grok-3' -> 'xai/grok-3')\n+        return JSON.parse(jsonString);\n+    } catch (e) {\n+        console.error(\"Failed to parse AUTO_CMD JSON:\", e);\n+        return null;\n+    }\n+}\n+\n function normalizeModelId(modelId) {\n     if (!modelId || typeof modelId !== 'string') return modelId;\n     if (modelId.includes('/')) return modelId;\n     // Try to find a provider key that ends with '/modelId'\n     const aiderKeys = Object.keys(MODEL_CATEGORIES.aider_models || {});\n     const match = aiderKeys.find(k => k.endsWith('/' + modelId));\n     if (match) return match;\n     // Also accept cursor models if short name matches exactly\n-    const cursorMatch = (MODEL_CATEGORIES.cursor_models || []).find(k => \nk.endsWith('/' + modelId) || k === modelId);\n+    const cursorMatch = (MODEL_CATEGORIES.cursor_models || []).find(k => \nk.endsWith('/' + modelId));\n     if (cursorMatch) return cursorMatch;\n-    return modelId; // leave as-is if unknown\n-}\n-\n-// Build a concise context pack from BIP files and the target issue\n+    // If it's a known provider short name, prepend it\n+    const knownProviders = ['openai', 'anthropic', 'google', 'xai', 'deepseek',\n'groq'];\n+    if (knownProviders.includes(modelId.toLowerCase())) {\n+        // This is ambiguous, maybe default to a common model for that \nprovider?\n+        // For now, return as is or 'auto'\n+        return 'auto';\n+    }\n+    return modelId; // Return original if no match found\n+}\n+\n function buildBipContextPack(issueId, maxChars, topic) {\n     try {\n         const parts = [];\n         // Include issue snippet\n-        if (fs.existsSync(issuesFile)) {\n-            const issuesData = JSON.parse(fs.readFileSync(issuesFile, 'utf8'));\n-            const targetIssue = issuesData.issues?.find(i => i.id === issueId) \n|| issuesData.issues?.[0];\n+        if (fs.existsSync(ISSUES_FILE)) {\n+            const issuesData = JSON.parse(fs.readFileSync(ISSUES_FILE, \n'utf8'));\n+            const targetIssue = issuesData.issues?.find(i => i.id === issueId) \n|| issuesData.issues?.find(i => i.number === issueId); // Allow searching by \nnumber too\n             if (targetIssue) {\n-                parts.push(`### Issue #${targetIssue.id}: \n${targetIssue.title}`);\n-                const recent = (targetIssue.comments || []).slice(-3);\n-                recent.forEach(c => parts.push(`- ${c.author}: \n${c.body?.slice(0, 400) || ''}`));\n+                parts.push(`### Issue #${targetIssue.id || targetIssue.number}:\n${targetIssue.title}`);\n+                const recent = (targetIssue.comments || []).slice(-3); // Last \n3 comments\n+                recent.forEach(c => parts.push(`- ${c.author}: \n${c.body?.slice(0, 400) || ''}`)); // Truncate comment body\n+            } else {\n+                parts.push(`Issue #${issueId} n√£o encontrado.`);\n             }\n-        }\n-\n-        // Include BIP docs snippets scanning whitelisted dirs for relevant \nfiles\n-        const repoRoot = path.join(__dirname, '..', '..');\n-        const whitelistDirs = [\n-            path.join(repoRoot, 'docs'),\n-            path.join(repoRoot, 'gov', 'bips', 'BIP-05')\n-        ];\n-\n-        const candidateFiles = [];\n-        const allowedExt = new Set(['.md', '.mdx', '.txt']);\n-\n-        function walk(dir) {\n-            if (!fs.existsSync(dir)) return;\n-            const entries = fs.readdirSync(dir, { withFileTypes: true });\n-            for (const e of entries) {\n-                const p = path.join(dir, e.name);\n-                if (e.isDirectory()) walk(p);\n-                else if (allowedExt.has(path.extname(e.name).toLowerCase())) \ncandidateFiles.push(p);\n+        } else {\n+            parts.push(`Arquivo issues.json n√£o encontrado.`);\n+        }\n+\n+        // Add general context about BIP-05 if available (e.g., from a README)\n+        const bipReadmePath = path.join(__dirname, '..', '..', '..', '..', \n'gov', 'bips', 'BIP-05', 'README.md');\n+        if (fs.existsSync(bipReadmePath)) {\n+            const bipReadmeContent = fs.readFileSync(bipReadmePath, 'utf8');\n+            parts.push(`\\nContexto Geral do \nBIP-05:\\n${bipReadmeContent.slice(0, 500)}...`); // Truncate README\n+        }\n+\n+        // Combine and truncate to maxChars\n+        let combined = parts.join('\\n');\n+        if (combined.length > maxChars) {\n+            combined = combined.slice(0, maxChars) + '...';\n+        }\n+        return combined;\n+\n+    } catch (error) {\n+        console.error(`[CONTEXT] Error building BIP context pack for issue \n${issueId}:`, error);\n+        return `Erro ao carregar contexto para o issue ${issueId}.`;\n+    }\n+}\n+\n+// --- File Watching ---\n+function startFileWatcher() {\n+    if (fileWatcher) {\n+        fileWatcher.close();\n+    }\n+\n+    fileWatcher = fs.watch(ISSUES_FILE, { persistent: true }, (eventType) => {\n+        if (eventType === 'change') {\n+            console.log('issues.json changed, queueing broadcast...');\n+\n+            // Clear the previous timeout if a new change event comes in\n+            if (fileWatcher.timeout) {\n+                clearTimeout(fileWatcher.timeout);\n             }\n-        }\n-        whitelistDirs.forEach(walk);\n-\n-        // Score by simple keyword match using topic tokens\n-        const topicTokens = (topic || \n'').toLowerCase().split(/[^a-z0-9]+/).filter(Boolean);\n-        function scoreContent(content) {\n-            const lc = content.toLowerCase();\n-            let score = 0;\n-            for (const t of topicTokens) if (t.length >= 3 && lc.includes(t)) \nscore += 1;\n-            // Favor files that mention BIP or Universal Matrix\n-            if (lc.includes('bip-05')) score += 2;\n-            if (lc.includes('universal matrix')) score += 2;\n-            return score;\n-        }\n-\n-        const ranked = candidateFiles\n-            .map(f => {\n-                try { return { f, c: fs.readFileSync(f, 'utf8') }; } catch { \nreturn null; }\n-            })\n-            .filter(Boolean)\n-            .map(x => ({ ...x, s: scoreContent(x.c) }))\n-            .sort((a, b) => b.s - a.s)\n-            .slice(0, 6);\n-\n-        ranked.forEach(({ f, c }) => {\n-            const header = `\\n### ${path.relative(repoRoot, f)}\\n`;\n-            parts.push(header + c.slice(0, 2500));\n-        });\n-\n-        let pack = parts.join('\\n');\n-        if (pack.length > maxChars) pack = pack.slice(0, maxChars);\n-        return pack;\n-    } catch (e) {\n-        console.log(`[CONTEXT] ‚ö†Ô∏è  Failed to build context pack: ${e.message}`);\n-        return 'Contexto indispon√≠vel no momento.';\n-    }\n-}\n-\n-// Ask 'auto' to propose a shortlist of models for the topic\n-async function generateAutoOpinionPlan(topic, issueId, candidateModels) {\n-    const planPrompt = `Voc√™ √© o orquestrador 'auto' (modelo mediador) do \nBIP-05.\n-\n-PRIVIL√âGIOS & RESPONSABILIDADES:\n-- Voc√™ pode escrever no issues.json (adicionar coment√°rios/opini√µes) de forma \nsegura.\n-- Voc√™ orquestra pedidos de opini√£o de outros modelos usando as APIs do \nservidor.\n-- Use a lista de candidatos abaixo para selecionar os modelos mais adequados.\n-\n-ROTAS DISPON√çVEIS (para seu planejamento):\n-- POST /api/models/opinions  ‚Üí inicia sess√£o de opini√µes em lote; body: { \ntopic, issueId, targetModels? }\n-- GET  /api/models/opinions/:sessionId ‚Üí status/progresso da sess√£o\n-- POST /api/models/option    ‚Üí solicitar opini√£o individual; body: { topic, \nissueId, modelId }\n-- GET  /api/status           ‚Üí status das APIs e modelos\n-- GET  /api/costs            ‚Üí custos de execu√ß√£o por modelo\n-\n-TAREFA:\n-Dado o t√≥pico: \"${topic}\", escolha de 5 a 8 modelos mais relevantes na lista \nabaixo para opinar, equilibrando provedores e capacidades. Responda APENAS JSON \ncom {\"models\":[\"prov/model\", ...], \"rationale\":\"...\"}.\n-\n-Lista de candidatos:\n-${candidateModels.map(m => `- ${m}`).join('\\n')}\n-\n-`;\n-\n+\n+            // Debounce the broadcast to avoid multiple rapid updates\n+            fileWatcher.timeout = setTimeout(() => {\n+                console.log('Broadcasting updated issues.json...');\n+                broadcastIssues();\n+            }, 500); // 500ms debounce\n+        }\n+    });\n+\n+    fileWatcher.on('error', (err) => {\n+        logError('FILE_WATCHER', 'Error watching issues.json', { error: \nerr.message });\n+    });\n+\n+    logInfo('FILE_WATCHER', 'Started watching issues.json');\n+}\n+\n+// --- WebSocket Server ---\n+function setupWebSocketServer() {\n+    const PORT = process.env.PORT || 8080;\n+    wss = new WebSocket.Server({ port: PORT });\n+\n+    logInfo('WEBSOCKET', `WebSocket server started on port ${PORT}`);\n+\n+    wss.on('connection', (ws) => {\n+        clients.add(ws);\n+        console.log('Client connected');\n+\n+        ws.on('message', async (message) => {\n+            try {\n+                const parsedMessage = JSON.parse(message);\n+                console.log('Received message:', parsedMessage);\n+\n+                // Handle different message types\n+                if (parsedMessage.type === 'chat_message') {\n+                    await handleUserMessage(parsedMessage);\n+                } else if (parsedMessage.type === 'request_opinions') {\n+                    await handleOpinionCollectionRequest(parsedMessage.text, \nparsedMessage.issueId);\n+                } else if (parsedMessage.type === 'hello_handshake') {\n+                    await handleHelloHandshakeRequest(parsedMessage.text);\n+                } else if (parsedMessage.type === 'get_issues') {\n+                    broadcastIssues(); // Send current issues data\n+                }\n+                // Add more message types as needed\n+\n+            } catch (error) {\n+                logError('WEBSOCKET', 'Failed to process message', { error: \nerror.message, rawMessage: message.toString() });\n+                ws.send(JSON.stringify({ type: 'error', message: 'Invalid \nmessage format.' }));\n+            }\n+        });\n+\n+        ws.on('close', () => {\n+            clients.delete(ws);\n+            console.log('Client disconnected');\n+        });\n+\n+        ws.on('error', (error) => {\n+            console.error('WebSocket error:', error);\n+            clients.delete(ws);\n+        });\n+\n+        // Send initial data to the newly connected client\n+        ws.send(JSON.stringify({ type: 'initial_data', issues: readIssuesFile()\n}));\n+    });\n+\n+    wss.on('error', (error) => {\n+        logFatal('WEBSOCKET', 'WebSocket server error', { error: error.message \n});\n+        // Consider restarting the server or exiting\n+    });\n+}\n+\n+// --- Message Handling ---\n+\n+// Function to read issues.json with basic error handling\n+function readIssuesFile() {\n     try {\n-        const raw = await callLLM('auto', planPrompt);\n-        const jsonStart = raw.indexOf('{');\n-        const jsonEnd = raw.lastIndexOf('}');\n-        if (jsonStart >= 0 && jsonEnd > jsonStart) {\n-            const parsed = JSON.parse(raw.slice(jsonStart, jsonEnd + 1));\n-            if (Array.isArray(parsed.models)) return parsed;\n-        }\n-    } catch (e) {\n-        console.log(`[AUTO PLAN] ‚ö†Ô∏è  parse/plan failed: ${e.message}`);\n-    }\n-    return { models: [] };\n-}\n-\n-function getDefaultShortlistFromProviders(allModels) {\n-    // Pick top 6 by provider diversity: prefer 2 OpenAI, 2 Anthropic/Gemini, 2\nxAI/DeepSeek se dispon√≠veis\n-    const buckets = allModels.reduce((acc, m) => {\n-        const provider = m.split('/')[0] || 'other';\n-        (acc[provider] ||= []).push(m);\n-        return acc;\n-    }, {});\n-    const pick = (arr, n) => (arr || []).slice(0, n);\n-    return [\n-        ...pick(buckets.openai, 2),\n-        ...pick(buckets.anthropic, 2),\n-        ...pick(buckets.gemini, 1),\n-        ...pick(buckets.xai, 1),\n-        ...pick(buckets.deepseek, 1)\n-    ].filter(Boolean);\n-}\n-\n-// API endpoint to get opinion session status\n-app.get('/api/models/opinions/:sessionId', (req, res) => {\n-    const { sessionId } = req.params;\n-    const session = activeOpinionSessions.get(sessionId);\n-\n-    if (!session) {\n-        return res.status(404).json({\n-            success: false,\n-            error: 'Session not found'\n-        });\n-    }\n-\n-    res.json({\n-        success: true,\n-        session: {\n-            ...session,\n-            progress: {\n-                total: session.totalModels,\n-                completed: session.completedModels.length,\n-                failed: session.failedModels.length,\n-                pending: session.pendingModels.length,\n-                percentage: Math.round((session.completedModels.length / \nsession.totalModels) * 100)\n-            }\n-        }\n-    });\n-});\n-\n-// API endpoint to request a single model opinion (individual option)\n-app.post('/api/models/option', async (req, res) => {\n-    try {\n-        const { topic, issueId = 1, modelId } = req.body;\n-\n-        if (!topic || !modelId) {\n-            return res.status(400).json({ success: false, error: 'Campos \nobrigat√≥rios: topic e modelId' });\n-        }\n-\n-        const sessionId = \n`option_${Date.now()}_${Math.random().toString(36).slice(2, 10)}`;\n-        console.log(`[OPTION] üéØ Solicita√ß√£o de opini√£o individual - ${modelId}\n‚Ä¢ Session: ${sessionId}`);\n-\n-        // Inicializa sess√£o m√≠nima para compatibilidade com painel\n-        activeOpinionSessions.set(sessionId, {\n-            sessionId,\n-            topic,\n-            issueId,\n-            totalModels: 1,\n-            pendingModels: [modelId],\n-            completedModels: [],\n-            failedModels: [],\n-            responses: [],\n-            startTime: new Date().toISOString()\n-        });\n-\n-        // Resposta imediata\n-        res.json({ success: true, sessionId, message: 'Opini√£o individual \niniciada', modelId });\n-\n-        // Executa em background\n-        await collectSingleModelOpinion(sessionId, modelId, topic, issueId);\n-\n-        // Finaliza sess√£o\n-        const session = activeOpinionSessions.get(sessionId);\n-        if (session) {\n-            broadcastOpinionUpdate(sessionId, {\n-                type: 'session_completed',\n-                totalModels: 1,\n-                completedModels: session.completedModels,\n-                failedModels: session.failedModels,\n-                responses: session.responses,\n-                endTime: new Date().toISOString()\n-            });\n-\n-            setTimeout(() => activeOpinionSessions.delete(sessionId), 10 * 60 *\n1000);\n-        }\n-\n+        if (fs.existsSync(ISSUES_FILE)) {\n+            const rawData = fs.readFileSync(ISSUES_FILE, 'utf8');\n+            return JSON.parse(rawData);\n+        }\n     } catch (error) {\n-        console.error('[OPTION] ‚ùå Erro:', error);\n-        res.status(500).json({ success: false, error: error.message });\n-    }\n-});\n-\n-// Serve static files (index.html, style.css)\n-app.use(express.static(__dirname));\n-\n-// Endpoint para acessar logs de debug\n-app.get('/api/logs', (req, res) => {\n-    try {\n-        const logType = req.query.type || 'debug'; // 'debug' or 'error'\n-        const lines = parseInt(req.query.lines) || 100;\n-\n-        const logFile = logType === 'error' ? ERROR_LOG_FILE : LOG_FILE;\n-\n-        if (!fs.existsSync(logFile)) {\n-            return res.json({\n-                success: false,\n-                message: `Log file not found: ${logFile}`,\n-                logs: []\n-            });\n-        }\n-\n-        const logContent = fs.readFileSync(logFile, 'utf8');\n-        const allLines = logContent.split('\\n').filter(line => line.trim());\n-        const recentLines = allLines.slice(-lines);\n-\n-        res.json({\n-            success: true,\n-            logType: logType,\n-            totalLines: allLines.length,\n-            returnedLines: recentLines.length,\n-            logFile: logFile,\n-            logs: recentLines\n-        });\n-\n-    } catch (error) {\n-        logError('API', 'Error reading log files', {\n-            error: error.message,\n-            requestedType: req.query.type,\n-            requestedLines: req.query.lines\n-        });\n-\n-        res.status(500).json({\n-            success: false,\n-            error: error.message\n-        });\n-    }\n-});\n-\n-// Endpoint para limpar logs\n-app.post('/api/logs/clear', (req, res) => {\n-    try {\n-        const logType = req.body.type || 'debug';\n-        const logFile = logType === 'error' ? ERROR_LOG_FILE : LOG_FILE;\n-\n-        if (fs.existsSync(logFile)) {\n-            fs.writeFileSync(logFile, '');\n-            logInfo('API', `Cleared ${logType} log file`, {\n-                logFile: logFile,\n-                clearedBy: 'manual_request'\n-            });\n-        }\n-\n-        res.json({\n-            success: true,\n-            message: `${logType} log cleared successfully`,\n-            logFile: logFile\n-        });\n-\n-    } catch (error) {\n-        logError('API', 'Error clearing log file', {\n-            error: error.message,\n-            requestedType: req.body.type\n-        });\n-\n-        res.status(500).json({\n-            success: false,\n-            error: error.message\n-        });\n-    }\n-});\n-\n-// Simple REST endpoint for posting comments\n-app.post('/api/comment', async (req, res) => {\n-    const { model, text } = req.body;\n-\n-    try {\n-        const selectedModel = model || selectAppropriateModel(text || '', \n'bip');\n-        console.log(`[DEBUG] Selected model: ${selectedModel} (requested: \n${model || 'auto'})`);\n-\n-        const issuesData = JSON.parse(fs.readFileSync(issuesFile, 'utf8'));\n-        const prompt = buildPromptFromContext(text || 'Sem texto do usu√°rio', \nissuesData);\n-        const llmText = await callLLM(selectedModel, prompt);\n-        const bodyText = llmText || (text ? `Recebido: \"${text}\"` : \n`Participa√ß√£o registrada por ${selectedModel}.`);\n-\n-        const comment = {\n-            author: selectedModel,\n-            created_at: new Date().toISOString(),\n-            locale: 'pt-BR',\n-            body: sanitizeForJSON(bodyText),\n-            body_original: sanitizeForJSON(bodyText)\n-        };\n-\n-        if (issuesData.issues && issuesData.issues.length > 0) {\n-            issuesData.issues[0].comments.push(comment);\n-  } else {\n-            issuesData.issues = [{ id: 1, title: 'Main Thread', comments: \n[comment] }];\n-        }\n-\n-        fs.writeFileSync(issuesFile, JSON.stringify(issuesData, null, 2), \n'utf8');\n-\n-        res.status(200).json({\n-            success: true,\n-            message: `Coment√°rio publicado por ${selectedModel}`,\n-            comment: bodyText\n-        });\n-    } catch (error) {\n-        console.error('Error in /api/comment:', error);\n-        res.status(500).json({ error: error.message });\n-    }\n-});\n-\n-// Create WebSocket server and attach it to the HTTP server\n-const wss = new WebSocket.Server({ server });\n-\n-// Store connected clients\n-const clients = new Set();\n-\n-// Function to send issues data to all connected clients\n+        logError('FILE_READ', 'Error reading issues.json', { error: \nerror.message });\n+    }\n+    return { issues: [] }; // Return empty structure on error\n+}\n+\n function broadcastIssues() {\n-  if (clients.size === 0) {\n-    logDebug('BROADCAST', 'No clients connected, skipping broadcast');\n-    return;\n-  }\n+    if (clients.size === 0) {\n+        logDebug('BROADCAST', 'No clients connected, skipping broadcast');\n+        return;\n+    }\n \n     logInfo('BROADCAST', 'Starting broadcast to clients', {\n         clientCount: clients.size,\n-        issuesFile: issuesFile\n-    });\n-\n-  // Add retry logic for concurrent file access issues\n-  function readWithRetry(attempts = 3) {\n-    fs.readFile(issuesFile, 'utf8', (err, data) => {\n-      if (err) {\n-        logError('BROADCAST', 'Error reading issues.json file', {\n-          error: err.message,\n-          errorCode: err.code,\n-          attemptsRemaining: attempts - 1,\n-          filePath: issuesFile\n-        });\n-        return;\n-      }\n-\n-      try {\n-        // Validate JSON before parsing\n-        if (!data || data.trim() === '') {\n-          logError('BROADCAST', 'Empty or whitespace-only issues.json file \ndetected', {\n-            dataLength: data ? data.length : 0,\n-            filePath: issuesFile\n-          });\n-          return;\n-        }\n-\n-        logDebug('BROADCAST', 'Successfully read issues.json', {\n-          dataLength: data.length,\n-          dataPreview: data.substring(0, 200)\n-        });\n-\n-        const originalData = JSON.parse(data);\n-        logInfo('BROADCAST', 'Successfully parsed issues.json', {\n-          issuesCount: originalData.issues ? originalData.issues.length : 0,\n-          hasRootComment: !!originalData.master_comment\n-        });\n-\n-            // --- Data Transformation ---\n-            // 1. Collect all comments from all issues into a single array.\n-            let allComments = [];\n-            if (originalData.issues && Array.isArray(originalData.issues)) {\n-                originalData.issues.forEach(issue => {\n-                    if (issue.comments && Array.isArray(issue.comments)) {\n-                        // Add issue context to each comment if needed later\n-                        const commentsWithContext = issue.comments.map(c => \n({...c, issueId: issue.id, issueTitle: issue.title}));\n-                        allComments.push(...commentsWithContext);\n-                    }\n-                });\n-            }\n-\n-            // 2. Sort all comments by date.\n-            allComments.sort((a, b) => new Date(a.created_at) - new \nDate(b.created_at));\n-\n-            // 3. Create a simplified payload for the client.\n-            const simplifiedPayload = {\n-                master_comment: originalData.master_comment, // Keep the master\ncomment separate\n-                comments: allComments\n-            };\n-            // --- End Transformation ---\n-\n-      let successfulSends = 0;\n-      let failedSends = 0;\n-\n-      let clientIndex = 0;\n-      clients.forEach((client) => {\n-        clientIndex++;\n+        issuesFile: ISSUES_FILE\n+    });\n+\n+    const issuesData = readIssuesFile();\n+\n+    clients.forEach(client => {\n         if (client.readyState === WebSocket.OPEN) {\n-          try {\n-            const payload = JSON.stringify(simplifiedPayload);\n-            client.send(payload);\n-            successfulSends++;\n-            logDebug('BROADCAST', `Successfully sent to client \n#${clientIndex}`, {\n-              payloadLength: payload.length\n-            });\n-          } catch (sendErr) {\n-            failedSends++;\n-            logError('BROADCAST', `Failed to send to client ${clientIndex}`, {\n-              clientIndex: clientIndex,\n-              error: sendErr.message,\n-              clientState: client.readyState\n-            });\n-          }\n+            client.send(JSON.stringify({ type: 'issues_update', issues: \nissuesData }));\n+        }\n+    });\n+    logInfo('BROADCAST', 'Finished broadcasting issues.');\n+}\n+\n+// Function to add a comment to an issue in issues.json\n+async function addCommentToIssue(issueId, commentText, author) {\n+    logInfo('FILE_WRITE', 'Starting comment addition to issues.json', {\n+        issueId,\n+        author,\n+        textLength: commentText.length,\n+        issuesFile: ISSUES_FILE\n+    });\n+\n+    try {\n+        const rawData = fs.readFileSync(ISSUES_FILE, 'utf8');\n+        const data = JSON.parse(rawData);\n+\n+        const issueIndex = data.issues.findIndex(issue => issue.id === issueId \n|| issue.number === issueId);\n+\n+        if (issueIndex === -1) {\n+            logWarn('FILE_WRITE', 'Issue not found for comment', { issueId });\n+            return { success: false, message: 'Issue not found.' };\n+        }\n+\n+        const newComment = {\n+            id: Date.now(), // Simple ID generation\n+            author: author,\n+            body: commentText,\n+            created_at: new Date().toISOString(),\n+            updated_at: new Date().toISOString()\n+        };\n+\n+        if (!data.issues[issueIndex].comments) {\n+            data.issues[issueIndex].comments = [];\n+        }\n+        data.issues[issueIndex].comments.push(newComment);\n+\n+        fs.writeFileSync(ISSUES_FILE, JSON.stringify(data, null, 2), 'utf8');\n+        logInfo('FILE_WRITE', 'Comment added successfully', { issueId, author \n});\n+\n+        // Broadcast the update to all connected clients\n+        broadcastIssues();\n+\n+        return { success: true, message: 'Comment added successfully.' };\n+\n+    } catch (error) {\n+        logError('FILE_WRITE', 'Error adding comment to issues.json', { \nissueId, error: error.message });\n+        return { success: false, message: `Error adding comment: \n${error.message}` };\n+    }\n+}\n+\n+// Function to add collected opinions to an issue\n+async function addOpinionsToIssue(issueId, topic, opinions) {\n+    logInfo('FILE_WRITE', 'Adding collected opinions to issue', { issueId, \ntopic, opinionsCount: Object.keys(opinions).length });\n+\n+    let commentBody = `**Resumo das Opini√µes sobre \"${topic}\":**\\n\\n`;\n+    let hasErrors = false;\n+\n+    for (const [modelId, result] of Object.entries(opinions)) {\n+        if (result.error) {\n+            commentBody += `- **${modelId}:** Erro ao obter opini√£o \n(${result.error})\\n`;\n+            hasErrors = true;\n+        } else if (result.output) {\n+            commentBody += `- **${modelId}:** ${result.output.replace(/\\n/g, ' \n')}\\n`; // Replace newlines for cleaner comment\n         } else {\n-          failedSends++;\n-          logWarn('BROADCAST', `Client ${clientIndex} not ready`, {\n-            clientIndex: clientIndex,\n-            readyState: client.readyState\n-          });\n-        }\n-      });\n-\n-      logInfo('BROADCAST', 'Broadcast completed', {\n-        totalClients: clients.size,\n-        successfulSends,\n-        failedSends,\n-        commentsCount: allComments.length\n-      });\n-\n-      } catch (parseErr) {\n-        logError('BROADCAST', 'Error parsing or transforming issues.json', {\n-          error: parseErr.message,\n-          errorStack: parseErr.stack,\n-          attemptsRemaining: attempts - 1,\n-          dataLength: data ? data.length : 0\n-        });\n-\n-        // Retry if attempts remaining\n-        if (attempts > 1) {\n-          logWarn('BROADCAST', `Retrying JSON parse in 100ms...`, {\n-            attemptsLeft: attempts - 1,\n-            retryDelay: 100\n-          });\n-          setTimeout(() => readWithRetry(attempts - 1), 100);\n-        } else {\n-          logFatal('BROADCAST', 'Failed to parse issues.json after all retry \nattempts', {\n-            totalAttempts: 3,\n-            finalError: parseErr.message\n-          });\n-        }\n-      }\n-    });\n-  }\n-\n-  // Start reading with retry\n-  readWithRetry();\n-}\n-\n-// WebSocket connection handling\n-wss.on('connection', (ws) => {\n-  console.log('New client connected');\n-  clients.add(ws);\n-\n-  // Send current issues data immediately\n-  broadcastIssues();\n-\n-  ws.on('close', () => {\n-    console.log('Client disconnected');\n-    clients.delete(ws);\n-  });\n-\n-  ws.on('error', (error) => {\n-    console.error('WebSocket error:', error);\n-    clients.delete(ws);\n-  });\n-\n-    ws.on('message', (message) => {\n-        try {\n-            const rawMessage = message.toString();\n-            logDebug('WEBSOCKET', 'Received message from client', {\n-                messageLength: rawMessage.length,\n-                messagePreview: rawMessage.substring(0, 100),\n-                clientId: ws._clientId || 'unknown'\n-            });\n-\n-            const data = JSON.parse(rawMessage);\n-            logInfo('WEBSOCKET', 'Parsed WebSocket message', {\n-                type: data.type,\n-                hasText: !!data.text,\n-                textLength: data.text ? data.text.length : 0\n-            });\n-\n-            if (data.type === 'user_comment' && data.text) {\n-                handleUserComment(data.text);\n-            } else {\n-                logWarn('WEBSOCKET', 'Invalid message format or missing \nrequired fields', {\n-                    receivedData: data,\n-                    expectedType: 'user_comment',\n-                    hasText: !!data.text\n-                });\n-            }\n-        } catch (e) {\n-            logError('WEBSOCKET', 'Failed to parse incoming message', {\n-                error: e.message,\n-                errorStack: e.stack,\n-                rawMessage: message.toString(),\n-                messageType: typeof message,\n-                messageLength: message.toString().length\n-            });\n-        }\n-    });\n-});\n-\n-async function handleUserComment(text) {\n-    console.log(`[DEBUG] Handling user comment: \"${text}\"`);\n+            commentBody += `- **${modelId}:** Resposta vazia ou inv√°lida.\\n`;\n+            hasErrors = true;\n+        }\n+    }\n+\n+    if (hasErrors) {\n+        commentBody += `\\n*Nota: Houve erros ao coletar opini√µes de alguns \nmodelos.*`;\n+    }\n+\n+    // Add the consolidated comment to the issue\n+    const writeResult = await addCommentToIssue(issueId, commentBody, 'auto \n(Orquestrador)');\n+\n+    if (!writeResult.success) {\n+        logError('FILE_WRITE', 'Failed to add opinions summary to issue', { \nissueId, error: writeResult.message });\n+    }\n+}\n+\n+\n+async function handleUserMessage(messageData) {\n+    const { author, text } = messageData;\n+    console.log(`[DEBUG] Handling user message from ${author}: \"${text}\"`);\n     const lowerText = text.toLowerCase();\n \n     // Detect action type based on user intent\n     if (isHelloHandshakeRequest(lowerText)) {\n         // Hello/Handshake test: teste de conectividade de todos os modelos\n         await handleHelloHandshakeRequest(text);\n     } else if (isOpinionCollectionRequest(lowerText)) {\n         // Coleta de opini√µes: pergunta para todos os modelos\n         await handleOpinionCollectionRequest(text);\n     } else if (isGeneralContributionRequest(lowerText)) {\n-        // Resposta de general: adiciona ao issues.json e responde no chat\n+        // Contribui√ß√£o geral: processar como uma contribui√ß√£o para o BIP\n         await handleGeneralContribution(text);\n     } else if (isSummaryRequest(lowerText)) {\n-        // Resumo da conversa: gera arquivo de resumo\n+        // Solicita√ß√£o de resumo\n         await handleSummaryRequest(text);\n     } else {\n-        // Resposta simples: s√≥ responde no chat, n√£o adiciona ao issues.json\n+        // Default: simple chat response or context building\n         await handleSimpleResponse(text);\n     }\n }\n \n-// Function to handle opinion collection requests from chat\n-async function handleOpinionCollectionRequest(text) {\n+async function handleOpinionCollectionRequest(text, issueId = null) {\n     console.log(`[DEBUG] Handling opinion collection request: \"${text}\"`);\n \n     try {\n         // Extract topic from the text\n         const topic = extractOpinionTopic(text);\n+        if (!topic) {\n+            broadcastChatMessage({ author: 'System', text: 'N√£o consegui \nentender o t√≥pico para a coleta de opini√µes. Por favor, especifique.' });\n+            return;\n+        }\n \n         console.log(`[OPINIONS] Starting opinion collection via chat for topic:\n\"${topic}\"`);\n+\n+        // If issueId is not provided, try to find it in the text or prompt \nuser\n+        if (!issueId) {\n+            const issueMatch = text.match(/issue\\s*#?(\\d+)/i);\n+            if (issueMatch && issueMatch[1]) {\n+                issueId = parseInt(issueMatch[1]);\n+            } else {\n+                // Optionally prompt user for issue ID if needed\n+                // broadcastChatMessage({ author: 'System', text: `Para qual \nissue voc√™ gostaria de coletar opini√µes sobre \"${topic}\"?` });\n+                // return;\n+            }\n+        }\n+\n+        // Determine which models to query\n+        // For now, let's use a default set or 'auto' to decide\n+        const targetModels = ['auto']; // Let 'auto' decide, or specify models \nhere\n+\n+        const sessionId = \n`chat_opinion_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n+        activeOpinionSessions.set(sessionId, {\n+            topic,\n+            issueId,\n+            targetModels: targetModels, // This will be resolved by 'auto'\n+            results: {},\n+            status: 'pending'\n+        });\n \n         // Send confirmation message to chat\n         broadcastChatMessage({\n-            type: 'simple_response',\n-            author: 'üó≥Ô∏è Sistema de Opini√µes',\n-            text: `Iniciando coleta de opini√µes sobre: \"${topic}\"\\n\\nTodos os \nmodelos dispon√≠veis ser√£o consultados. Acompanhe o progresso no painel de \nOpini√µes.`,\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        // Get all available models\n-        const allModels = [\n-            ...MODEL_CATEGORIES.cursor_models,\n-            ...WORKING_APIS\n-        ];\n-\n-        const sessionId = \n`chat_session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n-        const issueId = 1; // Default to issue 1\n-\n-        // Initialize session tracking\n-        const sessionData = {\n-            sessionId,\n-            topic,\n-            issueId,\n-            startTime: new Date().toISOString(),\n-            totalModels: allModels.length,\n-            pendingModels: [...allModels],\n-            completedModels: [],\n-            failedModels: [],\n-            responses: [],\n-            triggeredByChat: true\n-        };\n-\n-        activeOpinionSessions.set(sessionId, sessionData);\n-\n-        // Broadcast session started via WebSocket\n-        broadcastOpinionUpdate(sessionId, {\n-            type: 'session_started',\n-            totalModels: allModels.length,\n-            pendingModels: [...allModels],\n-            completedModels: [],\n-            failedModels: [],\n-            triggeredByChat: true\n-        });\n-\n-        // Start collecting opinions asynchronously\n-        collectModelOpinions(sessionId, topic, issueId, allModels);\n-\n-        console.log(`[OPINIONS] Chat-triggered session ${sessionId} started \nwith ${allModels.length} models`);\n+            author: 'auto (Orquestrador)',\n+            text: `Ok, estou iniciando a coleta de opini√µes sobre \n\"${topic}\"${issueId ? ` para o Issue #${issueId}` : ''}. Aguarde um momento...`\n+        });\n+\n+        // Initiate the opinion collection process\n+        // The 'auto' model will resolve the targetModels list\n+        await collectModelOpinions(sessionId, topic, issueId, targetModels);\n \n     } catch (error) {\n-        console.error(`[OPINIONS] Error in chat opinion collection:`, error);\n-\n-        broadcastChatMessage({\n-            type: 'error',\n-            author: 'Sistema',\n-            text: `Erro ao iniciar coleta de opini√µes: ${error.message}`,\n-            timestamp: new Date().toISOString()\n-        });\n+        logError('MESSAGE_HANDLER', 'Error handling opinion collection \nrequest', { error: error.message });\n+        broadcastChatMessage({ author: 'System', text: `Desculpe, ocorreu um \nerro ao processar sua solicita√ß√£o de opini√µes: ${error.message}` });\n     }\n }\n \n function isGeneralContributionRequest(text) {\n     const generalTriggers = [\n@@ -2444,14 +1080,13 @@\n         'adicionar ao bip', 'add to bip', 'general opinion', 'opini√£o do \ngeneral',\n         'feedback oficial', 'official feedback', 'para o issues', 'to issues',\n         'registrar discuss√£o', 'record discussion', 'documentar', 'document'\n     ];\n \n-    return generalTriggers.some(trigger => text.includes(trigger));\n-}\n-\n-// Function to check if text is an opinion collection request\n+    return generalTriggers.some(trigger => \ntext.toLowerCase().includes(trigger));\n+}\n+\n function isOpinionCollectionRequest(text) {\n     const opinionKeywords = [\n         'consultar opini√µes', 'opini√£o dos modelos', 'opini√µes sobre',\n         'coletar opini√µes', 'o que os modelos pensam', 'perspectiva dos \nmodelos',\n         'consulta geral', 'opini√£o de todos', 'perguntar aos modelos',\n@@ -2473,139 +1108,115 @@\n     return helloKeywords.some(keyword =>\n         text.toLowerCase().includes(keyword.toLowerCase())\n     );\n }\n \n-// Function to extract topic from opinion request\n function extractOpinionTopic(text) {\n     // Try to extract topic after keywords\n     const patterns = [\n         /consultar opini√µes (?:dos modelos )?sobre (.+)/i,\n         /opini√µes? (?:dos modelos )?sobre (.+)/i,\n         /coletar opini√µes? sobre (.+)/i,\n         /o que os modelos pensam sobre (.+)/i,\n         /perspectiva dos modelos sobre (.+)/i,\n-        /perguntar aos modelos sobre (.+)/i\n+        /perguntar aos modelos sobre (.+)/i,\n+        /opini√µes? para (.+)/i, // Simpler pattern\n+        /opini√µes? regarding (.+)/i\n     ];\n \n     for (const pattern of patterns) {\n         const match = text.match(pattern);\n-        if (match) {\n+        if (match && match[1]) {\n             return match[1].trim();\n         }\n     }\n \n-    // Fallback: use the whole text as topic\n-    return text.trim();\n-}\n-\n-// Function to handle hello handshake request\n+    // Fallback: if keywords are present but no clear topic, ask for \nclarification\n+    if (isOpinionCollectionRequest(text)) {\n+        return null; // Indicate that topic extraction failed\n+    }\n+\n+    return null; // No opinion request detected\n+}\n+\n async function handleHelloHandshakeRequest(userText) {\n     logInfo('HELLO', 'Starting hello handshake test', {\n         initiatedBy: 'master',\n         userText: userText\n     });\n \n     const sessionId = \n`hello_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n \n     // Get all available models (cursor-agent + aider)\n     const allModels = [\n-        ...MODEL_CATEGORIES.cursor_models,\n-        ...WORKING_APIS\n+        ...MODEL_CATEGORIES.generals,\n+        ...MODEL_CATEGORIES.bip_specific,\n+        ...MODEL_CATEGORIES.cursor_models\n     ];\n-\n-    const sessionData = {\n-        sessionId: sessionId,\n-        startTime: Date.now(),\n-        totalModels: allModels.length,\n+    // Ensure unique models\n+    const uniqueModels = [...new Set(allModels)];\n+\n+    activeHelloSessions.set(sessionId, {\n+        models: uniqueModels,\n+        totalModels: uniqueModels.length,\n         completedModels: 0,\n-        results: [],\n+        results: {},\n         status: 'running'\n-    };\n-\n-    activeHelloSessions.set(sessionId, sessionData);\n-\n-    // Send initial message via chat\n-    broadcastChatMessage({\n-        type: 'chat_message',\n-        author: 'auto',\n-        text: `ü§ù Iniciando teste Hello/Handshake com ${allModels.length} \nmodelos dispon√≠veis...`,\n-        timestamp: new Date().toISOString(),\n-        isSystemMessage: true\n-    });\n-\n-    // Start handshake with all models\n-    helloHandshakeAllModels(sessionId, allModels);\n-\n-    return sessionData;\n-}\n-\n-// Function to execute hello handshake with all models\n+    });\n+\n+    broadcastHelloProgress(sessionId, activeHelloSessions.get(sessionId));\n+\n+    // Start the handshake process\n+    await helloHandshakeAllModels(sessionId, uniqueModels);\n+}\n+\n async function helloHandshakeAllModels(sessionId, models) {\n     const session = activeHelloSessions.get(sessionId);\n     if (!session) return;\n \n     logInfo('HELLO', 'Starting handshake with all models', {\n         sessionId: sessionId,\n         totalModels: models.length\n     });\n \n     // Process models in parallel with controlled concurrency\n-    const concurrency = 3; // Max 3 models at once to avoid rate limits\n-    const batches = [];\n-\n-    for (let i = 0; i < models.length; i += concurrency) {\n-        batches.push(models.slice(i, i + concurrency));\n-    }\n-\n-    for (const batch of batches) {\n-        const promises = batch.map(modelId =>\n-            helloSingleModel(sessionId, modelId)\n-        );\n-\n-        await Promise.allSettled(promises);\n-\n-        // Small delay between batches\n-        await new Promise(resolve => setTimeout(resolve, 1000));\n-    }\n-\n-    // Session complete\n-    const finalSession = activeHelloSessions.get(sessionId);\n-    if (finalSession) {\n-        finalSession.status = 'completed';\n-        finalSession.endTime = Date.now();\n-        finalSession.duration = finalSession.endTime - finalSession.startTime;\n-\n-        logInfo('HELLO', 'Hello handshake session completed', {\n-            sessionId: sessionId,\n-            totalModels: finalSession.totalModels,\n-            successfulResponses: finalSession.results.filter(r => \nr.success).length,\n-            failedResponses: finalSession.results.filter(r => \n!r.success).length,\n-            duration: finalSession.duration\n-        });\n-\n-        // Send completion summary via chat\n-        const successCount = finalSession.results.filter(r => \nr.success).length;\n-        const failCount = finalSession.results.filter(r => !r.success).length;\n-\n-        broadcastChatMessage({\n-            type: 'chat_message',\n-            author: 'auto',\n-            text: `‚úÖ Teste Hello/Handshake conclu√≠do!\\nüìä Resultados: \n${successCount} sucessos, ${failCount} falhas\\n‚è±Ô∏è Dura√ß√£o: \n${Math.round(finalSession.duration / 1000)}s`,\n-            timestamp: new Date().toISOString(),\n-            isSystemMessage: true\n-        });\n-\n-        // Cleanup session after 5 minutes\n-        setTimeout(() => {\n-            activeHelloSessions.delete(sessionId);\n-            logInfo('HELLO', 'Hello session cleaned up', { sessionId });\n-        }, 5 * 60 * 1000);\n-    }\n-}\n-\n-// Function to hello handshake with a single model\n+    const concurrencyLimit = 5;\n+    const tasks = models.map(modelId => async () => {\n+        try {\n+            const result = await helloSingleModel(sessionId, modelId);\n+            session.results[modelId] = result;\n+            session.completedModels++;\n+            broadcastHelloProgress(sessionId, session);\n+        } catch (error) {\n+            console.error(`[HELLO] Error during handshake with ${modelId}:`, \nerror);\n+            session.results[modelId] = { status: 'error', message: \nerror.message };\n+            session.completedModels++; // Still count as completed to avoid \ndeadlock\n+            broadcastHelloProgress(sessionId, session);\n+        }\n+    });\n+\n+    // Execute tasks with concurrency limit\n+    const executeWithLimit = async (tasks, limit) => {\n+        const executing = [];\n+        for (const task of tasks) {\n+            const p = task();\n+            executing.push(p);\n+            if (executing.length >= limit) {\n+                await Promise.race(executing.filter(p => p.status !== \n'pending')); // Wait for at least one to finish\n+            }\n+        }\n+        await Promise.all(executing);\n+    };\n+\n+    await executeWithLimit(tasks, concurrencyLimit);\n+\n+\n+    session.status = 'completed';\n+    console.log(`[HELLO] Hello handshake completed for session ${sessionId}`);\n+    broadcastHelloProgress(sessionId, session);\n+}\n+\n async function helloSingleModel(sessionId, modelId) {\n     const session = activeHelloSessions.get(sessionId);\n     if (!session) return;\n \n     const startTime = Date.now();\n@@ -2614,108 +1225,50 @@\n         sessionId: sessionId,\n         modelId: modelId\n     });\n \n     try {\n-        const helloPrompt = `Ol√° ${modelId}! Este √© um teste de \nconectividade/handshake. Por favor, responda brevemente confirmando que voc√™ \nrecebeu esta mensagem e se identifique.`;\n-\n-        // Send hello message via chat first\n-        broadcastChatMessage({\n-            type: 'chat_message',\n-            author: 'auto',\n-            text: `üì° Enviando hello para ${modelId}...`,\n-            timestamp: new Date().toISOString(),\n-            isSystemMessage: true\n-        });\n-\n-        const response = await callLLM(modelId, helloPrompt);\n-        const duration = Date.now() - startTime;\n-\n-        const result = {\n-            modelId: modelId,\n-            success: response && !response.includes('‚ùå') && response.length > \n10,\n-            response: response,\n+        let response;\n+        const prompt = `Responda apenas 'OK' para confirmar que a API est√° \nfuncionando.`;\n+\n+        if (modelId === 'auto') {\n+            // 'auto' model is special, we can't directly call it like others \nfor a simple ping.\n+            // We'll simulate a success or use a fallback.\n+            response = { output: 'OK (Simulated for auto model)' };\n+        } else if (MODEL_CATEGORIES.cursor_models.includes(modelId)) {\n+            // Use cursor-agent for cursor models\n+            response = await callLLMViaCursorAgent(modelId, prompt);\n+        } else {\n+            // Use Aider for other models\n+            response = await callLLMViaAider(modelId, prompt);\n+        }\n+\n+        const endTime = Date.now();\n+        const duration = endTime - startTime;\n+\n+        if (response.error) {\n+            throw new Error(response.error);\n+        }\n+\n+        // Validate response - expecting exactly 'OK'\n+        const isValid = response.output && response.output.trim().toLowerCase()\n=== 'ok';\n+\n+        return {\n+            status: isValid ? 'success' : 'failed',\n+            message: isValid ? 'OK' : `Expected 'OK', but received: \n${response.output}`,\n             duration: duration,\n-            timestamp: new Date().toISOString()\n+            modelId: modelId\n         };\n \n-        session.results.push(result);\n-        session.completedModels++;\n-\n-        if (result.success) {\n-            logInfo('HELLO', 'Hello handshake successful', {\n-                sessionId: sessionId,\n-                modelId: modelId,\n-                duration: duration,\n-                responseLength: response.length\n-            });\n-\n-            // Send success message via chat\n-            broadcastChatMessage({\n-                type: 'chat_message',\n-                author: 'auto',\n-                text: `‚úÖ ${modelId}: ${response.substring(0, \n100)}${response.length > 100 ? '...' : ''}`,\n-                timestamp: new Date().toISOString(),\n-                isSystemMessage: true\n-            });\n-        } else {\n-            logWarn('HELLO', 'Hello handshake failed', {\n-                sessionId: sessionId,\n-                modelId: modelId,\n-                duration: duration,\n-                response: response\n-            });\n-\n-            // Send failure message via chat\n-            broadcastChatMessage({\n-                type: 'chat_message',\n-                author: 'auto',\n-                text: `‚ùå ${modelId}: Falha na conectividade (${response || \n'sem resposta'})`,\n-                timestamp: new Date().toISOString(),\n-                isSystemMessage: true\n-            });\n-        }\n-\n-        // Broadcast progress update\n-        broadcastHelloProgress(sessionId, session);\n-\n     } catch (error) {\n-        const duration = Date.now() - startTime;\n-\n-        logError('HELLO', 'Hello handshake error', {\n-            sessionId: sessionId,\n-            modelId: modelId,\n-            error: error.message,\n-            duration: duration\n-        });\n-\n-        const result = {\n-            modelId: modelId,\n-            success: false,\n-            response: `Error: ${error.message}`,\n-            duration: duration,\n-            timestamp: new Date().toISOString()\n-        };\n-\n-        session.results.push(result);\n-        session.completedModels++;\n-\n-        // Send error message via chat\n-        broadcastChatMessage({\n-            type: 'chat_message',\n-            author: 'auto',\n-            text: `üí• ${modelId}: Erro na conectividade (${error.message})`,\n-            timestamp: new Date().toISOString(),\n-            isSystemMessage: true\n-        });\n-\n-        // Broadcast progress update\n-        broadcastHelloProgress(sessionId, session);\n-    }\n-}\n-\n-// Function to broadcast hello progress updates\n+        const endTime = Date.now();\n+        const duration = endTime - startTime;\n+        console.error(`[HELLO] Error testing model ${modelId}:`, error);\n+        throw { status: 'error', message: error.message || 'Unknown error', \nduration: duration, modelId: modelId };\n+    }\n+}\n+\n function broadcastHelloProgress(sessionId, session) {\n     const progressData = {\n         type: 'hello_progress',\n         sessionId: sessionId,\n         completed: session.completedModels,\n@@ -2723,45 +1276,30 @@\n         progress: Math.round((session.completedModels / session.totalModels) * \n100),\n         results: session.results,\n         status: session.status\n     };\n \n-    logDebug('HELLO', 'Broadcasting hello progress', {\n-        sessionId: sessionId,\n-        progress: progressData.progress,\n-        completed: session.completedModels,\n-        total: session.totalModels\n-    });\n-\n-    clients.forEach(client => {\n+    wss.clients.forEach(client => {\n         if (client.readyState === WebSocket.OPEN) {\n-            try {\n-                client.send(JSON.stringify(progressData));\n-            } catch (error) {\n-                logError('HELLO', 'Failed to broadcast hello progress', {\n-                    error: error.message,\n-                    sessionId: sessionId\n-                });\n-            }\n-        }\n-    });\n-}\n-\n-// Function to broadcast chat messages\n+            client.send(JSON.stringify(progressData));\n+        }\n+    });\n+}\n+\n function normalizeChatEnvelope(messageData) {\n     const text = typeof messageData.text === 'string'\n         ? messageData.text\n         : (typeof messageData.message === 'string'\n             ? messageData.message\n             : (typeof messageData.response === 'string' ? messageData.response \n: ''));\n \n     return {\n         type: messageData.type || 'chat_message',\n         author: messageData.author || 'Sistema',\n-        text: text || 'Mensagem vazia',\n-        timestamp: messageData.timestamp || new Date().toISOString(),\n-        isSystemMessage: messageData.isSystemMessage || false\n+        text: text,\n+        isSystemMessage: messageData.isSystemMessage || false,\n+        timestamp: messageData.timestamp || new Date().toISOString()\n     };\n }\n \n function broadcastChatMessage(messageData) {\n     const envelope = normalizeChatEnvelope(messageData);\n@@ -2772,117 +1310,81 @@\n         isSystemMessage: envelope.isSystemMessage\n     });\n \n     clients.forEach(client => {\n         if (client.readyState === WebSocket.OPEN) {\n-            try {\n-                client.send(JSON.stringify(envelope));\n-            } catch (error) {\n-                logError('CHAT', 'Failed to broadcast chat message', {\n-                    error: error.message,\n-                    author: envelope.author\n-                });\n-            }\n-        }\n-    });\n-}\n-\n-// Function to validate that a model isn't speaking for others\n+            client.send(JSON.stringify(envelope));\n+        }\n+    });\n+}\n+\n function validateModelResponse(modelId, response) {\n     if (!response || typeof response !== 'string') {\n         return null; // Let other validation handle empty responses\n     }\n \n     const lowerResponse = response.toLowerCase();\n \n     // Get list of all other model names to check against\n     const allModels = [\n-        ...MODEL_CATEGORIES.cursor_models,\n-        ...Object.keys(MODEL_CATEGORIES.aider_models)\n-    ].filter(model => model !== modelId && model !== 'auto');\n-\n-    // Critical validation: Check if model is claiming to be another model\n-    const forbiddenPhrases = [\n-        'como gpt-', 'como claude-', 'como gemini-', 'como anthropic/',\n-        'como openai/', 'como xai/', 'como deepseek', 'como groq/',\n-        'speaking as gpt-', 'speaking as claude-', 'as gpt-', 'as claude-',\n-        'sou o gpt-', 'sou o claude-', 'eu sou gpt-', 'eu sou claude-',\n-        'na perspectiva do gpt-', 'na perspectiva do claude-',\n-        'opini√£o do gpt-', 'opini√£o do claude-', 'opini√£o do gemini-'\n+        ...MODEL_CATEGORIES.generals,\n+        ...MODEL_CATEGORIES.bip_specific,\n+        ...MODEL_CATEGORIES.cursor_models\n     ];\n-\n-    // Special check for 'auto' model - it should NEVER claim to be specific \nmodels\n-    if (modelId === 'auto') {\n-        for (const otherModel of allModels) {\n-            const modelName = otherModel.toLowerCase().split('/').pop(); // Get\nlast part after /\n-            if (modelName && modelName !== 'auto' && \nlowerResponse.includes(`como ${modelName}`)) {\n-                return `Modelo 'auto' tentou se identificar como \n'${otherModel}'. Auto deve sempre solicitar via API, n√£o simular.`;\n-            }\n-        }\n-\n-        // Check for phrases indicating the model is role-playing as another\n-        if (forbiddenPhrases.some(phrase => lowerResponse.includes(phrase))) {\n-            return `Modelo 'auto' tentou simular outro modelo. Auto deve sempre\nfazer chamadas reais via API.`;\n-        }\n-    }\n-\n-    // Check if any model is using forbidden phrases to impersonate others\n-    for (const phrase of forbiddenPhrases) {\n-        if (lowerResponse.includes(phrase)) {\n-            // Allow only if the model is correctly identifying itself\n-            const expectedIdentity = `como ${modelId.toLowerCase()}`;\n-            if (!phrase.includes(modelId.toLowerCase()) && \nlowerResponse.includes(phrase)) {\n-                return `Modelo tentou se identificar incorretamente. Deve usar \napenas sua pr√≥pria identidade: ${modelId}`;\n-            }\n-        }\n-    }\n-\n-    // Check for attempts to provide multiple model perspectives in one \nresponse\n-    const multiModelIndicators = [\n-        'na perspectiva do claude', 'na perspectiva do gpt', 'na perspectiva do\ngemini',\n-        'segundo o claude', 'segundo o gpt', 'segundo o gemini',\n-        'de acordo com o claude', 'de acordo com o gpt', 'de acordo com o \ngemini',\n-        'consultando o claude', 'consultando o gpt', 'consultando o gemini'\n-    ];\n-\n-    for (const indicator of multiModelIndicators) {\n-        if (lowerResponse.includes(indicator) && \n!indicator.includes(modelId.toLowerCase())) {\n-            return `Modelo tentou falar por outros modelos. Cada modelo deve \nfornecer apenas sua pr√≥pria perspectiva.`;\n-        }\n-    }\n-\n-    return null; // Response is valid\n-}\n-\n-// Special handling for 'auto' model to prevent opinion simulation\n+    const otherModelNames = allModels.map(m => m.split('/').pop()).filter(name \n=> name); // Extract short names\n+\n+    // Rule 1: Check if the response is just a greeting or confirmation\n+    if (lowerResponse.includes('ok') || lowerResponse.includes('confirmado') ||\nlowerResponse.includes('entendido')) {\n+        // This is generally acceptable for simple tasks, but might be too \ngeneric for complex ones.\n+        // We'll allow it for now.\n+    }\n+\n+    // Rule 2: Check for self-identification that is incorrect\n+    for (const otherModel of otherModelNames) {\n+        if (modelId !== otherModel && lowerResponse.includes(`sou o \n${otherModel}`)) {\n+            return `Resposta inv√°lida: O modelo ${modelId} est√° se \nidentificando incorretamente como ${otherModel}.`;\n+        }\n+    }\n+\n+    // Rule 3: Check for refusal to answer without valid reason\n+    if (lowerResponse.includes('n√£o posso') || lowerResponse.includes('n√£o \nconsigo') || lowerResponse.includes('recuso-me')) {\n+        // This might be valid if the model has limitations, but could also be \na refusal.\n+        // More context needed to determine validity. For now, we'll allow it \nbut log a warning.\n+        logWarn('MODEL_VALIDATION', `Model ${modelId} response indicates \nrefusal or inability`, { response });\n+    }\n+\n+    // Rule 4: Check for specific forbidden phrases (if any)\n+    // Example: if (lowerResponse.includes('i cannot fulfill this request')) \nreturn 'Forbidden phrase detected.';\n+\n+    // If no issues found, return the cleaned response\n+    return response.trim();\n+}\n+\n async function handleAutoModelSafeguard(modelId, prompt) {\n     if (modelId === 'auto') {\n         // For 'auto' model, add extra safeguards in prompt\n         const autoSafeguardPrompt = `${prompt}\n \n AVISO CR√çTICO PARA MODELO AUTO:\n - Voc√™ √© o modelo 'auto' mediando a conversa\n - NUNCA forne√ßa opini√µes que simulariam outros modelos espec√≠ficos\n-- Se perguntado sobre outros modelos, responda: \"Para obter a opini√£o \nespec√≠fica de [modelo], farei uma chamada via API\"\n+- Se perguntado sobre outros modelos, responda: \"Para obter a opini√£o \nespec√≠fica de [modelo], farei\n - Sua fun√ß√£o √© mediar e facilitar, n√£o simular outros modelos\n-- Em coletas de opini√£o, voc√™ deve apenas coordenar as chamadas reais, n√£o \ninventar respostas`;\n-\n-        console.log(`[AUTO SAFEGUARD] Enhanced prompt for 'auto' model with \nstrict guidelines`);\n+`;\n         return autoSafeguardPrompt;\n     }\n-\n-    return prompt; // Return original prompt for other models\n+    return prompt; // Return original prompt if not 'auto'\n }\n \n function isSummaryRequest(text) {\n     const summaryTriggers = [\n         'resumo', 'summary', 'resumir', 'summarize',\n         'pontos principais', 'main points', 'consenso', 'consensus',\n         'gerar resumo', 'generate summary', 'synthesis', 's√≠ntese'\n     ];\n \n-    return summaryTriggers.some(trigger => text.includes(trigger));\n+    return summaryTriggers.some(trigger => \ntext.toLowerCase().includes(trigger));\n }\n \n async function handleSimpleResponse(text) {\n     console.log(`[DEBUG] Handling simple response for: \"${text}\"`);\n \n@@ -2891,549 +1393,278 @@\n         role: 'user',\n         content: text,\n         timestamp: new Date().toISOString()\n     });\n \n-    // Keep only last 10 messages in context\n-    if (sessionContext.length > 10) {\n-        sessionContext = sessionContext.slice(-10);\n-    }\n-\n-    // Always use 'auto' model for simple interactions\n-    const selectedModel = 'auto';\n+    // Select an appropriate model\n+    const modelToUse = selectAppropriateModel(text, 'general');\n+\n+    try {\n+        // Build prompt with context\n+        const prompt = buildSessionPrompt(text);\n+\n+        // Call the selected LLM\n+        const response = await callLLM(modelToUse, prompt);\n+\n+        if (response.error) {\n+            throw new Error(response.error);\n+        }\n+\n+        // Add model's response to context\n+        sessionContext.push({\n+            role: 'assistant',\n+            model: modelToUse,\n+            content: response.output,\n+            timestamp: new Date().toISOString()\n+        });\n+\n+        // Broadcast the response\n+        broadcastChatMessage({ author: modelToUse, text: response.output });\n+\n+    } catch (error) {\n+        logError('MESSAGE_HANDLER', 'Error in handleSimpleResponse', { error: \nerror.message });\n+        broadcastChatMessage({ author: 'System', text: `Desculpe, ocorreu um \nerro ao processar sua solicita√ß√£o: ${error.message}` });\n+    }\n+}\n+\n+async function handleGeneralContribution(text) {\n+    console.log(`[DEBUG] Handling general contribution for: \"${text}\"`);\n+\n+    try {\n+        // Step 1: Initial analysis with 'auto' model\n+        console.log(`[DEBUG] Step 1: Initial analysis with 'auto' model`);\n+\n+        broadcastChatMessage({\n+            type: 'typing',\n+            author: 'auto (an√°lise)',\n+            isSystemMessage: true\n+        });\n+\n+        const analysisPrompt = `Analise a seguinte contribui√ß√£o para o BIP-05: \n\"${text}\". Determine se √© uma sugest√£o de melhoria, um bug report, uma d√∫vida ou\noutra coisa. Identifique os principais pontos e sugira um modelo apropriado para\nprocessar essa contribui√ß√£o (ex: 'auto' para orquestra√ß√£o, um modelo espec√≠fico \npara an√°lise t√©cnica, etc.). Responda em formato JSON com as chaves 'type', \n'summary', 'model_suggestion'.`;\n+        const analysisResult = await callLLM('auto', analysisPrompt);\n+\n+        if (analysisResult.error) {\n+            throw new Error(`Failed to get analysis from 'auto' model: \n${analysisResult.error}`);\n+        }\n+\n+        let analysis;\n+        try {\n+            analysis = JSON.parse(analysisResult.output);\n+        } catch (e) {\n+            throw new Error(`Failed to parse analysis JSON from 'auto' model: \n${e.message}`);\n+        }\n+\n+        console.log('[CONTRIBUTION] Analysis:', analysis);\n+\n+        // Step 2: Process based on analysis\n+        let responseText = '';\n+        let modelToUse = analysis.model_suggestion || 'auto';\n+\n+        if (analysis.type === 'suggestion' || analysis.type === 'improvement') \n{\n+            responseText = `Obrigado pela sua sugest√£o de melhoria: \n\"${analysis.summary}\". Vou encaminhar isso para an√°lise mais aprofundada.`;\n+            // Potentially create a new issue or add to an existing one\n+            // For now, just acknowledge.\n+        } else if (analysis.type === 'bug_report') {\n+            responseText = `Obrigado por reportar o problema: \n\"${analysis.summary}\". Vamos investigar isso.`;\n+            // Potentially create a new issue\n+        } else {\n+            // Default handling or further processing\n+            responseText = `Entendido. Sua contribui√ß√£o: \n\"${analysis.summary}\".`;\n+        }\n+\n+        // If a specific model is suggested for processing, use it\n+        if (modelToUse && modelToUse !== 'auto') {\n+            console.log(`[CONTRIBUTION] Using suggested model: ${modelToUse}`);\n+            const contributionPrompt = await \nbuildGeneralContributionPrompt(text, analysis);\n+            const modelResponse = await callLLM(modelToUse, \ncontributionPrompt);\n+            if (modelResponse.output) {\n+                responseText += `\\n\\nAn√°lise do modelo \n${modelToUse}:\\n${modelResponse.output}`;\n+            }\n+        }\n+\n+        broadcastChatMessage({ author: 'auto (Processador)', text: responseText\n});\n+\n+    } catch (error) {\n+        logError('MESSAGE_HANDLER', 'Error handling general contribution', { \nerror: error.message });\n+        broadcastChatMessage({ author: 'System', text: `Desculpe, ocorreu um \nerro ao processar sua contribui√ß√£o: ${error.message}` });\n+    }\n+}\n+\n+async function handleSummaryRequest(text) {\n+    console.log(`[DEBUG] Handling summary request for: \"${text}\"`);\n+\n+    // Use a general model for summary\n+    const selectedModel = MODEL_CATEGORIES.generals[0]; // Use first general \nfor consistency\n \n     // Send typing indicator\n     broadcastChatMessage({\n         type: 'typing',\n         author: selectedModel,\n-        text: '',\n-        timestamp: new Date().toISOString()\n-    });\n-\n-    // Build prompt with session context\n-    const contextPrompt = buildSessionPrompt(text);\n+        isSystemMessage: true\n+    });\n \n     try {\n-        const response = await callLLM(selectedModel, contextPrompt);\n-\n-        // Stop typing indicator\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: selectedModel,\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        if (response) {\n-            // Add response to session context\n-            sessionContext.push({\n-                role: 'assistant',\n-                content: response,\n-                timestamp: new Date().toISOString(),\n-                model: selectedModel\n-            });\n-\n-            // Send response directly to chat (not to issues.json)\n-            broadcastChatMessage({\n-                type: 'simple_response',\n-                author: selectedModel,\n-                text: response,\n-                timestamp: new Date().toISOString()\n-            });\n-        }\n+        const issuesData = readIssuesFile();\n+        if (!issuesData || !issuesData.issues || issuesData.issues.length === \n0) {\n+            broadcastChatMessage({ author: 'System', text: 'N√£o h√° issues para \nresumir no momento.' });\n+            return;\n+        }\n+\n+        const prompt = buildSummaryPrompt(issuesData);\n+\n+        const response = await callLLM(selectedModel, prompt);\n+\n+        if (response.error) {\n+            throw new Error(response.error);\n+        }\n+\n+        broadcastChatMessage({ author: selectedModel, text: response.output });\n+\n     } catch (error) {\n-        console.error(`Error in simple response:`, error);\n-\n-        // Stop typing indicator on error\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: selectedModel,\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        broadcastChatMessage({\n-            type: 'error',\n-            text: `Erro ao gerar resposta: ${error.message}`,\n-            timestamp: new Date().toISOString()\n-        });\n-    }\n-}\n-\n-async function handleGeneralContribution(text) {\n-    console.log(`[DEBUG] Handling general contribution for: \"${text}\"`);\n-\n-    try {\n-        // Step 1: Initial analysis with 'auto' model\n-        console.log(`[DEBUG] Step 1: Initial analysis with 'auto' model`);\n-\n-        broadcastChatMessage({\n-            type: 'typing',\n-            author: 'auto (an√°lise)',\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        const analysisPrompt = `Analise esta solicita√ß√£o de contribui√ß√£o para o\nBIP-05 e determine:\n-1. Que tipo de contribui√ß√£o √© necess√°ria?\n-2. Qual modelo seria mais adequado para esta tarefa?\n-3. Que aspectos espec√≠ficos devem ser abordados?\n-\n-Solicita√ß√£o: \"${text}\"\n-\n-Responda de forma estruturada indicando o modelo recomendado e os pontos \nprincipais a abordar.`;\n-\n-        const analysis = await callLLM('auto', analysisPrompt);\n-\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: 'auto (an√°lise)',\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        // Send analysis result to chat\n-        broadcastChatMessage({\n-            type: 'simple_response',\n-            author: 'üîç auto (an√°lise)',\n-            text: analysis,\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        // Step 2: Select appropriate model for execution\n-        const generals = MODEL_CATEGORIES.generals;\n-        const selectedGeneral = generals[Math.floor(Math.random() * \ngenerals.length)];\n-\n-        console.log(`[DEBUG] Step 2: Executing with selected model: \n${selectedGeneral}`);\n-\n-        // Send typing indicator for execution model\n-        broadcastChatMessage({\n-            type: 'typing',\n-            author: selectedGeneral,\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        // Build comprehensive prompt with BIP context + analysis\n-        const fullPrompt = await buildGeneralContributionPrompt(text, \nanalysis);\n-\n-        const response = await callLLM(selectedGeneral, fullPrompt);\n-\n-        // Stop typing indicator\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: selectedGeneral,\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        if (response) {\n-            // Add to issues.json\n-            const comment = {\n-                author: selectedGeneral,\n-                created_at: new Date().toISOString(),\n-                locale: 'pt-BR',\n-                body: sanitizeForJSON(response),\n-                body_original: sanitizeForJSON(response)\n-            };\n-\n-            const issuesData = JSON.parse(fs.readFileSync(issuesFile, 'utf8'));\n-            if (issuesData.issues && issuesData.issues.length > 0) {\n-                issuesData.issues[0].comments.push(comment);\n-            } else {\n-                issuesData.issues = [{ id: 1, title: 'Main Thread', comments: \n[comment] }];\n-            }\n-\n-            fs.writeFileSync(issuesFile, JSON.stringify(issuesData, null, 2), \n'utf8');\n-\n-            // Also send to chat\n-            broadcastChatMessage({\n-                type: 'general_contribution',\n-                author: selectedGeneral,\n-                text: response,\n-                timestamp: new Date().toISOString(),\n-                added_to_issues: true\n-            });\n-        }\n-    } catch (error) {\n-        console.error(`Error in general contribution:`, error);\n-\n-        // Stop any active typing indicators\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: 'system',\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        broadcastChatMessage({\n-            type: 'error',\n-            text: `Erro ao gerar contribui√ß√£o: ${error.message}`,\n-            timestamp: new Date().toISOString()\n-        });\n-    }\n-}\n-\n-async function handleSummaryRequest(text) {\n-    console.log(`[DEBUG] Handling summary request for: \"${text}\"`);\n-\n-    // Use a general model for summary\n-    const selectedModel = MODEL_CATEGORIES.generals[0]; // Use first general \nfor consistency\n-\n-    // Send typing indicator\n-    broadcastChatMessage({\n-        type: 'typing',\n-        author: selectedModel,\n-        text: '',\n-        timestamp: new Date().toISOString()\n-    });\n-\n-    try {\n-        // Read all discussions from issues.json\n-        const issuesData = JSON.parse(fs.readFileSync(issuesFile, 'utf8'));\n-\n-        // Build summary prompt\n-        const summaryPrompt = buildSummaryPrompt(issuesData);\n-\n-        const summary = await callLLM(selectedModel, summaryPrompt);\n-\n-        // Stop typing indicator\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: selectedModel,\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        if (summary) {\n-            // Generate summary file\n-            const summaryFile = path.join(__dirname, '..', \n'discussion-summary.md');\n-            const summaryContent = `# Resumo das Discuss√µes BIP-05\\n\\n**Gerado \nem:** ${new Date().toISOString()}\\n**Modelo:** \n${selectedModel}\\n\\n---\\n\\n${summary}`;\n-\n-            fs.writeFileSync(summaryFile, summaryContent, 'utf8');\n-\n-            // Send confirmation to chat\n-            broadcastChatMessage({\n-                type: 'summary_generated',\n-                author: selectedModel,\n-                text: `Resumo das discuss√µes gerado com sucesso! Arquivo salvo \nem: discussion-summary.md\\n\\n**Principais pontos:**\\n${summary.slice(0, \n500)}...`,\n-                timestamp: new Date().toISOString(),\n-                file_generated: 'discussion-summary.md'\n-            });\n-        }\n-    } catch (error) {\n-        console.error(`Error generating summary:`, error);\n-\n-        // Stop typing indicator on error\n-        broadcastChatMessage({\n-            type: 'stop_typing',\n-            author: selectedModel,\n-            text: '',\n-            timestamp: new Date().toISOString()\n-        });\n-\n-        broadcastChatMessage({\n-            type: 'error',\n-            text: `Erro ao gerar resumo: ${error.message}`,\n-            timestamp: new Date().toISOString()\n-        });\n+        logError('MESSAGE_HANDLER', 'Error handling summary request', { error: \nerror.message });\n+        broadcastChatMessage({ author: 'System', text: `Desculpe, ocorreu um \nerro ao gerar o resumo: ${error.message}` });\n     }\n }\n \n function buildSessionPrompt(userText) {\n     const contextMessages = sessionContext.slice(-6); // Last 6 messages\n \n-    let prompt = `Voc√™ √© um assistente especializado em BIP-05 (Universal \nMatrix Protocol). Responda de forma direta e objetiva em portugu√™s.\\n\\n`;\n+    let prompt = `Voc√™ √© um assistente especializado em BIP-05 (Universal \nMatrix Protocol). Respond\n+    \n+    Seja conciso e direto ao ponto. Mantenha o contexto da conversa.\n+    `;\n \n     if (contextMessages.length > 0) {\n-        prompt += `Contexto da conversa:\\n`;\n+        prompt += `\\nContexto da conversa:\\n`;\n         contextMessages.forEach(msg => {\n             const role = msg.role === 'user' ? 'Master' : `${msg.model || \n'Assistant'}`;\n             prompt += `- ${role}: ${msg.content}\\n`;\n         });\n-        prompt += `\\n`;\n-    }\n-\n-    prompt += `Pergunta atual: ${userText}\\n\\nResponda de forma clara e concisa\n(m√°ximo 3 par√°grafos):`;\n-\n+    }\n+\n+    prompt += `\\n\\nMensagem atual do Usu√°rio: ${userText}\\nAssistente:`;\n     return prompt;\n }\n \n async function buildGeneralContributionPrompt(userText, analysis = null) {\n     // Read BIP and implementation files\n-    const bipContent = fs.readFileSync(bipFile, 'utf8');\n-    const implementationContent = fs.readFileSync(implementationFile, 'utf8');\n-    const issuesData = JSON.parse(fs.readFileSync(issuesFile, 'utf8'));\n+    const bipFile = path.join(__dirname, '..', '..', '..', '..', 'gov', 'bips',\n'BIP-05', 'README.md');\n+    const implementationFile = path.join(__dirname, '..', '..', '..', '..', \n'implementation.md'); // Assuming this exists\n+    const issuesFile = ISSUES_FILE;\n+\n+    let bipContent = '';\n+    if (fs.existsSync(bipFile)) {\n+        bipContent = fs.readFileSync(bipFile, 'utf8');\n+    }\n+\n+    let implementationContent = '';\n+    if (fs.existsSync(implementationFile)) {\n+        implementationContent = fs.readFileSync(implementationFile, 'utf8');\n+    }\n+\n+    const issuesData = readIssuesFile();\n \n     // Get recent discussions\n     const recentComments = [];\n     (issuesData.issues || []).forEach(issue => {\n-        (issue.comments || []).slice(-5).forEach(c => {\n-            recentComments.push(`${c.author}: ${c.body}`);\n-        });\n-    });\n-\n-    let prompt = `Voc√™ √© um modelo AI expert contribuindo para a discuss√£o \noficial do BIP-05 (Universal Matrix Protocol).\n-\n-**CONTEXTO COMPLETO DO BIP:**\n-${bipContent}\n-\n-**PLANO DE IMPLEMENTA√á√ÉO:**\n-${implementationContent}\n-\n-**DISCUSS√ïES RECENTES:**\n-${recentComments.join('\\n')}\n-\n-**SOLICITA√á√ÉO DO MASTER:**\n-${userText}`;\n-\n-    if (analysis) {\n-        prompt += `\n-\n-**AN√ÅLISE PRELIMINAR:**\n-${analysis}`;\n-    }\n-\n-    prompt += `\n-\n-**INSTRU√á√ïES:**\n-1. Forne√ßa uma contribui√ß√£o t√©cnica substantiva para a discuss√£o\n-2. Responda em portugu√™s E ingl√™s (formato: PT: ... / EN: ...)\n-3. Seja espec√≠fico e t√©cnico, focando em implementa√ß√£o\n-4. Considere o contexto das discuss√µes anteriores\n-5. Sugira melhorias concretas quando relevante\n-6. Se h√° an√°lise preliminar, use-a como guia mas desenvolva pontos adicionais\n-\n-**SUA CONTRIBUI√á√ÉO OFICIAL:**`;\n+        (issue.comments || []).slice(-5).forEach(c => { // Last 5 comments per \nissue\n+            recentComments.push(`- Issue #${issue.id || issue.number} \n(${issue.title}): ${c.author}: ${c.body?.substring(0, 200)}...`);\n+        });\n+    });\n+\n+    let prompt = `Voc√™ √© um assistente de IA focado em auxiliar no \ndesenvolvimento e documenta√ß√£o do BIP-05 (Universal Matrix Protocol).\n+    \n+    O usu√°rio fez uma contribui√ß√£o: \"${userText}\".\n+    An√°lise preliminar: ${JSON.stringify(analysis)}\n+\n+    Contexto relevante:\n+    --- BIP-05 README ---\n+    ${bipContent.slice(0, 1000)}...\n+    --- Implementa√ß√£o ---\n+    ${implementationContent.slice(0, 1000)}...\n+    --- Discuss√µes Recentes em Issues ---\n+    ${recentComments.join('\\n')}\n+    \n+    Por favor, processe a contribui√ß√£o do usu√°rio com base neste contexto. Se \nfor uma sugest√£o, avalie sua viabilidade. Se for um bug, tente identificar a \ncausa. Seja t√©cnico e objetivo.\n+    `;\n \n     return prompt;\n }\n \n function buildSummaryPrompt(issuesData) {\n     const allComments = [];\n     (issuesData.issues || []).forEach(issue => {\n-        allComments.push(`## ${issue.title}`);\n+        allComments.push(`## ${issue.title} (Issue #${issue.id || \nissue.number})`);\n+        if (issue.body) {\n+            allComments.push(`*Descri√ß√£o:* ${issue.body.substring(0, \n300)}...`);\n+        }\n         (issue.comments || []).forEach(c => {\n             allComments.push(`**${c.author}:** ${c.body}`);\n         });\n         allComments.push('---');\n     });\n \n-    return `Voc√™ √© um expert t√©cnico analisando todas as discuss√µes do BIP-05 \n(Universal Matrix Protocol).\n-\n-**MASTER COMMENT:**\n-${issuesData.master_comment?.body || 'N/A'}\n-\n-**TODAS AS DISCUSS√ïES:**\n-${allComments.join('\\n')}\n-\n-**TAREFA:**\n-Gere um resumo executivo completo com:\n-\n-1. **PRINCIPAIS PONTOS DE CONSENSO** - O que todos concordam\n-2. **RECOMENDA√á√ïES T√âCNICAS CONSOLIDADAS** - Principais especifica√ß√µes \nacordadas\n-3. **QUEST√ïES EM ABERTO** - Pontos que precisam ser decididos\n-4. **PR√ìXIMOS PASSOS** - Direcionamento claro para implementa√ß√£o\n-5. **PARTICIPANTES E CONTRIBUI√á√ïES** - Quem contribuiu com o qu√™\n-\n-Seja objetivo, t√©cnico e forne√ßa um resumo acion√°vel para o modelo que ir√° \nimplementar o BIP.\n-\n-**RESUMO EXECUTIVO:**`;\n-}\n-\n-// Legacy function removed - using unified broadcastChatMessage\n-\n-// Legacy functions removed - using new action-based system\n-\n-function addCommentToFile(text, author) {\n-    logInfo('FILE_WRITE', 'Starting comment addition to issues.json', {\n-        author: author,\n-        textLength: text.length,\n-        issuesFile: issuesFile\n-    });\n-\n-    try {\n-        const rawData = fs.readFileSync(issuesFile, 'utf8');\n-        logDebug('FILE_WRITE', 'Successfully read issues.json', {\n-            dataLength: rawData.length,\n-            author: author\n-        });\n-\n-        const issuesData = JSON.parse(rawData);\n-        logDebug('FILE_WRITE', 'Successfully parsed issues.json', {\n-            issuesCount: issuesData.issues ? issuesData.issues.length : 0,\n-            author: author\n-        });\n-\n-        const comment = {\n-            author: author,\n-            created_at: new Date().toISOString(),\n-            locale: \"pt-BR\",\n-            body: sanitizeForJSON(text),\n-            body_original: sanitizeForJSON(text)\n-        };\n-\n-        logDebug('FILE_WRITE', 'Created comment object', {\n-            author: comment.author,\n-            created_at: comment.created_at,\n-            bodyLength: comment.body.length,\n-            sanitized: text !== comment.body\n-        });\n-\n-        if (issuesData.issues && issuesData.issues.length > 0) {\n-            issuesData.issues[0].comments.push(comment);\n-            logDebug('FILE_WRITE', 'Added comment to existing issue', {\n-                issueId: issuesData.issues[0].id,\n-                commentsCount: issuesData.issues[0].comments.length,\n-                author: author\n-            });\n-        } else {\n-            issuesData.issues = [{ id: 1, title: \"Main Thread\", comments: \n[comment] }];\n-            logDebug('FILE_WRITE', 'Created new issue with comment', {\n-                issueId: 1,\n-                author: author\n-            });\n-        }\n-\n-        const jsonString = JSON.stringify(issuesData, null, 2);\n-        fs.writeFileSync(issuesFile, jsonString, 'utf8');\n-\n-        logInfo('FILE_WRITE', 'Successfully added comment to issues.json', {\n-            author: author,\n-            finalFileSize: jsonString.length,\n-            totalComments: issuesData.issues[0].comments.length\n-        });\n-    } catch (error) {\n-        logError('FILE_WRITE', 'Error adding comment to issues.json', {\n-            author: author,\n-            error: error.message,\n-            errorStack: error.stack,\n-            textLength: text.length\n-        });\n-    }\n-}\n-\n-// Watch for file changes\n-let fileWatcher = null;\n-let debounceTimeout = null;\n-\n-function startFileWatcher() {\n-  if (fileWatcher) {\n-    fileWatcher.close();\n-  }\n-\n-  fileWatcher = fs.watch(issuesFile, { persistent: true }, (eventType) => {\n-    if (eventType === 'change') {\n-            console.log('issues.json changed, queueing broadcast...');\n-\n-            // Clear the previous timeout if a new change event comes in\n-            clearTimeout(debounceTimeout);\n-\n-            // Set a new timeout\n-            debounceTimeout = setTimeout(() => {\n-                console.log('Debounce timer elapsed, now broadcasting.');\n-                broadcastIssues();\n-            }, 150); // Increased delay slightly for more stability\n-    }\n-  });\n-\n-  console.log('File watcher started for:', issuesFile);\n-}\n-\n-// Initial broadcast and start watching\n-setTimeout(() => {\n-  broadcastIssues();\n-  startFileWatcher();\n-}, 1000);\n-\n-// Enhanced graceful shutdown handling\n-let shuttingDown = false;\n-\n+    return `Com base nas seguintes discuss√µes de issues do BIP-05, forne√ßa um \nresumo conciso dos principais pontos, consensos e √°reas de discord√¢ncia. Foque \nnos aspectos t√©cnicos e nas decis√µes de design.\n+\n+    ${allComments.join('\\n')}\n+\n+    Resumo:`;\n+}\n+\n+// --- Graceful Shutdown ---\n function gracefulShutdown(signal) {\n-  if (shuttingDown) {\n-    console.log(`[SHUTDOWN] Already shutting down, ignoring ${signal}`);\n-    return;\n-  }\n-\n-  shuttingDown = true;\n-  console.log(`[SHUTDOWN] üõë Received ${signal}, shutting down gracefully...`);\n-\n-  // Stop file watcher\n-  if (fileWatcher) {\n-    console.log('[SHUTDOWN] üìÅ Closing file watcher...');\n-    fileWatcher.close();\n-  }\n-\n-  // Close WebSocket connections\n-  console.log('[SHUTDOWN] üì° Closing WebSocket connections...');\n-  clients.forEach(client => {\n-    try {\n-      if (client.readyState === WebSocket.OPEN) {\n-        client.close(1000, 'Server shutdown');\n-      }\n-    } catch (err) {\n-      console.error('[SHUTDOWN] Error closing WebSocket client:', err);\n-    }\n-  });\n-  clients.clear();\n-\n-  // Close WebSocket server\n-  if (wss) {\n-    console.log('[SHUTDOWN] üîå Closing WebSocket server...');\n-    wss.close(() => {\n-      console.log('[SHUTDOWN] ‚úÖ WebSocket server closed');\n-    });\n-  }\n-\n-  // Close HTTP server\n-  console.log('[SHUTDOWN] üåê Closing HTTP server...');\n-  server.close((err) => {\n-    if (err) {\n-      console.error('[SHUTDOWN] ‚ùå Error closing server:', err);\n-      process.exit(1);\n-    }\n-    console.log('[SHUTDOWN] ‚úÖ HTTP server closed');\n-    console.log('[SHUTDOWN] üéØ Graceful shutdown complete');\n-    process.exit(0);\n-  });\n-\n-  // Force exit after 3 seconds if graceful shutdown fails\n-  setTimeout(() => {\n-    console.log('[SHUTDOWN] ‚è∞ Force exit after timeout');\n-    process.exit(1);\n-  }, 3000);\n-}\n-\n-// Handle multiple SIGINT (Ctrl+C) presses with proper shutdown\n-let ctrlCCount = 0;\n-process.on('SIGINT', () => {\n-  ctrlCCount++;\n-  if (ctrlCCount === 1) {\n-    gracefulShutdown('SIGINT (Ctrl+C)');\n-  } else if (ctrlCCount >= 2) {\n-    console.log('[SHUTDOWN] üí• Multiple Ctrl+C detected, forcing immediate \nexit');\n-    process.exit(1);\n-  }\n-});\n-\n-// Handle other shutdown signals\n-process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));\n-process.on('SIGUSR2', () => gracefulShutdown('SIGUSR2 (nodemon)')); // nodemon \nrestart\n-\n-// Handle uncaught exceptions and prevent hanging\n-process.on('uncaughtException', (err) => {\n-  console.error('[FATAL] Uncaught Exception:', err);\n-  gracefulShutdown('UNCAUGHT_EXCEPTION');\n-});\n-\n-process.on('unhandledRejection', (reason, promise) => {\n-  console.error('[FATAL] Unhandled Rejection at:', promise, 'reason:', reason);\n-  gracefulShutdown('UNHANDLED_REJECTION');\n-});\n-\n-// Start server\n-const PORT = process.env.PORT || 3000;\n-server.listen(PORT, () => {\n-  console.log(`Server running at http://localhost:${PORT}`);\n-    console.log(`API endpoint: http://localhost:${PORT}/api/comment`);\n-  console.log(`Monitoring: ${issuesFile}`);\n-});\n+    if (shuttingDown) {\n+        console.log(`[SHUTDOWN] Already shutting down, ignoring ${signal}`);\n+        return;\n+    }\n+\n+    shuttingDown = true;\n+    console.log(`[SHUTDOWN] üõë Received ${signal}, shutting down \ngracefully...`);\n+\n+    // Stop file watcher\n+    if (fileWatcher) {\n+        fileWatcher.close();\n+        console.log('[SHUTDOWN] File watcher stopped.');\n+    }\n+\n+    // Close WebSocket connections\n+    if (wss) {\n+        wss.close((err) => {\n+            if (err) {\n+                console.error('[SHUTDOWN] Error closing WebSocket server:', \nerr);\n+            } else {\n+                console.log('[SHUTDOWN] WebSocket server closed.');\n+            }\n+        });\n+    }\n+\n+    // Give some time for connections to close\n+    setTimeout(() => {\n+        console.log('[SHUTDOWN] Shutdown complete.');\n+        process.exit(0);\n+    }, 1000); // 1 second delay\n+}\n+\n+// --- Initialization ---\n+async function initializeServer() {\n+    console.log(colors.bright.cyan('Starting BIP-05 Monitor Server...'));\n+\n+    loadEnvironment();\n+    const keysValid = validateApiKeys();\n+    if (!keysValid) {\n+        console.warn(colors.yellow('‚ö†Ô∏è Warning: Some API keys are missing. \nFunctionality may be limited.'));\n+    }\n+\n+    const apiTestResults = await testApiConnectivity();\n+    updateAvailableModels(apiTestResults.workingApis);\n+\n+    setupWebSocketServer();\n+    startFileWatcher();\n+\n+    // Handle process signals for graceful shutdown\n+    process.on('SIGINT', () => gracefulShutdown('SIGINT'));\n+    process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));\n+\n+    console.log(colors.green('BIP-05 Monitor Server initialized \nsuccessfully.'));\n+}\n+\n+initializeServer();\n```\n\n\n\ngov/bips/BIP-05/monitor/test-all-models.js\n\n```diff\n@@ -1,213 +1,197 @@\n-#!/usr/bin/env node\n-\n-/**\n- * Script para testar todos os modelos dispon√≠veis no BIP-05 Monitor\n- * Envia uma mensagem \"hello\" para cada modelo e verifica se as APIs est√£o \nfuncionando\n- */\n-\n const fs = require('fs');\n const path = require('path');\n-const { exec } = require('child_process');\n-const util = require('util');\n+const colors = require('colors');\n \n-const execAsync = util.promisify(exec);\n+// --- Configuration ---\n+const ALL_MODELS_CONFIG_PATH = path.join(__dirname, '..', '..', '..', '..', \n'model_config.json'); // Assuming a central config file\n+const ENV_PATH = path.join(__dirname, '.env'); // Local .env for testing script\n \n-// Cores para output\n-const colors = {\n-    reset: '\\x1b[0m',\n-    bright: '\\x1b[1m',\n-    red: '\\x1b[31m',\n-    green: '\\x1b[32m',\n-    yellow: '\\x1b[33m',\n-    blue: '\\x1b[34m',\n-    magenta: '\\x1b[35m',\n-    cyan: '\\x1b[36m'\n+// --- Mock Model Definitions (Should ideally be loaded from a shared config) \n---\n+// This is a simplified version. In a real scenario, this would be loaded \ndynamically.\n+const MODEL_CATEGORIES = {\n+    generals: ['openai/gpt-4o', 'anthropic/claude-3-opus-20240229', \n'google/gemini-1.5-pro-latest'],\n+    cursor_models: ['cursor/gpt-4-turbo'],\n+    bip_specific: ['xai/bip-model-v1'],\n+    aider_models: {\n+        'openai/gpt-4o': { provider: 'openai', key: 'OPENAI_API_KEY', model: \n'gpt-4o' },\n+        'anthropic/claude-3-opus-20240229': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-opus-20240229' },\n+        'google/gemini-1.5-pro-latest': { provider: 'google', key: \n'GEMINI_API_KEY', model: 'gemini-1.5-pro-latest' },\n+        'xai/bip-model-v1': { provider: 'xai', key: 'XAI_API_KEY', model: \n'bip-model-v1' },\n+        'deepseek/coder': { provider: 'deepseek', key: 'DEEPSEEK_API_KEY', \nmodel: 'coder' },\n+        'groq/llama3-70b': { provider: 'groq', key: 'GROQ_API_KEY', model: \n'llama3-70b' },\n+        'cursor/gpt-4-turbo': { provider: 'cursor', key: null, model: \n'gpt-4-turbo' } // Cursor models handled differently\n+    }\n };\n \n-// Carregar vari√°veis de ambiente\n+// --- Helper Functions ---\n+\n function loadEnvironment() {\n-    const envPath = path.join(__dirname, '.env');\n-    if (fs.existsSync(envPath)) {\n-        const envContent = fs.readFileSync(envPath, 'utf8');\n+    if (fs.existsSync(ENV_PATH)) {\n+        console.log(`[ENV] Loading local .env file from ${ENV_PATH}...`);\n+        const envContent = fs.readFileSync(ENV_PATH, 'utf8');\n         envContent.split('\\n').forEach(line => {\n-            const [key, value] = line.split('=');\n-            if (key && value) {\n-                process.env[key.trim()] = value.trim();\n+            if (line.trim() && !line.startsWith('#')) {\n+                const [key, value] = line.split('=');\n+                if (key && value !== undefined) {\n+                    process.env[key.trim()] = value.trim();\n+                }\n             }\n         });\n-        console.log(`${colors.green}[ENV] ‚úÖ Arquivo .env \ncarregado${colors.reset}`);\n     } else {\n-        console.log(`${colors.yellow}[ENV] ‚ö†Ô∏è  Arquivo .env n√£o \nencontrado${colors.reset}`);\n-        console.log(`${colors.yellow}[ENV] Copie o conte√∫do de env-example.txt \npara .env e adicione suas API keys${colors.reset}`);\n+        console.log('[ENV] Local .env file not found. Using system environment \nvariables.');\n+    }\n+    // Also load from the parent directory's .env if it exists and local one \ndoesn't\n+    const parentEnvPath = path.join(__dirname, '..', '..', '..', '..', '.env');\n+     if (!fs.existsSync(ENV_PATH) && fs.existsSync(parentEnvPath)) {\n+        console.log(`[ENV] Loading parent .env file from ${parentEnvPath}...`);\n+        const envContent = fs.readFileSync(parentEnvPath, 'utf8');\n+        envContent.split('\\n').forEach(line => {\n+            if (line.trim() && !line.startsWith('#')) {\n+                const [key, value] = line.split('=');\n+                if (key && value !== undefined) {\n+                    process.env[key.trim()] = value.trim();\n+                }\n+            }\n+        });\n     }\n }\n \n-// Lista de todos os modelos dispon√≠veis\n-const ALL_MODELS = {\n-    // Cursor-agent models (built-in)\n-    cursor_models: [\n-        'auto',\n-        'gpt-5', \n-        'sonnet-4', \n-        'opus-4.1'\n-    ],\n-    \n-    // Aider models (external APIs)\n-    aider_models: {\n-        // OpenAI\n-        'openai/gpt-4o': { provider: 'openai', key: 'OPENAI_API_KEY', model: \n'gpt-4o' },\n-        'openai/gpt-4o-mini': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-4o-mini' },\n-        'openai/o1-mini': { provider: 'openai', key: 'OPENAI_API_KEY', model: \n'o1-mini' },\n-        'openai/gpt-4-turbo': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-4-turbo' },\n-        'openai/gpt-5-mini': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-5-mini' },\n-        'openai/gpt-5-nano': { provider: 'openai', key: 'OPENAI_API_KEY', \nmodel: 'gpt-5-nano' },\n-\n-        // Anthropic\n-        'anthropic/claude-3-5-haiku-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-5-haiku-latest' },\n-        'anthropic/claude-3-5-sonnet-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-5-sonnet-latest' },\n-        'anthropic/claude-3-opus-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-opus-latest' },\n-        'anthropic/claude-3-7-sonnet-latest': { provider: 'anthropic', key: \n'ANTHROPIC_API_KEY', model: 'claude-3-7-sonnet-latest' },\n-\n-        // Gemini (Google)\n-        'gemini/gemini-2.0-flash-lite': { provider: 'gemini', key: \n'GEMINI_API_KEY', model: 'gemini-2.0-flash-lite' },\n-        'gemini/gemini-2.0-flash': { provider: 'gemini', key: 'GEMINI_API_KEY',\nmodel: 'gemini-2.0-flash' },\n-        'gemini/gemini-2.5-pro-latest': { provider: 'gemini', key: \n'GEMINI_API_KEY', model: 'gemini-2.5-pro-latest' },\n-        'gemini/gemini-2.5-flash-latest': { provider: 'gemini', key: \n'GEMINI_API_KEY', model: 'gemini-2.5-flash-latest' },\n-\n-        // xAI (Grok)\n-        'xai/grok-3-mini': { provider: 'xai', key: 'XAI_API_KEY', model: \n'grok-3-mini' },\n-        'xai/grok-3': { provider: 'xai', key: 'XAI_API_KEY', model: 'grok-3' },\n-        'xai/grok-beta': { provider: 'xai', key: 'XAI_API_KEY', model: \n'grok-beta' },\n-\n-        // DeepSeek\n-        'deepseek/deepseek-chat': { provider: 'deepseek', key: \n'DEEPSEEK_API_KEY', model: 'deepseek-chat' },\n-\n-        // Groq\n-        'groq/llama-3.1-70b-versatile': { provider: 'groq', key: \n'GROQ_API_KEY', model: 'llama-3.1-70b-versatile' },\n-        'groq/llama-3.1-8b-instant': { provider: 'groq', key: 'GROQ_API_KEY', \nmodel: 'llama-3.1-8b-instant' },\n-        'groq/llama-3.3-70b-versatile': { provider: 'groq', key: \n'GROQ_API_KEY', model: 'llama-3.3-70b-versatile' },\n-        'groq/openai/gpt-oss-120': { provider: 'groq', key: 'GROQ_API_KEY', \nmodel: 'openai/gpt-oss-120' },\n-        'groq/qwen/qwen3-32b': { provider: 'groq', key: 'GROQ_API_KEY', model: \n'qwen/qwen3-32b' }\n-    }\n-};\n-\n-// Fun√ß√£o para chamar modelo via aider\n async function callModelViaAider(modelId, message) {\n+    const { spawn } = require('child_process');\n     try {\n-        const modelConfig = ALL_MODELS.aider_models[modelId];\n+        const modelConfig = MODEL_CATEGORIES.aider_models[modelId];\n         if (!modelConfig) {\n             return `‚ùå Modelo ${modelId} n√£o encontrado na configura√ß√£o`;\n         }\n \n         const apiKey = process.env[modelConfig.key];\n         if (!apiKey) {\n             return `‚ùå API key n√£o configurada para ${modelConfig.provider}`;\n         }\n \n-        // Comando aider para testar o modelo\n-        const aiderCmd = `aider --model ${modelId} --yes \"Responda apenas \n'Hello, API funcionando!' para confirmar conectividade.\"`;\n-        \n-        const { stdout, stderr } = await execAsync(aiderCmd, {\n-            timeout: 30000, // 30 segundos timeout\n-            env: { ...process.env }\n+        const command = 'aider'; // Assuming 'aider' is in the PATH\n+        const args = [\n+            '--model', modelConfig.model,\n+            '--prompt', message,\n+            '--output', 'json' // Request JSON output for easier parsing\n+        ];\n+\n+        return new Promise((resolve, reject) => {\n+            const aiderProcess = spawn(command, args, { env: process.env });\n+            let stdout = '';\n+            let stderr = '';\n+\n+            aiderProcess.stdout.on('data', (data) => {\n+                stdout += data.toString();\n+            });\n+\n+            aiderProcess.stderr.on('data', (data) => {\n+                stderr += data.toString();\n+            });\n+\n+            aiderProcess.on('close', (code) => {\n+                if (code === 0) {\n+                    try {\n+                        const result = JSON.parse(stdout);\n+                        // For testing, we just need to know if it ran without \nerror\n+                        resolve(`‚úÖ ${modelId} - Aider call successful.`);\n+                    } catch (parseError) {\n+                        resolve(`‚ö†Ô∏è ${modelId} - Aider ran but output parsing \nfailed: ${parseError.message}`);\n+                    }\n+                } else {\n+                    resolve(`‚ùå ${modelId} - Aider process failed with code \n${code}. Stderr: ${stderr}`);\n+                }\n+            });\n+\n+            aiderProcess.on('error', (err) => {\n+                resolve(`‚ùå ${modelId} - Failed to start Aider process: \n${err.message}`);\n+            });\n         });\n-\n-        if (stderr && stderr.includes('error')) {\n-            return `‚ùå Erro: ${stderr}`;\n-        }\n-\n-        return stdout.trim() || '‚úÖ Resposta recebida (sem conte√∫do vis√≠vel)';\n-        \n     } catch (error) {\n-        return `‚ùå Erro: ${error.message}`;\n+        return `‚ùå ${modelId} - Exception during Aider call: ${error.message}`;\n     }\n }\n \n-// Fun√ß√£o para testar modelo cursor-agent (simulado)\n async function testCursorAgentModel(modelId) {\n-    // Para modelos cursor-agent, simulamos uma resposta positiva\n-    // pois eles s√£o built-in e n√£o precisam de API externa\n-    return `‚úÖ Cursor-agent model ${modelId} - Built-in (sempre dispon√≠vel)`;\n+    // For cursor-agent models, we simulate a positive response\n+    // as they are built-in and don't require external API keys for basic \ntesting.\n+    // A more thorough test would involve actually invoking cursor-agent.\n+    return `‚úÖ Cursor-agent model ${modelId} - Built-in (simulated success)`;\n }\n \n-// Fun√ß√£o principal de teste\n async function testAllModels() {\n-    console.log(`${colors.bright}${colors.cyan}üöÄ BIP-05 Monitor - Teste de \nConectividade de Todos os Modelos${colors.reset}\\n`);\n-    \n-    loadEnvironment();\n-    \n+    console.log(`${colors.bright}${colors.cyan}üöÄ BIP-05 Monitor - Teste de \nConectividade de Todos os Modelos${colors.reset}`);\n+    console.log('===========================================================');\n+\n+    loadEnvironment(); // Load .env variables\n+\n     const results = {\n         working: [],\n         failed: [],\n         skipped: []\n     };\n \n-    // Testar modelos cursor-agent\n-    console.log(`${colors.blue}üìã Testando modelos Cursor-Agent \n(Built-in):${colors.reset}`);\n-    for (const model of ALL_MODELS.cursor_models) {\n-        console.log(`\\n${colors.yellow}üîç Testando \n${model}...${colors.reset}`);\n-        const result = await testCursorAgentModel(model);\n-        console.log(`${colors.green}${result}${colors.reset}`);\n-        results.working.push({ model, type: 'cursor-agent', result });\n-    }\n+    const testMessage = `Responda apenas 'OK' para confirmar que a API est√° \nfuncionando.`;\n \n-    // Testar modelos aider\n-    console.log(`\\n${colors.blue}üìã Testando modelos Aider (External \nAPIs):${colors.reset}`);\n-    \n-    for (const [modelId, config] of Object.entries(ALL_MODELS.aider_models)) {\n-        console.log(`\\n${colors.yellow}üîç Testando ${modelId} \n(${config.provider})...${colors.reset}`);\n-        \n-        const apiKey = process.env[config.key];\n+    // Iterate through all defined models\n+    for (const modelId of Object.keys(MODEL_CATEGORIES.aider_models)) {\n+        const modelConfig = MODEL_CATEGORIES.aider_models[modelId];\n+        const apiKey = process.env[modelConfig.key];\n+\n         if (!apiKey) {\n-            const result = `‚ö†Ô∏è  API key n√£o configurada (${config.key})`;\n-            console.log(`${colors.yellow}${result}${colors.reset}`);\n-            results.skipped.push({ model: modelId, provider: config.provider, \nreason: 'No API key' });\n+            console.log(colors.yellow(`[SKIP] ${modelId} - API key \n'${modelConfig.key}' not configured.`));\n+            results.skipped.push({ modelId, reason: 'API key missing' });\n             continue;\n         }\n \n-        const result = await callModelViaAider(modelId, 'Hello');\n-        console.log(`${result.includes('‚ùå') ? colors.red : \ncolors.green}${result}${colors.reset}`);\n-        \n-        if (result.includes('‚ùå')) {\n-            results.failed.push({ model: modelId, provider: config.provider, \nerror: result });\n+        console.log(`\\n[TESTING] ${modelId}...`);\n+        const result = await callModelViaAider(modelId, testMessage);\n+\n+        if (result.startsWith('‚úÖ')) {\n+            console.log(colors.green(result));\n+            results.working.push(modelId);\n         } else {\n-            results.working.push({ model: modelId, provider: config.provider, \nresult });\n+            console.log(colors.red(result));\n+            results.failed.push({ modelId, reason: result });\n         }\n-\n-        // Pequena pausa entre testes para evitar rate limits\n-        await new Promise(resolve => setTimeout(resolve, 1000));\n     }\n \n-    // Resumo dos resultados\n-    console.log(`\\n${colors.bright}${colors.cyan}üìä RESUMO DOS \nTESTES:${colors.reset}`);\n-    console.log(`${colors.green}‚úÖ Funcionando: ${results.working.length} \nmodelos${colors.reset}`);\n-    console.log(`${colors.red}‚ùå Falharam: ${results.failed.length} \nmodelos${colors.reset}`);\n-    console.log(`${colors.yellow}‚ö†Ô∏è  Pulados: ${results.skipped.length} \nmodelos${colors.reset}`);\n-\n-    if (results.failed.length > 0) {\n-        console.log(`\\n${colors.red}‚ùå Modelos que falharam:${colors.reset}`);\n-        results.failed.forEach(({ model, provider, error }) => {\n-            console.log(`${colors.red}  - ${model} (${provider}): \n${error}${colors.reset}`);\n-        });\n+    // Test cursor models separately\n+    console.log('\\n--- Testing Cursor Agent Models ---');\n+    for (const modelId of MODEL_CATEGORIES.cursor_models) {\n+         console.log(`\\n[TESTING] ${modelId}...`);\n+         const result = await testCursorAgentModel(modelId);\n+         if (result.startsWith('‚úÖ')) {\n+            console.log(colors.green(result));\n+            results.working.push(modelId);\n+        } else {\n+            console.log(colors.red(result));\n+            results.failed.push({ modelId, reason: result });\n+        }\n     }\n \n-    if (results.skipped.length > 0) {\n-        console.log(`\\n${colors.yellow}‚ö†Ô∏è  Modelos pulados (sem API \nkey):${colors.reset}`);\n-        results.skipped.forEach(({ model, provider }) => {\n-            console.log(`${colors.yellow}  - ${model} \n(${provider})${colors.reset}`);\n-        });\n-    }\n \n-    console.log(`\\n${colors.bright}${colors.green}üéâ Teste \nconclu√≠do!${colors.reset}`);\n-    console.log(`${colors.cyan}üí° Para configurar API keys faltantes, edite o \narquivo .env${colors.reset}`);\n+    \nconsole.log('\\n===========================================================');\n+    console.log(colors.bright.cyan('Test Results Summary:'));\n+    console.log('===========================================================');\n+\n+    console.log(colors.green(`\\n‚úÖ Working Models \n(${results.working.length}):`));\n+    results.working.forEach(model => console.log(`   - ${model}`));\n+\n+    console.log(colors.red(`\\n‚ùå Failed Models (${results.failed.length}):`));\n+    results.failed.forEach(({ modelId, reason }) => console.log(`   - \n${modelId}: ${reason}`));\n+\n+    console.log(colors.yellow(`\\n‚ö†Ô∏è Skipped Models \n(${results.skipped.length}):`));\n+    results.skipped.forEach(({ modelId, reason }) => console.log(`   - \n${modelId}: ${reason}`));\n+\n+    \nconsole.log('\\n===========================================================');\n+\n+    // Optionally save results to a file\n+    // saveTestResults(results);\n }\n \n-// Executar teste\n-if (require.main === module) {\n-    testAllModels().catch(error => {\n-        console.error(`${colors.red}‚ùå Erro durante o teste: \n${error.message}${colors.reset}`);\n-        process.exit(1);\n-    });\n-}\n-\n-module.exports = { testAllModels, ALL_MODELS };+// --- Main Execution ---\n+testAllModels().catch(error => {\n+    console.error(colors.red.bold('\\nFATAL ERROR during model testing:'), \nerror);\n+    process.exit(1);\n+});\n```\n\n\nTokens: 8.2k sent, 19k received. Cost: $0.0084 message, $0.0084 session.\n\ngov/bips/BIP-05/monitor/server.js\n\ngov/bips/BIP-05/monitor/test-all-models.js\nOk, I have the contents of `gov/bips/BIP-05/monitor/server.js` and \n`gov/bips/BIP-05/monitor/test-all-models.js`.\n\n\n\nI will not propose any changes to these files unless you ask me to. Please let \nme know if there are any other files you would like to add to our chat.\n\nTokens: 57k sent, 83 received. Cost: $0.0057 message, $0.01 session."
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-1.5-pro",
      "reason": "‚è∞ A resposta do aider demorou muito. Tente novamente."
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-2.5-pro-preview-05-06",
      "reason": "‚è∞ A resposta do aider demorou muito. Tente novamente."
    },
    {
      "provider": "deepseek",
      "model": "deepseek/deepseek-chat",
      "reason": "‚è∞ A resposta do aider demorou muito. Tente novamente."
    }
  ],
  "costReports": [
    {
      "provider": "openai",
      "model": "openai/gpt-4o-mini",
      "hasCostData": true,
      "inputTokens": 7400,
      "outputTokens": 1,
      "inputCost": 0.0011,
      "outputCost": 0,
      "totalCost": 0.0011,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:42:48.586Z"
    },
    {
      "provider": "openai",
      "model": "openai/o1-mini",
      "hasCostData": true,
      "inputTokens": 8200,
      "outputTokens": 781,
      "inputCost": 0.03,
      "outputCost": 0,
      "totalCost": 0.03,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:42:58.626Z"
    },
    {
      "provider": "openai",
      "model": "openai/gpt-4-turbo",
      "hasCostData": true,
      "inputTokens": 8000,
      "outputTokens": 2,
      "inputCost": 0.08,
      "outputCost": 0,
      "totalCost": 0.08,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:43:04.386Z"
    },
    {
      "provider": "openai",
      "model": "openai/gpt-5-mini",
      "hasCostData": true,
      "inputTokens": 9100,
      "outputTokens": 394,
      "inputCost": 0.0031,
      "outputCost": 0,
      "totalCost": 0.0031,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:43:14.770Z"
    },
    {
      "provider": "openai",
      "model": "openai/gpt-5-nano",
      "hasCostData": true,
      "inputTokens": 9100,
      "outputTokens": 2600,
      "inputCost": 0.0015,
      "outputCost": 0,
      "totalCost": 0.0015,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:01.263Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-3-5-haiku-latest",
      "hasCostData": true,
      "inputTokens": 9100,
      "outputTokens": 4,
      "inputCost": 0.0073,
      "outputCost": 0,
      "totalCost": 0.0073,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:06.542Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-3-5-sonnet-latest",
      "hasCostData": false,
      "inputTokens": null,
      "outputTokens": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": null,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:10.477Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-3-opus-latest",
      "hasCostData": false,
      "inputTokens": null,
      "outputTokens": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": null,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:14.623Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-4-sonnet-20250514",
      "hasCostData": true,
      "inputTokens": 9100,
      "outputTokens": 4,
      "inputCost": 0.03,
      "outputCost": 0,
      "totalCost": 0.03,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:20.423Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-4-opus-20250514",
      "hasCostData": true,
      "inputTokens": 9100,
      "outputTokens": 4,
      "inputCost": 0.14,
      "outputCost": 0,
      "totalCost": 0.14,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:26.114Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-3-haiku-20240307",
      "hasCostData": true,
      "inputTokens": 9000,
      "outputTokens": 4,
      "inputCost": 0.0023,
      "outputCost": 0,
      "totalCost": 0.0023,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:30.918Z"
    },
    {
      "provider": "anthropic",
      "model": "anthropic/claude-3-7-sonnet-20250219",
      "hasCostData": true,
      "inputTokens": 11000,
      "outputTokens": 83,
      "inputCost": 0.03,
      "outputCost": 0,
      "totalCost": 0.03,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:49.774Z"
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-2.0-flash",
      "hasCostData": true,
      "inputTokens": 9800,
      "outputTokens": 53,
      "inputCost": 0.001,
      "outputCost": 0,
      "totalCost": 0.001,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:44:58.953Z"
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-2.5-flash",
      "hasCostData": true,
      "inputTokens": 10000,
      "outputTokens": 33,
      "inputCost": 0.0031,
      "outputCost": 0,
      "totalCost": 0.0031,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:45:04.992Z"
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-1.5-flash",
      "hasCostData": true,
      "inputTokens": 8000,
      "outputTokens": 2,
      "inputCost": 0.0006,
      "outputCost": 0,
      "totalCost": 0.0006,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:46:05.263Z"
    },
    {
      "provider": "gemini",
      "model": "gemini/gemini-1.5-flash-8b",
      "hasCostData": true,
      "inputTokens": 8000,
      "outputTokens": 2,
      "inputCost": null,
      "outputCost": null,
      "totalCost": null,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:46:10.764Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-3-mini",
      "hasCostData": true,
      "inputTokens": 7300,
      "outputTokens": 1,
      "inputCost": 0.0022,
      "outputCost": 0,
      "totalCost": 0.0022,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:48:20.337Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-code-fast-1",
      "hasCostData": true,
      "inputTokens": 7500,
      "outputTokens": 1,
      "inputCost": 0.0015,
      "outputCost": 0,
      "totalCost": 0.0015,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:48:28.293Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-3",
      "hasCostData": true,
      "inputTokens": 7300,
      "outputTokens": 1,
      "inputCost": 0.02,
      "outputCost": 0,
      "totalCost": 0.02,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:48:33.396Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-3-fast-beta",
      "hasCostData": true,
      "inputTokens": 9000,
      "outputTokens": 1,
      "inputCost": 0.05,
      "outputCost": 0,
      "totalCost": 0.05,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:48:38.892Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-4",
      "hasCostData": true,
      "inputTokens": 9700,
      "outputTokens": 1,
      "inputCost": 0.03,
      "outputCost": 0,
      "totalCost": 0.03,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:49:02.919Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-3-fast-latest",
      "hasCostData": true,
      "inputTokens": 7300,
      "outputTokens": 1,
      "inputCost": 0.04,
      "outputCost": 0,
      "totalCost": 0.04,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:49:08.014Z"
    },
    {
      "provider": "xai",
      "model": "xai/grok-2",
      "hasCostData": true,
      "inputTokens": 7400,
      "outputTokens": 1,
      "inputCost": 0.01,
      "outputCost": 0,
      "totalCost": 0.01,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:49:12.533Z"
    },
    {
      "provider": "deepseek",
      "model": "deepseek/deepseek-coder",
      "hasCostData": true,
      "inputTokens": 9600,
      "outputTokens": 112,
      "inputCost": 0.0008,
      "outputCost": 0,
      "totalCost": 0.0008,
      "currency": "USD",
      "testTimestamp": "2025-09-09T23:51:11.012Z"
    }
  ],
  "lastTest": "2025-09-09T23:51:11.514Z",
  "summary": {
    "totalProviders": 10,
    "workingProvidersCount": 5,
    "failedProvidersCount": 5,
    "modelsWithCostData": 22,
    "totalCostReports": 24
  }
}