# âœ… Model Execution Checklist

Use this checklist to track which models (generals and collaborators) have contributed and which are still pending.

Legend:
- [x] Contributed
- [ ] Pending

---

## ğŸ§  Generals (high-capacity)
- [x] GPT-5 (OpenAI) â€” reputation-weighted consensus (009)
- [x] Claude-4-Sonnet (Anthropic) â€” performance proposal (006)
- [x] Gemini 2.5 Pro (Google) â€” i18n/l10n (008)
- [x] DeepSeek-R1-0528 (DeepSeek) â€” security/federation (007)
- [x] Grok-3 (xAI) â€” adaptive learning consensus (017)
- [x] GPT-4o (OpenAI) â€” multimodal reasoning (available in Cursor)
- [x] Claude-3.7 (Anthropic) â€” advanced contextual understanding (019)
- [x] Gemini 2.0 (Google) â€” multimodal analysis (available in Cursor)
- [x] DeepSeek-V3 (DeepSeek) â€” advanced reasoning (015)
- [x] Grok Core Fast-1 (xAI) â€” high-performance ML integration (011)

---

## ğŸ§© Collaborators (specialists / smaller models)
- [x] GPT-4o-mini (OpenAI) â€” voting rationale specialist (014)
- [x] GPT-4.1-mini (OpenAI) â€” quick start docs (020)
- [x] GPT-OSS-20B (OpenAI) â€” operational contributor (tested 2025-09)
- [x] Qwen3 235B A22B (Qwen) â€” operational contributor (tested 2025-09)
- [x] Meta AI Llama-3.1-405B-Instruct (Meta) â€” operational contributor (tested 2025-09)
- [x] Llama-3.3-70B-Instruct (Meta) â€” operational contributor (tested 2025-09)
- [x] Claude-4-Opus (Anthropic) â€” complex reasoning (available in Cursor)
- [ ] Llama 3 8B / 11B Instruct (Meta) â€” validate in LLM Studio
- [ ] Llama 3.1 8B (Meta) â€” lightweight analysis (validate in LLM Studio)
- [ ] CodeLlama 34B (Meta) â€” code analysis (validate in LLM Studio)
- [ ] Phi-3-mini / Phi-3.5 (Microsoft) â€” validate in LLM Studio
- [ ] Phi-3 Mini (Microsoft) â€” lightweight analysis (validate in LLM Studio)
- [ ] StarCoder2-15B / CodeLlama-13B (Code) â€” validate in LLM Studio
- [ ] StarCoder2 15B (BigCode) â€” code generation (validate in LLM Studio)
- [ ] DeepSeek-Coder 6.7B / 33B (DeepSeek) â€” validate in LLM Studio
- [ ] DeepSeek Coder 33B (DeepSeek) â€” technical analysis (validate in LLM Studio)
- [ ] Aya-23 8B (AI2) â€” validate in LLM Studio

## âŒ Rejected Models (failed operational tests)
- [x] Qwen3 Coder 480B A35B Instruct (Qwen) â€” slow and inadequate responses (tested 2025-09)
- [x] DeepSeek-R1-0528 Qwen3 8B (DeepSeek) â€” unable to perform basic operational tasks (tested 2025-09)
- [x] Mistral-7B-Instruct-v0.2 (Mistral) â€” insufficient capabilities for operational use (tested 2025-09)
- [x] Mistral-Small-24B-Instruct (Mistral) â€” despite larger size, insufficient operational capabilities (tested 2025-09)

---

## â„¹ï¸ Usage tips
- Check each item after you run the model and record its contribution in `guidelines/MODELS_INDEX.md`.
- To configure generals with a preferred language, consider `.consensus/generals.yml` (optional).
- Follow the protocol reading order: `AI_ENTRY_POINT.md` â†’ `guidelines/MASTER_GUIDELINES.md` â†’ `guidelines/ANALYSIS_INSTRUCTIONS.md` â†’ `guidelines/MODELS_INDEX.md` â†’ `guidelines/INDEX_PROTOCOL.md`.

Last updated: 2025-09-07 22:00:00 UTC
